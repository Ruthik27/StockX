
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50
319/319 [==============================] - 4s 6ms/step - loss: 5.6428e-04 - val_loss: 8.1673e-04
Epoch 2/50
319/319 [==============================] - 2s 5ms/step - loss: 1.4270e-04 - val_loss: 7.9133e-04
Epoch 3/50
319/319 [==============================] - 2s 5ms/step - loss: 1.1164e-04 - val_loss: 0.0013
Epoch 4/50
319/319 [==============================] - 2s 5ms/step - loss: 9.5753e-05 - val_loss: 3.9563e-04
Epoch 5/50
319/319 [==============================] - 2s 5ms/step - loss: 9.3537e-05 - val_loss: 0.0019
Epoch 6/50
319/319 [==============================] - 1s 5ms/step - loss: 9.4904e-05 - val_loss: 0.0018
Epoch 7/50
319/319 [==============================] - 2s 5ms/step - loss: 8.6850e-05 - val_loss: 1.6155e-04
Epoch 8/50
319/319 [==============================] - 1s 5ms/step - loss: 7.7415e-05 - val_loss: 1.6259e-04
Epoch 9/50
319/319 [==============================] - 2s 5ms/step - loss: 8.1663e-05 - val_loss: 0.0026
Epoch 10/50
319/319 [==============================] - 1s 5ms/step - loss: 7.8932e-05 - val_loss: 5.5599e-04
Epoch 11/50
319/319 [==============================] - 2s 5ms/step - loss: 7.8484e-05 - val_loss: 0.0015
Epoch 12/50
319/319 [==============================] - 1s 5ms/step - loss: 7.9119e-05 - val_loss: 9.4579e-04
Epoch 13/50
319/319 [==============================] - 2s 5ms/step - loss: 8.0003e-05 - val_loss: 0.0021
Epoch 14/50
319/319 [==============================] - 2s 5ms/step - loss: 7.7955e-05 - val_loss: 0.0041
Epoch 15/50
319/319 [==============================] - 1s 5ms/step - loss: 7.4754e-05 - val_loss: 0.0069
Epoch 16/50
319/319 [==============================] - 2s 5ms/step - loss: 7.2556e-05 - val_loss: 0.0075
Epoch 17/50
313/319 [============================>.] - ETA: 0s - loss: 7.2884e-05
319/319 [==============================] - 1s 5ms/step - loss: 7.2854e-05 - val_loss: 0.0068
80/80 [==============================] - 0s 1ms/step
80/80 [==============================] - 0s 1ms/step
MMC : 0.0001615458344051584 0.009970694616732462 0.012710068229760153
MMC : 193.8071910962403 43.5929872775436 -150.21420381869672
Epoch 1/50
110/110 [==============================] - 3s 9ms/step - loss: 0.0046 - val_loss: 8.6413e-04
Epoch 2/50
110/110 [==============================] - 1s 5ms/step - loss: 9.5422e-04 - val_loss: 0.0020
Epoch 3/50
110/110 [==============================] - 1s 5ms/step - loss: 7.3258e-04 - val_loss: 2.2813e-04
Epoch 4/50
110/110 [==============================] - 1s 5ms/step - loss: 5.4931e-04 - val_loss: 3.0263e-04
Epoch 5/50
110/110 [==============================] - 1s 5ms/step - loss: 4.8861e-04 - val_loss: 0.0017
Epoch 6/50
110/110 [==============================] - 1s 5ms/step - loss: 5.1018e-04 - val_loss: 3.4214e-04
Epoch 7/50
110/110 [==============================] - 1s 5ms/step - loss: 4.0986e-04 - val_loss: 6.7829e-04
Epoch 8/50
110/110 [==============================] - 1s 5ms/step - loss: 4.2983e-04 - val_loss: 0.0018
Epoch 9/50
110/110 [==============================] - 1s 5ms/step - loss: 3.6137e-04 - val_loss: 4.7952e-04
Epoch 10/50
110/110 [==============================] - 1s 5ms/step - loss: 3.9711e-04 - val_loss: 3.1968e-04
Epoch 11/50
110/110 [==============================] - 1s 5ms/step - loss: 3.6429e-04 - val_loss: 5.1739e-04
Epoch 12/50
110/110 [==============================] - 1s 5ms/step - loss: 3.3724e-04 - val_loss: 9.4829e-04
Epoch 13/50
 86/110 [======================>.......] - ETA: 0s - loss: 3.7730e-04
110/110 [==============================] - 1s 5ms/step - loss: 3.7517e-04 - val_loss: 5.0822e-04
28/28 [==============================] - 0s 1ms/step
28/28 [==============================] - 0s 1ms/step
TDG : 0.00022812501673167605 0.012714412782736018 0.01510380802088255
TDG : 894.9385487465277 347.08312159325766 -547.85542715327
Epoch 1/50
199/199 [==============================] - 3s 7ms/step - loss: 3.4272e-04 - val_loss: 0.0216
Epoch 2/50
199/199 [==============================] - 1s 5ms/step - loss: 7.1317e-05 - val_loss: 0.0147
Epoch 3/50
199/199 [==============================] - 1s 5ms/step - loss: 4.6755e-05 - val_loss: 0.0150
Epoch 4/50
199/199 [==============================] - 1s 5ms/step - loss: 3.6045e-05 - val_loss: 0.0144
Epoch 5/50
199/199 [==============================] - 1s 5ms/step - loss: 3.0683e-05 - val_loss: 0.0165
Epoch 6/50
199/199 [==============================] - 1s 5ms/step - loss: 3.1026e-05 - val_loss: 0.0181
Epoch 7/50
199/199 [==============================] - 1s 5ms/step - loss: 2.8670e-05 - val_loss: 0.0218
Epoch 8/50
199/199 [==============================] - 1s 5ms/step - loss: 2.2265e-05 - val_loss: 0.0201
Epoch 9/50
199/199 [==============================] - 1s 5ms/step - loss: 2.4369e-05 - val_loss: 0.0268
Epoch 10/50
199/199 [==============================] - 1s 5ms/step - loss: 1.9807e-05 - val_loss: 0.0314
Epoch 11/50
199/199 [==============================] - 1s 5ms/step - loss: 1.9590e-05 - val_loss: 0.0444
Epoch 12/50
199/199 [==============================] - 1s 5ms/step - loss: 1.8074e-05 - val_loss: 0.0556
Epoch 13/50
179/199 [=========================>....] - ETA: 0s - loss: 1.7829e-05
199/199 [==============================] - 1s 5ms/step - loss: 1.7627e-05 - val_loss: 0.0555
Epoch 14/50
199/199 [==============================] - 1s 5ms/step - loss: 1.5558e-05 - val_loss: 0.0427
50/50 [==============================] - 0s 1ms/step
50/50 [==============================] - 0s 1ms/step
SNPS : 0.014435385900301882 0.07915246819104416 0.1201473507835353
SNPS : 489.4938175153237 73.61244413163021 -415.8813733836935
Epoch 1/50
144/263 [===============>..............] - ETA: 0s - loss: 0.00150061 - val_loss: 9.1143e-04
Epoch 2/50
263/263 [==============================] - 1s 5ms/step - loss: 9.8075e-04 - val_loss: 5.7660e-04
Epoch 3/50
263/263 [==============================] - 1s 5ms/step - loss: 9.8075e-04 - val_loss: 5.7660e-04
Epoch 4/50
263/263 [==============================] - 1s 5ms/step - loss: 8.7706e-04 - val_loss: 6.1225e-04
Epoch 5/50
229/263 [=========================>....] - ETA: 0s - loss: 8.9255e-04
263/263 [==============================] - 1s 5ms/step - loss: 7.9437e-04 - val_loss: 4.0759e-04
Epoch 7/50
133/263 [==============>...............] - ETA: 0s - loss: 6.9611e-04
263/263 [==============================] - 1s 5ms/step - loss: 7.3274e-04 - val_loss: 7.3684e-04
Epoch 9/50
 25/263 [=>............................] - ETA: 1s - loss: 6.1417e-04
203/263 [======================>.......] - ETA: 0s - loss: 7.0133e-04e-04 - val_loss: 7.3684e-04
109/263 [===========>..................] - ETA: 0s - loss: 6.2699e-04e-04 - val_loss: 3.9441e-04
  1/263 [..............................] - ETA: 1s - loss: 7.2952e-04e-04 - val_loss: 4.2629e-04
263/263 [==============================] - 1s 5ms/step - loss: 6.4313e-04 - val_loss: 5.4787e-04
263/263 [==============================] - 1s 5ms/step - loss: 5.8476e-04 - val_loss: 5.2610e-04
253/263 [===========================>..] - ETA: 0s - loss: 6.2310e-04e-04 - val_loss: 5.2610e-04
145/263 [===============>..............] - ETA: 0s - loss: 6.0725e-04e-04 - val_loss: 4.8741e-04
145/263 [===============>..............] - ETA: 0s - loss: 6.0725e-04e-04 - val_loss: 4.8741e-04
OXY : 62.684752629671024 74.77451765024321 12.089765020572187: 5.9468e-04 - val_loss: 7.7453e-04
OXY : 62.684752629671024 74.77451765024321 12.089765020572187: 5.9468e-04 - val_loss: 7.7453e-04
104/104 [==============================] - 0s 5ms/step - loss: 8.7193e-04 - val_loss: 0.0053e-04
104/104 [==============================] - 0s 5ms/step - loss: 6.6372e-04 - val_loss: 0.0038e-04
Epoch 9/50
 97/104 [==========================>...] - ETA: 0s - loss: 6.7834e-04
104/104 [==============================] - 1s 5ms/step - loss: 5.3077e-04 - val_loss: 0.0016e-04
Epoch 11/50
104/104 [==============================] - 0s 5ms/step - loss: 5.8093e-04 - val_loss: 3.9413e-04
Epoch 12/50
104/104 [==============================] - 1s 5ms/step - loss: 5.3077e-04 - val_loss: 0.0016e-04
Epoch 13/50
 96/104 [==========================>...] - ETA: 0s - loss: 5.2823e-04
104/104 [==============================] - 1s 5ms/step - loss: 5.2072e-04 - val_loss: 9.7184e-04
26/26 [==============================] - 0s 1ms/step
26/26 [==============================] - 0s 1ms/step
BR : 0.0001964588544410552 0.011121478375736992 0.01401637807855707
BR : 177.0698124221888 131.70478188103664 -45.36503054115215
Epoch 1/50
264/274 [===========================>..] - ETA: 0s - loss: 9.0033e-04e-04 - val_loss: 9.7184e-04
274/274 [==============================] - 4s 7ms/step - loss: 8.7424e-04 - val_loss: 0.0014e-04
179/274 [==================>...........] - ETA: 0s - loss: 9.5866e-05e-04 - val_loss: 0.0014e-04
274/274 [==============================] - 1s 5ms/step - loss: 9.2503e-05 - val_loss: 4.0731e-04
229/274 [========================>.....] - ETA: 0s - loss: 7.8307e-05e-05 - val_loss: 4.0731e-04
274/274 [==============================] - 1s 5ms/step - loss: 7.6606e-05 - val_loss: 0.0038e-04
265/274 [============================>.] - ETA: 0s - loss: 7.5124e-05e-05 - val_loss: 0.0038e-04
274/274 [==============================] - 1s 5ms/step - loss: 7.4680e-05 - val_loss: 0.0088e-04
69/69 [==============================] - 0s 1ms/stepep - loss: 7.4680e-05 - val_loss: 0.0088e-04
69/69 [==============================] - 0s 1ms/stepep - loss: 7.4680e-05 - val_loss: 0.0088e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
121/271 [============>.................] - ETA: 0s - loss: 1.1949e-04e-05 - val_loss: 0.0088e-04
271/271 [==============================] - 1s 5ms/step - loss: 1.0936e-04 - val_loss: 4.6366e-04
168/271 [=================>............] - ETA: 0s - loss: 7.8754e-05e-04 - val_loss: 4.6366e-04
271/271 [==============================] - 1s 5ms/step - loss: 7.4207e-05 - val_loss: 0.0132e-04
225/271 [=======================>......] - ETA: 0s - loss: 6.1684e-05e-05 - val_loss: 0.0132e-04
271/271 [==============================] - 1s 5ms/step - loss: 6.1452e-05 - val_loss: 1.4107e-04
265/271 [============================>.] - ETA: 0s - loss: 6.0036e-05e-05 - val_loss: 1.4107e-04
271/271 [==============================] - 1s 5ms/step - loss: 6.0655e-05 - val_loss: 2.7044e-04
271/271 [==============================] - 1s 5ms/step - loss: 5.2788e-05 - val_loss: 0.0023e-04
271/271 [==============================] - 1s 5ms/step - loss: 5.2788e-05 - val_loss: 0.0023e-04
271/271 [==============================] - 1s 5ms/step - loss: 5.8151e-05 - val_loss: 0.0058e-04
271/271 [==============================] - 1s 5ms/step - loss: 5.8151e-05 - val_loss: 0.0058e-04
NDSN : 222.04876727568592 76.46786801757186 -145.580899258114075.8151e-05 - val_loss: 0.0058e-04
NDSN : 222.04876727568592 76.46786801757186 -145.580899258114075.8151e-05 - val_loss: 0.0058e-04
121/121 [==============================] - 1s 5ms/step - loss: 5.6566e-04 - val_loss: 2.9726e-04
121/121 [==============================] - 1s 5ms/step - loss: 3.9080e-04 - val_loss: 3.7399e-04
121/121 [==============================] - 1s 5ms/step - loss: 3.5793e-04 - val_loss: 7.2625e-04
DPZ : 361.6846347613023 291.5471754623276 -70.1374592989747ss: 2.9305e-04 - val_loss: 0.0016e-04
DPZ : 361.6846347613023 291.5471754623276 -70.1374592989747ss: 2.9305e-04 - val_loss: 0.0016e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
DPZ : 361.6846347613023 291.5471754623276 -70.1374592989747ss: 2.9305e-04 - val_loss: 0.0016e-04
323/323 [==============================] - 1s 4ms/step - loss: 7.6166e-04 - val_loss: 2.1159e-04
323/323 [==============================] - 1s 4ms/step - loss: 7.6166e-04 - val_loss: 2.1159e-04
323/323 [==============================] - 1s 4ms/step - loss: 5.2128e-04 - val_loss: 1.4241e-04
323/323 [==============================] - 1s 4ms/step - loss: 5.2128e-04 - val_loss: 1.4241e-04
323/323 [==============================] - 1s 4ms/step - loss: 4.8452e-04 - val_loss: 1.4220e-04
323/323 [==============================] - 1s 4ms/step - loss: 4.8452e-04 - val_loss: 1.4220e-04
323/323 [==============================] - 1s 4ms/step - loss: 3.8543e-04 - val_loss: 1.7957e-04
  1/323 [..............................] - ETA: 1s - loss: 2.8683e-04e-04 - val_loss: 1.7957e-04
323/323 [==============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 18/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 18/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 21/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 23/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 23/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 26/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 26/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 29/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 31/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 31/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 34/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 34/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 37/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 39/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 39/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 42/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 42/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 45/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 47/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 47/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 50/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
Epoch 50/50============================] - 1s 4ms/step - loss: 4.0740e-04 - val_loss: 4.0185e-04
F : 6.61232165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
F : 6.61232165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 19/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 19/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 25/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 1/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 1/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 1/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 4/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 4/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 4/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 7/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 7/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 7/500165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 10/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 10/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
Epoch 10/50165029294e-05 0.005756837244085191 0.008131618320047333740e-04 - val_loss: 4.0185e-04
/var/folders/nr/0m3_2w416k95_79fx2rpjb7h0000gn/T/ipykernel_33985/2142636080.py:148: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 6))
BA : 0.0005126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
BA : 0.0005126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 3/505126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 6/505126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 9/505126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 12/50126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 15/50126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
Epoch 15/50126545024395575 0.015980397648527062 0.0226418749762372260e-04 - val_loss: 4.0185e-04
37/37 [==============================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
37/37 [==============================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
37/37 [==============================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step26418749762372260e-04 - val_loss: 4.0185e-04
LUV : 23.774740685733914 44.731302296299695 20.9565616105657862372260e-04 - val_loss: 4.0185e-04
Epoch 3/5074740685733914 44.731302296299695 20.9565616105657862372260e-04 - val_loss: 4.0185e-04
Epoch 8/5074740685733914 44.731302296299695 20.9565616105657862372260e-04 - val_loss: 4.0185e-04
Epoch 14/504740685733914 44.731302296299695 20.9565616105657862372260e-04 - val_loss: 4.0185e-04
18/18 [==============================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
18/18 [==============================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
18/18 [==============================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step16105657862372260e-04 - val_loss: 4.0185e-04
AEP : 75.80115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
AEP : 75.80115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
AEP : 75.80115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 4/500115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 4/500115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 7/500115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 7/500115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 10/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 10/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 13/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 16/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 16/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 19/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 19/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 22/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
Epoch 22/50115892086722 30.779840201139447 -45.0213187197277762372260e-04 - val_loss: 4.0185e-04
APA : 0.0001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
APA : 0.0001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 3/5001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 5/5001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 7/5001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 9/5001390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 11/501390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
Epoch 11/501390681918503046 0.007085059974674318 0.011792717746571594e-04 - val_loss: 4.0185e-04
IEX : 0.00011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
IEX : 0.00011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 3/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 9/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 12/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 18/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 21/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 27/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 30/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 33/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 39/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 42/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 42/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 1/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 1/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 1/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 4/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 4/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 7/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 7/50011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 10/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 10/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 14/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
Epoch 14/5011450654233807559 0.00829858276285801 0.010700772978531765e-04 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step10700772978531765e-04 - val_loss: 4.0185e-04
MS : 0.00011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
MS : 0.00011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
MS : 0.00011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 4/5011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 6/5011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 6/5011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 9/5011943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 11/501943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 13/501943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
Epoch 13/501943039970764494 0.00877815372316798 0.0109284216475960045e-04 - val_loss: 4.0185e-04
WM : 6.617502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
WM : 6.617502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 3/50502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 3/50502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 6/50502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 6/50502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 9/50502239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
Epoch 11/5002239233035e-05 0.006271899171852237 0.0081348031563357675e-04 - val_loss: 4.0185e-04
DOV : 0.00011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
DOV : 0.00011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
DOV : 0.00011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
Epoch 3/50011514386309813458 0.008392231037368936 0.01073051084982139e-04 - val_loss: 4.0185e-04
KR : 9.699505912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
KR : 9.699505912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 3/50505912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 6/50505912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 8/50505912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 10/5005912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 13/5005912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
Epoch 15/5005912509873e-05 0.007477029241866545 0.0098486069636826689e-04 - val_loss: 4.0185e-04
DRI : 0.0003029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
DRI : 0.0003029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5003029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
Epoch 5/5003029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
Epoch 7/5003029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
Epoch 10/503029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
Epoch 10/503029860836257175 0.012500036930126034 0.017406495443532496e-04 - val_loss: 4.0185e-04
ARE : 0.00036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
ARE : 0.00036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
ARE : 0.00036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 4/50036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 4/50036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 7/50036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 9/50036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 9/50036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 12/5036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 12/5036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 15/5036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 15/5036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
Epoch 15/5036162584372415556 0.015295715336297002 0.019016462439795566-04 - val_loss: 4.0185e-04
HAS : 0.00023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
HAS : 0.00023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 3/50023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 3/50023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 6/50023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 8/50023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 8/50023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 11/5023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 13/5023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 13/5023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 16/5023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
Epoch 16/5023581928990953514 0.011953127741058285 0.015356408756917586-04 - val_loss: 4.0185e-04
LRCX : 0.00227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
LRCX : 0.00227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 3/500227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 5/500227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 8/500227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 11/50227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 14/50227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 17/50227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
Epoch 17/50227433368212349 0.03831726994646649 0.047689974650061306586-04 - val_loss: 4.0185e-04
34/34 [==============================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
34/34 [==============================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
57/57 [==============================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
57/57 [==============================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
57/57 [==============================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step689974650061306586-04 - val_loss: 4.0185e-04
JBHT : 0.00016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
JBHT : 0.00016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 3/500016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 5/500016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 7/500016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 9/500016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 12/50016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 14/50016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 14/50016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
Epoch 14/50016899761226596145 0.010060020622760308 0.01299990816375106-04 - val_loss: 4.0185e-04
QCOM : 120.16181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 3/50.16181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 5/50.16181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 11/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 17/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 23/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 26/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 32/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 35/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
Epoch 35/5016181482420158 53.659347190846205 -66.502467633355386375106-04 - val_loss: 4.0185e-04
21/21 [==============================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
46/46 [==============================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
46/46 [==============================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step467633355386375106-04 - val_loss: 4.0185e-04
PCAR : 6.607441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
PCAR : 6.607441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
PCAR : 6.607441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 4/5007441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 6/5007441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 6/5007441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 9/5007441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 11/507441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 13/507441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
Epoch 13/507441916957781e-05 0.006147961164437767 0.008128617297522242-04 - val_loss: 4.0185e-04
59/59 [==============================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
59/59 [==============================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
59/59 [==============================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
59/59 [==============================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 22/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 24/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 24/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 27/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 27/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 30/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 30/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 33/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 35/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 35/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 38/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 38/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 41/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 41/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
Epoch 44/50==========================] - 0s 1ms/step008128617297522242-04 - val_loss: 4.0185e-04
HST : 0.0001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
HST : 0.0001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
HST : 0.0001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
HST : 0.0001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 5/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 5/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 5/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 9/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 9/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 9/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 13/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 13/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 13/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 17/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 17/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 17/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 17/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 22/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 22/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 25/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 25/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 25/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 29/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 29/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 29/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 33/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 33/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 33/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 37/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 37/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 37/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 37/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 42/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 42/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 45/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 45/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 45/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 1/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 1/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 4/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 7/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 9/5001009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 12/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 15/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
Epoch 17/501009936390188701 0.007066206287693632 0.0100495591454983792-04 - val_loss: 4.0185e-04
STLD : 0.00019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
STLD : 0.00019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
STLD : 0.00019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 4/500019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 4/500019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 7/500019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 7/500019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 10/50019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 10/50019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 13/50019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
Epoch 13/50019526648454829283 0.010015957555632567 0.01397377846354710304 - val_loss: 4.0185e-04
AMD : 0.00042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
AMD : 0.00042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 3/50042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 3/50042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 6/50042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 8/50042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 8/50042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 11/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 13/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 13/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 13/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 17/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 19/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 19/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 22/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 24/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 24/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 27/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 29/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 29/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 32/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 34/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 34/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
Epoch 37/5042876737574203166 0.014554493265397333 0.020706698813235093304 - val_loss: 4.0185e-04
HAL : 0.0001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
HAL : 0.0001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
HAL : 0.0001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
HAL : 0.0001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 5/5001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 7/5001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 7/5001274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 10/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 12/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 12/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 15/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 17/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 17/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 20/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 20/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 23/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 25/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 25/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 28/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 28/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 31/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 31/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 34/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
Epoch 34/501274041638029986 0.007742767357355741 0.0112873452947537053304 - val_loss: 4.0185e-04
WMB : 0.00012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
WMB : 0.00012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
WMB : 0.00012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 4/50012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 4/50012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 7/50012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 7/50012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 10/5012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 10/5012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
Epoch 10/5012907320822750257 0.006858710818239823 0.011361039047001932304 - val_loss: 4.0185e-04
FMC : 9.531694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
FMC : 9.531694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 3/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 3/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 3/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 7/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 9/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 9/501694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 12/50694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 12/50694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 12/50694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
Epoch 12/50694913806462e-05 0.007638790864214992 0.0097630399537267432304 - val_loss: 4.0185e-04
CI : 311.29036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
CI : 311.29036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
CI : 311.29036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
CI : 311.29036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 5/509036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 7/509036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 7/509036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 10/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 12/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 12/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 12/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 16/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 18/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 18/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
Epoch 18/50036826992564 154.65352735860597 -156.6368409113196737267432304 - val_loss: 4.0185e-04
UDR : 0.00016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
UDR : 0.00016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 3/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 3/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 6/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 6/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 9/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 9/50016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 12/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 14/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 14/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 14/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 18/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 20/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 20/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 23/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 23/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 26/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 26/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 29/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 31/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 31/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 34/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 34/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 37/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
Epoch 37/5016407974404605898 0.009490819831076188 0.012809361578394882304 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
69/69 [==============================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step012809361578394882304 - val_loss: 4.0185e-04
SYK : 0.0002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
SYK : 0.0002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 3/5002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 5/5002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 7/5002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 7/5002514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 10/502514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 12/502514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 14/502514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 16/502514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
Epoch 18/502514227862346426 0.010583943410999358 0.0158563169189645872304 - val_loss: 4.0185e-04
53/53 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
53/53 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
53/53 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
11/11 [==============================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 23/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 25/50==========================] - 0s 2ms/step158563169189645872304 - val_loss: 4.0185e-04
53/53 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
53/53 [==============================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step158563169189645872304 - val_loss: 4.0185e-04
CPRT : 0.00032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
CPRT : 0.00032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 3/500032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 6/500032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 8/500032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 11/50032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 13/50032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 15/50032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
Epoch 15/50032827965194702073 0.014426919235094421 0.01811848922915541504 - val_loss: 4.0185e-04
ANSS : 289.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
ANSS : 289.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 3/50.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 5/50.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 7/50.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 9/50.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 9/50.1373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 12/501373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
Epoch 14/501373747353892 159.32346223032582 -129.813912505063372915541504 - val_loss: 4.0185e-04
MNST : 0.00010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
MNST : 0.00010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 3/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 6/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 6/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 9/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 9/500010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 12/50010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 12/50010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
Epoch 12/50010001677662066242 0.0073358122736223025 0.0100008387958541974 - val_loss: 4.0185e-04
OKE : 66.36402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
OKE : 66.36402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
Epoch 4/506402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
Epoch 6/506402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
Epoch 9/506402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
Epoch 12/50402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
Epoch 12/50402533896128 47.50290512255252 -18.861120216408764387958541974 - val_loss: 4.0185e-04
41/41 [==============================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
41/41 [==============================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step216408764387958541974 - val_loss: 4.0185e-04
GPN : 0.00022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
GPN : 0.00022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
GPN : 0.00022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 4/50022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 6/50022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 8/50022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 8/50022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 11/5022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
Epoch 11/5022084730242883583 0.011124804457004653 0.014860932084793195974 - val_loss: 4.0185e-04
EXPD : 0.00018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 3/500018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 8/500018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 17/50018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 23/50018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 32/50018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 38/50018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
Epoch 38/50018744333468574827 0.010216853687685126 0.01369099465655247974 - val_loss: 4.0185e-04
QRVO : 0.0001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
QRVO : 0.0001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
QRVO : 0.0001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 3/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 3/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 6/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 6/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 6/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 9/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 9/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 12/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 12/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 13/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 13/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 4/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 7/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 7/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 10/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 13/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 13/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 1/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 4/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 4/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 7/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 7/50001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 11/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
Epoch 11/5001266739908076283 0.008792319125566321 0.011254954056220234974 - val_loss: 4.0185e-04
ROK : 0.0002410445114767112 0.012299211661394412 0.0155256082482043584974 - val_loss: 4.0185e-04
ROK : 0.0002410445114767112 0.012299211661394412 0.0155256082482043584974 - val_loss: 4.0185e-04
Epoch 3/5002410445114767112 0.012299211661394412 0.0155256082482043584974 - val_loss: 4.0185e-04
Epoch 6/5002410445114767112 0.012299211661394412 0.0155256082482043584974 - val_loss: 4.0185e-04
Epoch 9/5002410445114767112 0.012299211661394412 0.0155256082482043584974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
35/35 [==============================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step155256082482043584974 - val_loss: 4.0185e-04
L : 6.49612021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
L : 6.49612021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/502021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 5/502021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 7/502021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 9/502021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 11/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 13/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 16/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 18/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 20/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 22/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 24/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
Epoch 27/50021384439e-05 0.005781619860313803 0.0080598512479104663584974 - val_loss: 4.0185e-04
CB : 0.0004178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
CB : 0.0004178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CB : 0.0004178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 4/504178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 4/504178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 7/504178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 9/504178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 9/504178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 12/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 14/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 16/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 16/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 19/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
Epoch 21/50178203079776909 0.016062834032154572 0.02044065331582361384974 - val_loss: 4.0185e-04
T : 0.0001636193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
T : 0.0001636193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 3/50636193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 1/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 1/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 4/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 4/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 7/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 7/50036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
Epoch 10/5036193701957582 0.0086043797464804 0.01279137874491089461384974 - val_loss: 4.0185e-04
SYY : 0.00043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
SYY : 0.00043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
SYY : 0.00043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 3/50043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 6/50043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 8/50043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 10/5043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 12/5043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
Epoch 14/5043536243121305085 0.016534409148841887 0.020865340428879924974 - val_loss: 4.0185e-04
SCHW : 0.00028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
SCHW : 0.00028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 6/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 9/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 15/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 15/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 1/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 1/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 3/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 5/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 7/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 7/500028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 10/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 12/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 12/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
Epoch 12/50028103611840840913 0.01080073456131716 0.016764131901426005974 - val_loss: 4.0185e-04
BRO : 70.63855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
BRO : 70.63855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 4/503855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 7/503855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 10/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 13/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 16/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 19/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 22/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
Epoch 22/50855375231313 16.476760447025296 -54.16179330528783901426005974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 1ms/step330528783901426005974 - val_loss: 4.0185e-04
CMI : 0.00039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
CMI : 0.00039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CMI : 0.00039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
CMI : 0.00039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 5/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 5/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 5/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 9/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 9/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 9/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 9/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
Epoch 9/50039521533646242783 0.013455613890692409 0.019880023552864015974 - val_loss: 4.0185e-04
SPGI : 0.0005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
SPGI : 0.0005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 3/50005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 3/50005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 6/50005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 6/50005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 9/50005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 11/5005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 11/5005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
Epoch 14/5005410043746018992 0.017825004536729753 0.023259500738448777974 - val_loss: 4.0185e-04
FITB : 5.293070930869835e-05 0.005305697287574579 0.007275349428632163974 - val_loss: 4.0185e-04
FITB : 5.293070930869835e-05 0.005305697287574579 0.007275349428632163974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5093070930869835e-05 0.005305697287574579 0.007275349428632163974 - val_loss: 4.0185e-04
Epoch 6/5093070930869835e-05 0.005305697287574579 0.007275349428632163974 - val_loss: 4.0185e-04
Epoch 9/5093070930869835e-05 0.005305697287574579 0.007275349428632163974 - val_loss: 4.0185e-04
MPWR : 0.00034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
MPWR : 0.00034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
Epoch 9/500034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
Epoch 15/50034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
Epoch 21/50034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
Epoch 21/50034276806093414355 0.013698816064296813 0.01851399635233148574 - val_loss: 4.0185e-04
18/18 [==============================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
18/18 [==============================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 2ms/step.01851399635233148574 - val_loss: 4.0185e-04
ADBE : 0.0009645362957553053 0.020606976767768195 0.031056984653299896574 - val_loss: 4.0185e-04
ADBE : 0.0009645362957553053 0.020606976767768195 0.031056984653299896574 - val_loss: 4.0185e-04
Epoch 3/50009645362957553053 0.020606976767768195 0.031056984653299896574 - val_loss: 4.0185e-04
Epoch 6/50009645362957553053 0.020606976767768195 0.031056984653299896574 - val_loss: 4.0185e-04
Epoch 9/50009645362957553053 0.020606976767768195 0.031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
36/36 [==============================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step031056984653299896574 - val_loss: 4.0185e-04
JNJ : 0.0006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
JNJ : 0.0006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
JNJ : 0.0006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 4/5006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 4/5006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 7/5006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 7/5006653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 10/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 10/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 13/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 15/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 15/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
Epoch 15/506653804515474205 0.020381625049112107 0.0257949695008042326574 - val_loss: 4.0185e-04
TMO : 0.00021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
TMO : 0.00021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 3/50021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 3/50021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 6/50021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 6/50021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 9/50021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
Epoch 11/5021755470392963538 0.010229322297981604 0.014749735724060826574 - val_loss: 4.0185e-04
PEP : 0.0008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
PEP : 0.0008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 5/5008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 7/5008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 9/5008847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 12/508847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 14/508847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
Epoch 14/508847624958431592 0.022155436625383714 0.0297449574859867526574 - val_loss: 4.0185e-04
WAB : 110.77562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
WAB : 110.77562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
WAB : 110.77562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
WAB : 110.77562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 5/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 5/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 5/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
Epoch 8/5077562362419727 80.73269143053645 -30.04293219366081859867526574 - val_loss: 4.0185e-04
KO : 0.0003450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
KO : 0.0003450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
KO : 0.0003450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 4/503450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 6/503450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 6/503450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 9/503450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 9/503450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
Epoch 12/50450460741874604 0.014276083140942862 0.01857541585503432526574 - val_loss: 4.0185e-04
NTRS : 0.0002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
NTRS : 0.0002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
Epoch 5/50002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
Epoch 8/50002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
Epoch 11/5002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
Epoch 11/5002461477751024057 0.012436887102011638 0.015689097332300724574 - val_loss: 4.0185e-04
MDLZ : 0.0006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
MDLZ : 0.0006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
MDLZ : 0.0006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 5/50006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 7/50006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 7/50006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 10/5006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 10/5006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 13/5006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 13/5006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
Epoch 13/5006021349758827167 0.019304352840333047 0.024538438741752024574 - val_loss: 4.0185e-04
EMR : 92.06821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
EMR : 92.06821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 3/506821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 3/506821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 6/506821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 6/506821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 9/506821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 11/50821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 11/50821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 14/50821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
Epoch 14/50821736451755 63.13684950698334 -28.931367857534212741752024574 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
48/48 [==============================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step857534212741752024574 - val_loss: 4.0185e-04
MTD : 1034.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
MTD : 1034.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 3/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 3/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 6/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 6/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 6/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 6/50.490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 11/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 11/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 11/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 15/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 15/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 15/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
Epoch 19/50490573130887 609.7032437858009 -424.78732934508616741752024574 - val_loss: 4.0185e-04
PFE : 8.35755844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
PFE : 8.35755844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50755844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 5/50755844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 8/50755844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 11/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 14/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 17/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 20/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 26/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 26/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
Epoch 29/5055844663847e-05 0.006240723164778978 0.00914196830372894524574 - val_loss: 4.0185e-04
WYNN : 0.00023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
WYNN : 0.00023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
Epoch 5/500023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
Epoch 8/500023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
Epoch 11/50023336438904305044 0.010336642117561589 0.01527626881941563974 - val_loss: 4.0185e-04
TDY : 0.00019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
TDY : 0.00019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
TDY : 0.00019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
TDY : 0.00019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 5/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 5/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 8/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 8/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 8/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 8/50019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 13/5019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 13/5019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 13/5019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 17/5019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
Epoch 17/5019965340379866713 0.010717189769742686 0.014129876283912296974 - val_loss: 4.0185e-04
CVS : 0.00016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
CVS : 0.00016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CVS : 0.00016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 4/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 4/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 7/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 9/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 9/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 9/50016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 13/5016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
Epoch 13/5016697420800239544 0.010310591491140662 0.012921850022438566974 - val_loss: 4.0185e-04
GPC : 0.0002102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
GPC : 0.0002102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 4/5002102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 6/5002102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 9/5002102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 12/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 15/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 18/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 21/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 24/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
Epoch 26/502102173961637362 0.010483423571662258 0.0144988756861949966974 - val_loss: 4.0185e-04
38/38 [==============================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
38/38 [==============================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
38/38 [==============================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step144988756861949966974 - val_loss: 4.0185e-04
COST : 0.00017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
COST : 0.00017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 9/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 12/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 18/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 21/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 24/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 1/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 1/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 1/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 4/500017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 11/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 11/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 11/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
Epoch 11/50017306201494587841 0.009812590043708344 0.01315530368124880874 - val_loss: 4.0185e-04
HON : 189.26281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
HON : 189.26281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 3/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 3/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 3/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 7/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 7/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 7/5026281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
Epoch 11/506281174321852 54.17415953127616 -135.0886522119423668124880874 - val_loss: 4.0185e-04
MDT : 0.00016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
MDT : 0.00016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
MDT : 0.00016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
Epoch 3/50016276873033563806 0.009832095104871347 0.012758084900784995874 - val_loss: 4.0185e-04
XOM : 0.00047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
XOM : 0.00047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
XOM : 0.00047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 4/50047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 4/50047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 7/50047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 7/50047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 7/50047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 11/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 11/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 11/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 15/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 15/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 15/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 19/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 19/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 19/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 19/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
Epoch 19/5047518535766969896 0.017780392974810384 0.021798746699516995874 - val_loss: 4.0185e-04
CL : 74.56138717230175 60.02084484457969 -14.5405423277220586699516995874 - val_loss: 4.0185e-04
CL : 74.56138717230175 60.02084484457969 -14.5405423277220586699516995874 - val_loss: 4.0185e-04
Epoch 4/50138717230175 60.02084484457969 -14.5405423277220586699516995874 - val_loss: 4.0185e-04
Epoch 7/50138717230175 60.02084484457969 -14.5405423277220586699516995874 - val_loss: 4.0185e-04
Epoch 10/5038717230175 60.02084484457969 -14.5405423277220586699516995874 - val_loss: 4.0185e-04
38/38 [==============================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
38/38 [==============================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step277220586699516995874 - val_loss: 4.0185e-04
FICO : 0.0005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
FICO : 0.0005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
FICO : 0.0005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 4/50005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 4/50005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 7/50005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 9/50005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 9/50005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 12/5005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 12/5005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
Epoch 12/5005943748813280067 0.014000292866161461 0.024379804784452373874 - val_loss: 4.0185e-04
PH : 0.00040848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
PH : 0.00040848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
Epoch 3/5040848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
Epoch 6/5040848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
Epoch 8/5040848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
Epoch 10/500848097969406314 0.016149635528509444 0.0202109123914301083874 - val_loss: 4.0185e-04
46/46 [==============================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
46/46 [==============================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step202109123914301083874 - val_loss: 4.0185e-04
CLX : 0.0002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
CLX : 0.0002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CLX : 0.0002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
CLX : 0.0002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 5/5002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 7/5002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 7/5002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 7/5002405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 11/502405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 13/502405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 13/502405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 13/502405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
Epoch 13/502405324712068811 0.010960187867046075 0.0155091092976637883874 - val_loss: 4.0185e-04
AON : 321.7359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
AON : 321.7359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 4/507359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 6/507359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 9/507359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 11/50359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 13/50359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
Epoch 16/50359347509531 93.18626690199898 -228.54966784895413976637883874 - val_loss: 4.0185e-04
TSCO : 0.000118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
TSCO : 0.000118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5000118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
Epoch 5/5000118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
Epoch 8/5000118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
Epoch 10/500118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
Epoch 10/500118813672456747 0.007971127326138758 0.0109001684600168913874 - val_loss: 4.0185e-04
AMZN : 0.0008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
AMZN : 0.0008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
AMZN : 0.0008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 4/50008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 6/50008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 6/50008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 9/50008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 11/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 11/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 14/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 14/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 17/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
Epoch 17/5008140487471761705 0.026565065358789098 0.028531539516404834874 - val_loss: 4.0185e-04
66/66 [==============================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
66/66 [==============================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
66/66 [==============================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
66/66 [==============================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
66/66 [==============================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step028531539516404834874 - val_loss: 4.0185e-04
NI : 25.34231989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
NI : 25.34231989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
Epoch 3/50231989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
Epoch 15/5031989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
Epoch 27/5031989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
Epoch 39/5031989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
Epoch 39/5031989546802 10.835375499129295 -14.506944396338724516404834874 - val_loss: 4.0185e-04
8/8 [==============================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 2/50=========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 2/50=========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 5/50=========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 8/50=========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 8/50=========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 14/50========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
Epoch 17/50========================] - 0s 2ms/step44396338724516404834874 - val_loss: 4.0185e-04
MCHP : 0.0017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
MCHP : 0.0017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 3/50017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 6/50017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 9/50017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 12/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 12/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 15/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 18/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 21/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 24/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 27/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 27/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
Epoch 27/5017162877922649344 0.03159896324135347 0.0414281038941554834874 - val_loss: 4.0185e-04
NTAP : 4.856471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
Epoch 3/5056471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
Epoch 6/5056471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
Epoch 9/5056471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
Epoch 12/506471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
Epoch 12/506471566928129e-05 0.005534319963601711 0.006968838903955327874 - val_loss: 4.0185e-04
PLD : 0.0008135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
PLD : 0.0008135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
Epoch 3/5008135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
Epoch 6/5008135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
Epoch 9/5008135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
Epoch 12/508135980513322463 0.018396096185094413 0.0285236402188122947874 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
47/47 [==============================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step285236402188122947874 - val_loss: 4.0185e-04
ECL : 174.75485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
ECL : 174.75485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 3/5075485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 5/5075485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 5/5075485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 8/5075485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 10/505485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 10/505485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
Epoch 13/505485274432057 98.95933413505554 -75.79551860926503188122947874 - val_loss: 4.0185e-04
HUM : 0.00046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
HUM : 0.00046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
Epoch 5/50046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
Epoch 7/50046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
Epoch 10/5046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
Epoch 10/5046434208967118727 0.014656453074136946 0.021548598322656337874 - val_loss: 4.0185e-04
NVDA : 0.0001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
Epoch 3/50001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
Epoch 11/5001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
Epoch 17/5001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
Epoch 26/5001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
Epoch 35/5001995463258605129 0.00821945641088102 0.0141260867143208817874 - val_loss: 4.0185e-04
PYPL : 0.00014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
PYPL : 0.00014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
Epoch 3/500014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
Epoch 9/500014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
Epoch 15/50014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
Epoch 18/50014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
Epoch 18/50014290839181736403 0.008890918367307194 0.01195442979892240874 - val_loss: 4.0185e-04
21/21 [==============================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step.01195442979892240874 - val_loss: 4.0185e-04
FDX : 0.0002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
FDX : 0.0002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 3/5002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 6/5002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 9/5002875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 12/502875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 12/502875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 15/502875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 18/502875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
Epoch 18/502875538233926416 0.013194695781293187 0.0169574120487956980874 - val_loss: 4.0185e-04
ALB : 127.03322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
ALB : 127.03322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 3/5003322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 5/5003322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 5/5003322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 8/5003322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 8/5003322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 11/503322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 11/503322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
Epoch 11/503322554910869 131.95844278857112 4.925217239462427487956980874 - val_loss: 4.0185e-04
TFC : 0.00017580528835210065 0.00918808189732789 0.0132591586592853180874 - val_loss: 4.0185e-04
TFC : 0.00017580528835210065 0.00918808189732789 0.0132591586592853180874 - val_loss: 4.0185e-04
TFC : 0.00017580528835210065 0.00918808189732789 0.0132591586592853180874 - val_loss: 4.0185e-04
TFC : 0.00017580528835210065 0.00918808189732789 0.0132591586592853180874 - val_loss: 4.0185e-04
121/319 [==========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
121/319 [==========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
Epoch 8/50=========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
Epoch 8/50=========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
Epoch 8/50=========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
Epoch 8/50=========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
Epoch 8/50=========>...................] - ETA: 0s - loss: 1.5280e-040874 - val_loss: 4.0185e-04
AVY : 181.54145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
AVY : 181.54145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 3/5054145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 5/5054145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 5/5054145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 5/5054145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 9/5054145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 11/504145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 11/504145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 14/504145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
Epoch 14/504145719425728 42.496849708988336 -139.0446074852689580e-040874 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
69/69 [==============================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step6074852689580e-040874 - val_loss: 4.0185e-04
OMC : 76.99013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
OMC : 76.99013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
OMC : 76.99013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
Epoch 5/509013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
Epoch 7/509013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
Epoch 9/509013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
Epoch 9/509013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
Epoch 12/50013109778312 78.01447588244474 1.024344784661622689580e-040874 - val_loss: 4.0185e-04
61/61 [==============================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
61/61 [==============================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
61/61 [==============================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step4661622689580e-040874 - val_loss: 4.0185e-04
UNP : 0.0014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
UNP : 0.0014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 3/5014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 5/5014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 5/5014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 8/5014555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 10/504555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 10/504555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 10/504555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
Epoch 10/504555739847413873 0.03298680313523181 0.03815198533158382040874 - val_loss: 4.0185e-04
BMY : 52.6906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
BMY : 52.6906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 3/50906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 3/50906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 6/50906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 6/50906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 6/50906097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 10/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 10/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 13/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 13/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 16/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 16/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
Epoch 16/5006097922417 43.73407406441576 -8.95653572782594433158382040874 - val_loss: 4.0185e-04
RF : 9.187210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
RF : 9.187210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 3/50210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 3/50210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 3/50210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 6/50210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 9/50210514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 12/5010514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 14/5010514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
Epoch 14/5010514687996e-05 0.007428405243101162 0.00958499374787902340874 - val_loss: 4.0185e-04
CSGP : 0.0005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
CSGP : 0.0005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
CSGP : 0.0005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 4/50005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 6/50005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 6/50005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 9/50005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 9/50005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 12/5005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 12/5005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
Epoch 15/5005336920423797932 0.019419927734309728 0.023101775740834150874 - val_loss: 4.0185e-04
PPG : 0.00018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
PPG : 0.00018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 6/50018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 12/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 12/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 1/50018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 1/50018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 7/50018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 13/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 19/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 22/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
Epoch 22/5018587465781658372 0.009410984835782928 0.013633585655159970874 - val_loss: 4.0185e-04
18/18 [==============================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step013633585655159970874 - val_loss: 4.0185e-04
V : 0.00013389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
V : 0.00013389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 3/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 3/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 6/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 6/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 9/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 9/503389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 12/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 12/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 1/500389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 3/500389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 5/500389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 11/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 14/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 17/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
Epoch 17/50389822529824764 0.00919949073121545 0.011571440070200754970874 - val_loss: 4.0185e-04
28/28 [==============================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
28/28 [==============================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 21/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 23/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 25/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 27/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 30/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 32/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 34/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 37/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 39/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 41/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 43/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 45/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 47/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 50/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
Epoch 50/50==========================] - 0s 1ms/step571440070200754970874 - val_loss: 4.0185e-04
SPG : 0.0002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
SPG : 0.0002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
SPG : 0.0002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 4/5002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 6/5002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 6/5002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 9/5002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 9/5002685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 12/502685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
Epoch 14/502685977641678212 0.01220569736182191 0.01638895250367823670874 - val_loss: 4.0185e-04
SWK : 0.0002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
SWK : 0.0002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
SWK : 0.0002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 3/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 3/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 3/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 7/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 7/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 7/5002519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 11/502519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 11/502519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 11/502519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 11/502519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
Epoch 11/502519036345746056 0.011556446276810867 0.0158714723505604740874 - val_loss: 4.0185e-04
XEL : 59.91811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
XEL : 59.91811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 3/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 5/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 5/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 8/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 8/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 8/501811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 12/50811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
Epoch 12/50811252624673 24.947606537786363 -34.97050598846037505604740874 - val_loss: 4.0185e-04
CMCSA : 0.00012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
CMCSA : 0.00012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
CMCSA : 0.00012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 4/5000012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 6/5000012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 6/5000012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 9/5000012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 9/5000012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 12/500012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
Epoch 14/500012526047933132675 0.007866410125631879 0.0111919828150031914 - val_loss: 4.0185e-04
NKE : 0.0006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
NKE : 0.0006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
NKE : 0.0006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 4/5006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 4/5006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 7/5006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 7/5006996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 10/506996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
Epoch 12/506996797762537689 0.017751680753665195 0.0264514607584112221914 - val_loss: 4.0185e-04
SWKS : 0.00020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
SWKS : 0.00020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 5/500020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 7/500020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 9/500020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 11/50020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 11/50020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
Epoch 11/50020383617696947566 0.012041207089667006 0.01427712075208008914 - val_loss: 4.0185e-04
ROP : 503.1181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
ROP : 503.1181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
Epoch 4/501181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
Epoch 7/501181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
Epoch 10/50181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
Epoch 12/50181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
Epoch 12/50181684399018 236.67069716192964 -266.4474712779721775208008914 - val_loss: 4.0185e-04
SBAC : 226.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
SBAC : 226.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
SBAC : 226.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
Epoch 5/50.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
Epoch 7/50.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
Epoch 9/50.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
Epoch 9/50.8432447710655 155.95027153905414 -70.8929732320113775208008914 - val_loss: 4.0185e-04
62/62 [==============================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
62/62 [==============================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 22/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 25/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 28/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 31/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 34/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 37/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 40/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 43/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 46/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 49/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
Epoch 49/50==========================] - 0s 1ms/step732320113775208008914 - val_loss: 4.0185e-04
TPR : 9.254019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
TPR : 9.254019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 4/504019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 6/504019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 8/504019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 10/50019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 10/50019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
Epoch 10/50019737598268e-05 0.007133433666745874 0.0096197815659183608914 - val_loss: 4.0185e-04
ORCL : 0.0002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
ORCL : 0.0002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
Epoch 3/50002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
Epoch 5/50002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
Epoch 7/50002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
Epoch 10/5002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
Epoch 12/5002637439416450142 0.009125434176922432 0.016240195246517642914 - val_loss: 4.0185e-04
APH : 9.259925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
APH : 9.259925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/509925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
Epoch 5/509925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
Epoch 7/509925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
Epoch 7/509925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
Epoch 10/50925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
Epoch 12/50925790922631e-05 0.007640963405534244 0.0096228508202728732914 - val_loss: 4.0185e-04
CIK : 0.0009143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
CIK : 0.0009143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5009143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
Epoch 9/5009143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
Epoch 12/509143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
Epoch 15/509143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
Epoch 15/509143179603742878 0.021537505716623804 0.0302376910556062132914 - val_loss: 4.0185e-04
TMUS : 0.0003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
TMUS : 0.0003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
Epoch 3/50003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
Epoch 6/50003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
Epoch 9/50003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
Epoch 12/5003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
Epoch 12/5003038850988248804 0.014225677042118176 0.017432300445577472914 - val_loss: 4.0185e-04
37/37 [==============================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step017432300445577472914 - val_loss: 4.0185e-04
ISRG : 279.04355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
ISRG : 279.04355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 4/50.04355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 6/50.04355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 8/50.04355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 10/5004355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 12/5004355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 12/5004355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
Epoch 12/5004355769209155 185.15803078298921 -93.885526909102345577472914 - val_loss: 4.0185e-04
MTCH : 28.92273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
MTCH : 28.92273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
MTCH : 28.92273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 5/5092273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 7/5092273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 9/5092273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 11/502273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 11/502273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 14/502273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
Epoch 14/502273941172113 23.47274808297518 -5.449991328745948545577472914 - val_loss: 4.0185e-04
ROST : 0.0004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
ROST : 0.0004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
ROST : 0.0004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
Epoch 6/50004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
Epoch 12/5004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
Epoch 18/5004450201745134424 0.016113103692836864 0.021095501286137822914 - val_loss: 4.0185e-04
IQV : 0.00015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
IQV : 0.00015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/50015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
Epoch 12/5015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
Epoch 21/5015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
Epoch 30/5015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
Epoch 36/5015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
Epoch 45/5015836817540996966 0.009639689555216486 0.012584441799697341914 - val_loss: 4.0185e-04
13/13 [==============================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
13/13 [==============================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step012584441799697341914 - val_loss: 4.0185e-04
WST : 0.0003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
WST : 0.0003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 3/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 3/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 6/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 6/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 9/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 9/5003265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 12/503265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
Epoch 12/503265273158325144 0.012325747770953634 0.0180700668463764841914 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step180700668463764841914 - val_loss: 4.0185e-04
EIX : 0.00017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
EIX : 0.00017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 9/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 12/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 15/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 15/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 1/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 1/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 3/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 6/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 8/50017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 10/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 12/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 14/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 16/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 18/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
Epoch 18/5017513394355715605 0.01055452372942073 0.0132338181775765741914 - val_loss: 4.0185e-04
AES : 16.85705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
AES : 16.85705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 3/505705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 5/505705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 7/505705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 9/505705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 11/50705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 11/50705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
Epoch 11/50705159311087 11.432972311973572 -5.424079281137299775765741914 - val_loss: 4.0185e-04
CDNS : 0.0013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 3/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 5/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 11/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 14/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 14/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 1/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 1/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 1/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 1/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 6/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 6/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 6/50013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 10/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 10/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 10/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 14/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
Epoch 14/5013780065168350142 0.019806603996947923 0.037121510163717941914 - val_loss: 4.0185e-04
WY : 0.00010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
WY : 0.00010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
WY : 0.00010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 4/5010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 4/5010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 7/5010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 9/5010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 9/5010167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 12/500167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
Epoch 14/500167606492123535 0.007127647215592568 0.0100834550091342871914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
NOC : 0.0007333804512495303 0.024856995779342427 0.0270809979736628371914 - val_loss: 4.0185e-04
DTE : 98.24388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
DTE : 98.24388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 3/504388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 6/504388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 8/504388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 10/50388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 10/50388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
Epoch 10/50388199168936 38.97231596094847 -59.271566030740899736628371914 - val_loss: 4.0185e-04
DECK : 597.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
DECK : 597.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 3/50.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 3/50.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 6/50.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 8/50.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 8/50.099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 11/50099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 11/50099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
Epoch 11/50099976 66.63641767960668 -530.46355832039330740899736628371914 - val_loss: 4.0185e-04
BAX : 33.91240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
BAX : 33.91240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
BAX : 33.91240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 5/501240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 5/501240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 8/501240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 10/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 10/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 13/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 13/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 16/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 16/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 19/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 19/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
Epoch 19/50240119465689 39.91477602228362 6.00237482762673199736628371914 - val_loss: 4.0185e-04
ETR : 98.33655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
Epoch 3/503655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
Epoch 8/503655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
Epoch 17/50655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
Epoch 23/50655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
Epoch 29/50655167614792 67.86430829018354 -30.472243385964376736628371914 - val_loss: 4.0185e-04
16/16 [==============================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
16/16 [==============================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step385964376736628371914 - val_loss: 4.0185e-04
WDC : 0.00019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
WDC : 0.00019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 3/50019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 5/50019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 7/50019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 9/50019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 12/5019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 14/5019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 16/5019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 18/5019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
Epoch 18/5019376019907666942 0.010415358086218034 0.013919777263902948914 - val_loss: 4.0185e-04
RCL : 92.09066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
RCL : 92.09066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 3/509066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 5/509066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 8/509066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 10/50066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 12/50066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 14/50066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
Epoch 14/50066870133537 124.54999062272448 32.459321921389105263902948914 - val_loss: 4.0185e-04
MSFT : 355.0179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
MSFT : 355.0179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 4/50.0179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 10/500179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 13/500179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 13/500179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 1/5000179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 1/5000179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 4/5000179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 6/5000179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 9/5000179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
Epoch 11/500179450711816 47.87618156653777 -307.1417635046438563902948914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
40/40 [==============================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 1ms/step635046438563902948914 - val_loss: 4.0185e-04
EQT : 44.44703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
EQT : 44.44703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
Epoch 4/504703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
Epoch 10/50703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
Epoch 13/50703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
Epoch 13/50703876946761 43.63962784235925 -0.8074109271083572563902948914 - val_loss: 4.0185e-04
CMG : 0.00025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
CMG : 0.00025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
CMG : 0.00025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 4/50025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 6/50025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 6/50025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 9/50025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 9/50025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 12/5025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
Epoch 12/5025270095045642923 0.011630918325433216 0.015896570399190804914 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step015896570399190804914 - val_loss: 4.0185e-04
DOC : 0.0002005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
DOC : 0.0002005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
Epoch 5/5002005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
Epoch 8/5002005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
Epoch 11/502005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
Epoch 13/502005676662105576 0.009777677017460623 0.0141621914339044864914 - val_loss: 4.0185e-04
41/41 [==============================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
41/41 [==============================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
41/41 [==============================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step141621914339044864914 - val_loss: 4.0185e-04
STT : 0.00018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
STT : 0.00018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
STT : 0.00018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
Epoch 4/50018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
Epoch 6/50018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
Epoch 6/50018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
Epoch 9/50018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
Epoch 9/50018114202113527365 0.010208583912373557 0.013458901186028287914 - val_loss: 4.0185e-04
 1/69 [..............................] - ETA: 29s 0.013458901186028287914 - val_loss: 4.0185e-04
ADI : 167.13318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
ADI : 167.13318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ADI : 167.13318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 3/5013318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 6/5013318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 9/5013318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 12/503318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 15/503318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 1/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 1/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 4/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 6/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 6/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 9/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 9/5003318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 12/503318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 12/503318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
Epoch 15/503318376636735 57.39861314102143 -109.7345706253459286028287914 - val_loss: 4.0185e-04
GL : 0.0005038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
GL : 0.0005038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
GL : 0.0005038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 4/505038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 4/505038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 7/505038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 7/505038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 10/50038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 10/50038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 13/50038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
Epoch 13/50038581258140119 0.01988039331761771 0.022446784308983146287914 - val_loss: 4.0185e-04
PGR : 0.00035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
PGR : 0.00035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 6/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 9/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 1/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 1/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/50035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 15/5035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 21/5035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 27/5035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 36/5035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
Epoch 36/5035577465927021546 0.014911118516475521 0.018861989801455617914 - val_loss: 4.0185e-04
15/15 [==============================] - 0s 1ms/step018861989801455617914 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step018861989801455617914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step018861989801455617914 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step018861989801455617914 - val_loss: 4.0185e-04
Epoch 26/50==========================] - 0s 1ms/step018861989801455617914 - val_loss: 4.0185e-04
ETSY : 0.00019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
ETSY : 0.00019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ETSY : 0.00019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 3/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 6/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 9/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 12/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 15/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 18/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 18/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 21/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 24/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 24/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 1/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 1/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 7/500019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 10/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
Epoch 13/50019869121236985065 0.010397074442110778 0.01409578704329242714 - val_loss: 4.0185e-04
33/33 [==============================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
33/33 [==============================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
30/30 [==============================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step.01409578704329242714 - val_loss: 4.0185e-04
FI : 0.0001371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
FI : 0.0001371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 3/501371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 3/501371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 6/501371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 6/501371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 6/501371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 10/50371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 10/50371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
Epoch 10/50371147406476277 0.009246993806160382 0.01170960036242175542714 - val_loss: 4.0185e-04
VMC : 0.0002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
VMC : 0.0002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
VMC : 0.0002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 4/5002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 4/5002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 7/5002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 7/5002185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 10/502185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
Epoch 10/502185705877903607 0.01059619284862986 0.01478413297391364442714 - val_loss: 4.0185e-04
LMT : 0.00014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
LMT : 0.00014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
LMT : 0.00014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 4/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 4/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 7/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 9/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 9/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
Epoch 9/50014401495627958463 0.00840984494288482 0.0120006231621355662714 - val_loss: 4.0185e-04
40/69 [================>.............] - ETA: 0s 0.0120006231621355662714 - val_loss: 4.0185e-04
40/69 [================>.............] - ETA: 0s 0.0120006231621355662714 - val_loss: 4.0185e-04
PEG : 63.554694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
PEG : 63.554694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
PEG : 63.554694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 5/5054694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 7/5054694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 7/5054694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 10/504694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 12/504694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
Epoch 12/504694812765064 42.07966163292223 -21.47503317984283221355662714 - val_loss: 4.0185e-04
 1/66 [..............................] - ETA: 23s503317984283221355662714 - val_loss: 4.0185e-04
 1/66 [..............................] - ETA: 23s503317984283221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
SO : 70.16320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
Epoch 7/50320484352332 43.86778162686574 -26.2954232166575753221355662714 - val_loss: 4.0185e-04
ED : 9.93558953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
ED : 9.93558953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5058953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
Epoch 6/5058953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
Epoch 9/5058953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
Epoch 11/508953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
Epoch 14/508953261511e-05 0.007564565323184039 0.009967742739765664662714 - val_loss: 4.0185e-04
39/39 [==============================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
39/39 [==============================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
39/39 [==============================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
39/39 [==============================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step967742739765664662714 - val_loss: 4.0185e-04
HPQ : 0.00010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
HPQ : 0.00010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 3/50010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 3/50010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 6/50010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 8/50010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 8/50010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 11/5010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 13/5010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 13/5010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 13/5010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
Epoch 13/5010823712939398973 0.006989626555857005 0.010403707483103786714 - val_loss: 4.0185e-04
VLO : 126.5057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
VLO : 126.5057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 3/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 3/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 6/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 6/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 9/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 9/505057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 12/50057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
Epoch 14/50057773553306 64.2142326525618 -62.2915447027687907483103786714 - val_loss: 4.0185e-04
TFX : 0.0003026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
TFX : 0.0003026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5003026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 5/5003026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 7/5003026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 10/503026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 13/503026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 15/503026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 18/503026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
Epoch 18/503026808846501742 0.014124461006663636 0.0173977264218682966714 - val_loss: 4.0185e-04
CTSH : 66.02864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 3/5002864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 8/5002864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 14/502864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 20/502864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 20/502864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 1/5002864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 1/5002864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 4/5002864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
Epoch 10/502864466366564 73.89051386627436 7.8618692026087164218682966714 - val_loss: 4.0185e-04
AMP : 0.00041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
AMP : 0.00041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
Epoch 6/50041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
Epoch 9/50041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
Epoch 9/50041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
Epoch 12/5041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
Epoch 12/5041773542886349986 0.01644673414087698 0.0204385769774585866714 - val_loss: 4.0185e-04
EBAY : 0.0002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
EBAY : 0.0002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 6/50002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 12/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 15/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 21/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 24/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 30/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 33/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 33/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 1/50002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 4/50002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 7/50002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 10/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 16/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 19/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 22/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 25/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 28/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 34/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 37/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 40/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 43/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
Epoch 49/5002231254345150924 0.010816608847788858 0.014937383790848129714 - val_loss: 4.0185e-04
PM : 0.00013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
PM : 0.00013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PM : 0.00013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 3/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 3/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 6/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 6/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 6/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 9/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
Epoch 9/5013969476106609482 0.009653853241931293 0.0118192538286515719714 - val_loss: 4.0185e-04
80/80 [==============================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
80/80 [==============================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step118192538286515719714 - val_loss: 4.0185e-04
PAYX : 0.00023997919774815195 0.010288787978171671 0.01549126198048925714 - val_loss: 4.0185e-04
PAYX : 0.00023997919774815195 0.010288787978171671 0.01549126198048925714 - val_loss: 4.0185e-04
Epoch 3/500023997919774815195 0.010288787978171671 0.01549126198048925714 - val_loss: 4.0185e-04
Epoch 17/50023997919774815195 0.010288787978171671 0.01549126198048925714 - val_loss: 4.0185e-04
Epoch 35/50023997919774815195 0.010288787978171671 0.01549126198048925714 - val_loss: 4.0185e-04
CARR : 0.0005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
CARR : 0.0005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CARR : 0.0005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 3/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 3/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 6/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 6/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 9/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 9/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 12/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 7/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 13/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 19/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 19/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 4/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 4/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 7/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 10/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 13/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 16/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 16/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 19/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 22/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 25/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 7/50005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 10/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 13/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 13/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
Epoch 13/5005237819456834951 0.01632032770410183 0.0228862829153948725714 - val_loss: 4.0185e-04
23/23 [==============================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
23/23 [==============================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500==========================] - 9s 1ms/step228862829153948725714 - val_loss: 4.0185e-04
 73/247 [=======>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
Epoch 6/50======>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
Epoch 6/50======>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
Epoch 9/50======>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
Epoch 11/50=====>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
Epoch 13/50=====>......................] - ETA: 0s - loss: 5.8353e-055714 - val_loss: 4.0185e-04
AJG : 0.0013417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
AJG : 0.0013417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5013417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 9/5013417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 12/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 1/5003417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 1/5003417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5003417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 10/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 16/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 19/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 25/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 28/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 34/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 40/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 46/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 49/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
Epoch 49/503417485346916382 0.0221857803770321 0.0366298858132487e-055714 - val_loss: 4.0185e-04
KMI : 0.00012488727981653297 0.009481156883729427 0.011175297750688032714 - val_loss: 4.0185e-04
Epoch 3/50012488727981653297 0.009481156883729427 0.011175297750688032714 - val_loss: 4.0185e-04
Epoch 12/5012488727981653297 0.009481156883729427 0.011175297750688032714 - val_loss: 4.0185e-04
Epoch 18/5012488727981653297 0.009481156883729427 0.011175297750688032714 - val_loss: 4.0185e-04
Epoch 18/5012488727981653297 0.009481156883729427 0.011175297750688032714 - val_loss: 4.0185e-04
CZR : 0.00020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
CZR : 0.00020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 3/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 3/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 6/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 6/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 9/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 9/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 12/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 1/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 1/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 13/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 22/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 34/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 43/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 43/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 1/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 1/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 4/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 4/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 7/50020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 10/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 13/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 13/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
Epoch 13/5020235602702271525 0.011350853167999426 0.014225189876508336714 - val_loss: 4.0185e-04
LIN : 0.0002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
LIN : 0.0002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 3/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 6/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 9/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 9/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 12/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 15/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 18/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 21/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 1/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 1/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 4/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 4/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 7/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 7/5002087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 10/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 10/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 13/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
Epoch 13/502087551844797705 0.010770918118417032 0.0144483626920066796714 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 1ms/step144483626920066796714 - val_loss: 4.0185e-04
LOW : 0.0013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
LOW : 0.0013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
LOW : 0.0013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
LOW : 0.0013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
LOW : 0.0013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 6/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 6/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 9/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 9/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 9/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 9/5013658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 14/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 14/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 14/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 14/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 19/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 19/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 19/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 23/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
Epoch 23/503658001921236249 0.023148999747657984 0.0369567340565102596714 - val_loss: 4.0185e-04
AIG : 3.532782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
AIG : 3.532782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
AIG : 3.532782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
Epoch 4/502782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
Epoch 6/502782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
Epoch 6/502782472331281e-06 0.0013501960615528094 0.001879569757240012914 - val_loss: 4.0185e-04
177/241 [=====================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 11/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 13/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 13/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 16/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 16/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 19/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 21/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 23/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 23/50===================>........] - ETA: 0s - loss: 5.6082e-042914 - val_loss: 4.0185e-04
61/61 [==============================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
61/61 [==============================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step loss: 5.6082e-042914 - val_loss: 4.0185e-04
MAA : 0.00039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
MAA : 0.00039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
MAA : 0.00039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 4/50039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 4/50039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 7/50039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 7/50039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 10/5039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 10/5039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 13/5039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
Epoch 13/5039396200359043175 0.012845703316589683 0.019848476102472746914 - val_loss: 4.0185e-04
VTRS : 0.0001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
VTRS : 0.0001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 5/50001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 5/50001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 8/50001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 10/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 10/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 13/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 13/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 16/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
Epoch 18/5001911139246126325 0.009131267690646113 0.013824395994495836914 - val_loss: 4.0185e-04
ATO : 0.0009776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
ATO : 0.0009776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5009776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
Epoch 5/5009776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
Epoch 8/5009776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
Epoch 11/509776166662505742 0.023481446585811236 0.0312668621107167436914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step312668621107167436914 - val_loss: 4.0185e-04
40/40 [==============================] - 0s 1ms/step312668621107167436914 - val_loss: 4.0185e-04
 15/323 [>.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
 15/323 [>.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 4/50.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 4/50.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 7/50.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 7/50.............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 10/50............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 10/50............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
Epoch 13/50............................] - ETA: 1s - loss: 0.0048  436914 - val_loss: 4.0185e-04
TXN : 0.0001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
TXN : 0.0001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
TXN : 0.0001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
TXN : 0.0001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 5/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 5/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 5/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 9/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 9/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
Epoch 9/5001246357194900698 0.008399609422285915 0.0111640368814362926914 - val_loss: 4.0185e-04
NEE : 0.000377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
NEE : 0.000377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 3/500377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 6/500377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 8/500377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 8/500377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 11/50377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
Epoch 13/50377461429347162 0.014020810585065463 0.01942836661552283926914 - val_loss: 4.0185e-04
JKHY : 0.00020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
JKHY : 0.00020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
JKHY : 0.00020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 4/500020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 4/500020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 7/500020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 7/500020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 10/50020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 12/50020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 12/50020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 12/50020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
Epoch 12/50020218544119842208 0.010096396205095544 0.01421919270558009114 - val_loss: 4.0185e-04
BIO : 0.00021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
BIO : 0.00021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
BIO : 0.00021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 4/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 4/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 4/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 4/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 9/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 9/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
Epoch 9/50021143812080120494 0.008497514179039562 0.014540911965939582114 - val_loss: 4.0185e-04
MKC : 0.0001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
MKC : 0.0001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 5/5001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 7/5001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 9/5001435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 11/501435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 11/501435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
Epoch 11/501435728320464483 0.008386085109011366 0.0119821881159681482114 - val_loss: 4.0185e-04
VRTX : 379.2872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
Epoch 3/50.2872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
Epoch 5/50.2872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
Epoch 11/502872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
Epoch 14/502872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
Epoch 17/502872582983916 131.6984666843286 -247.5887916140629859681482114 - val_loss: 4.0185e-04
LYV : 0.0002974303724178477 0.012159345589381672 0.0172461697897778952114 - val_loss: 4.0185e-04
LYV : 0.0002974303724178477 0.012159345589381672 0.0172461697897778952114 - val_loss: 4.0185e-04
Epoch 3/5002974303724178477 0.012159345589381672 0.0172461697897778952114 - val_loss: 4.0185e-04
Epoch 6/5002974303724178477 0.012159345589381672 0.0172461697897778952114 - val_loss: 4.0185e-04
Epoch 12/502974303724178477 0.012159345589381672 0.0172461697897778952114 - val_loss: 4.0185e-04
MPC : 0.0010204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
MPC : 0.0010204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5010204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 9/5010204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 15/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 18/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 18/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 1/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 1/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 1/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 4/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 4/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 7/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 7/5000204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 10/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 10/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 13/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 13/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 16/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 16/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 16/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 20/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
Epoch 20/500204816346292929 0.02273521691944573 0.03194497823804694952114 - val_loss: 4.0185e-04
C : 2.1350233866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
C : 2.1350233866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50233866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 5/50233866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 8/50233866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 11/5033866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 14/5033866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 17/5033866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 20/5033866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
Epoch 20/5033866962804e-05 0.0036706592229600156 0.0046206313277476282114 - val_loss: 4.0185e-04
ALGN : 198.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
ALGN : 198.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
ALGN : 198.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 5/50.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 7/50.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 9/50.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 9/50.6840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 12/506840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 12/506840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
Epoch 15/506840766604805 337.55827413714894 138.8741974766684377476282114 - val_loss: 4.0185e-04
ADSK : 0.0007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
ADSK : 0.0007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 7/50007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 16/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 19/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 19/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 4/50007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 7/50007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 10/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 13/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 16/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 19/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
Epoch 19/5007366651409834879 0.021837840625086154 0.027141575875094062114 - val_loss: 4.0185e-04
39/39 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
39/39 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step027141575875094062114 - val_loss: 4.0185e-04
GD : 0.00010436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
GD : 0.00010436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5010436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 6/5010436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 8/5010436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 10/500436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 13/500436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 15/500436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
Epoch 17/500436512200198106 0.008345699457192665 0.0102159249215125432114 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
50/50 [==============================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step102159249215125432114 - val_loss: 4.0185e-04
WELL : 88.44340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
WELL : 88.44340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
Epoch 4/5044340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
Epoch 6/5044340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
Epoch 8/5044340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
Epoch 11/504340753846029 76.15467809571827 -12.28872944274202415125432114 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 1ms/step944274202415125432114 - val_loss: 4.0185e-04
HOLX : 67.20838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
HOLX : 67.20838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 3/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 3/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 3/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 7/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 7/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 7/5020838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
Epoch 11/500838708235108 39.104489551489024 -28.1038975308620565125432114 - val_loss: 4.0185e-04
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
TGT : 0.00020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
Epoch 7/50020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
Epoch 7/50020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
Epoch 7/50020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
Epoch 11/5020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
Epoch 11/5020159522087959635 0.008189788022777676 0.014198423182860706114 - val_loss: 4.0185e-04
ITW : 0.0039242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
ITW : 0.0039242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5039242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
Epoch 5/5039242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
Epoch 7/5039242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
Epoch 10/509242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
Epoch 12/509242277599525165 0.05314106363772367 0.06264365698099462706114 - val_loss: 4.0185e-04
INTU : 0.00031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
INTU : 0.00031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
Epoch 6/500031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
Epoch 9/500031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
Epoch 15/50031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
Epoch 18/50031889541134944396 0.01192779698009007 0.017857642939353557114 - val_loss: 4.0185e-04
MKTX : 0.00024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
MKTX : 0.00024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 6/500024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 9/500024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 12/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 15/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 17/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 20/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 23/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 26/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
Epoch 26/50024103265012978664 0.011925237835188398 0.01552522625051843414 - val_loss: 4.0185e-04
MET : 61.74435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
MET : 61.74435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 3/504435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 5/504435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 5/504435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 8/504435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 8/504435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 11/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 13/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 13/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 16/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 18/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 18/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 21/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 23/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 23/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 23/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
Epoch 23/50435933868717 45.372272722481966 -16.37208661620520225051843414 - val_loss: 4.0185e-04
BBWI : 31.535973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
BBWI : 31.535973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 4/50535973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 7/50535973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 10/5035973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 13/5035973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 16/5035973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
Epoch 19/5035973298499012 67.64268451298028 36.10671121448126525051843414 - val_loss: 4.0185e-04
HSIC : 0.00019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
HSIC : 0.00019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
HSIC : 0.00019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 4/500019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 4/500019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 7/500019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 9/500019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 9/500019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 12/50019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 12/50019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 15/50019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
Epoch 15/50019842853578023583 0.010784622741372579 0.01408646640503699214 - val_loss: 4.0185e-04
68/68 [==============================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
68/68 [==============================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 1/50===========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 24/50==========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 33/50==========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
Epoch 42/50==========================] - 1s 1ms/step.01408646640503699214 - val_loss: 4.0185e-04
FTV : 0.00025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
FTV : 0.00025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 3/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 12/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 24/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 30/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 30/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 4/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 7/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 1/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 5/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 8/50025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 10/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
Epoch 13/5025897609633547064 0.012603662468516378 0.016092734271573324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
DGX : 0.00018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 8/50018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 20/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
Epoch 29/5018009122091298043 0.009682167957715902 0.013419807037099324214 - val_loss: 4.0185e-04
CNP : 4.1286995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
CNP : 4.1286995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5086995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 6/5086995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 9/5086995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 1/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 1/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 4/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 4/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 7/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 7/5006995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 10/506995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 10/506995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
Epoch 13/506995770152374e-05 0.004452575592038741 0.006425495760651653514 - val_loss: 4.0185e-04
AFL : 0.00030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
AFL : 0.00030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
Epoch 6/50030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
Epoch 9/50030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
Epoch 12/5030496681291778164 0.016022895967675053 0.017463299027325324514 - val_loss: 4.0185e-04
ON : 69.30526346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
ON : 69.30526346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50526346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 6/50526346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 8/50526346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 10/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 12/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 14/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 16/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 18/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 21/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 23/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
Epoch 25/5026346987364 21.77496326025647 -47.5303002096171799027325324514 - val_loss: 4.0185e-04
BSX : 0.000191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
BSX : 0.000191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 3/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 8/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 17/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 23/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 29/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 38/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 41/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 41/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 1/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 1/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 1/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 4/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 7/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 7/500191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 10/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 10/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 13/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 13/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 16/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 19/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 19/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 22/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 22/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 25/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 27/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 27/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
Epoch 27/50191905786327568 0.011171355646662903 0.01385300640032942524514 - val_loss: 4.0185e-04
CAH : 96.269997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
CAH : 96.269997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
Epoch 4/5069997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
Epoch 6/5069997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
Epoch 8/5069997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
Epoch 10/509997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
Epoch 12/509997 77.891288728539 -18.3787082714610081385300640032942524514 - val_loss: 4.0185e-04
51/51 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
51/51 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
51/51 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
48/48 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
48/48 [==============================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 4/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step385300640032942524514 - val_loss: 4.0185e-04
BIIB : 0.00010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
BIIB : 0.00010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 5/500010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 7/500010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 10/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 12/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 12/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 15/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 18/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
Epoch 20/50010582589642859289 0.007476620080937934 0.01028717144936317814 - val_loss: 4.0185e-04
ODFL : 0.0017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
ODFL : 0.0017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ODFL : 0.0017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
ODFL : 0.0017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 5/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 5/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 5/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 9/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 9/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
Epoch 9/50017471355034509425 0.03524823703652903 0.0417987500225897047814 - val_loss: 4.0185e-04
39/80 [=============>................] - ETA: 0s 0.0417987500225897047814 - val_loss: 4.0185e-04
39/80 [=============>................] - ETA: 0s 0.0417987500225897047814 - val_loss: 4.0185e-04
K : 51.569816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
K : 51.569816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 3/50816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 3/50816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 6/50816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 6/50816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 9/50816334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 11/5016334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 11/5016334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 14/5016334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
Epoch 14/5016334118016 55.096648051644856 3.52683171752683930225897047814 - val_loss: 4.0185e-04
KMB : 0.00025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
KMB : 0.00025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 3/50025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 6/50025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 6/50025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 6/50025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 10/5025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
Epoch 12/5025323847987478843 0.012118399533462615 0.015913468505476373814 - val_loss: 4.0185e-04
APD : 0.0013045678555542552 0.028088249069688907 0.0361188019673169373814 - val_loss: 4.0185e-04
APD : 0.0013045678555542552 0.028088249069688907 0.0361188019673169373814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5013045678555542552 0.028088249069688907 0.0361188019673169373814 - val_loss: 4.0185e-04
Epoch 9/5013045678555542552 0.028088249069688907 0.0361188019673169373814 - val_loss: 4.0185e-04
Epoch 15/503045678555542552 0.028088249069688907 0.0361188019673169373814 - val_loss: 4.0185e-04
19/19 [==============================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
19/19 [==============================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
Epoch 12/50==========================] - 0s 2ms/step361188019673169373814 - val_loss: 4.0185e-04
NEM : 0.00022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
NEM : 0.00022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
NEM : 0.00022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 4/50022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 4/50022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 7/50022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 9/50022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 9/50022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
Epoch 12/5022325734754514442 0.010722811526166642 0.014941798671684223814 - val_loss: 4.0185e-04
ALK : 0.0001892982952792902 0.00998742105292209 0.01375857170200781523814 - val_loss: 4.0185e-04
ALK : 0.0001892982952792902 0.00998742105292209 0.01375857170200781523814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 12/501892982952792902 0.00998742105292209 0.01375857170200781523814 - val_loss: 4.0185e-04
Epoch 27/501892982952792902 0.00998742105292209 0.01375857170200781523814 - val_loss: 4.0185e-04
7/7 [==============================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
7/7 [==============================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
7/7 [==============================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 2/50=========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 5/50=========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 5/50=========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 8/50=========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 8/50=========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 14/50========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
Epoch 14/50========================] - 0s 2ms/step01375857170200781523814 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
69/69 [==============================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step375857170200781523814 - val_loss: 4.0185e-04
DHR : 0.0001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
DHR : 0.0001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 25/501302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
Epoch 1/5001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
Epoch 1/5001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
Epoch 7/5001302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
Epoch 10/501302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
Epoch 13/501302823116961759 0.006668287121070505 0.0114141277238418843814 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
35/35 [==============================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
35/35 [==============================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step114141277238418843814 - val_loss: 4.0185e-04
DIS : 0.0002950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
DIS : 0.0002950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
Epoch 4/5002950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
Epoch 7/5002950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
Epoch 10/502950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
Epoch 13/502950881029259913 0.009725244701001284 0.0171781286211854653814 - val_loss: 4.0185e-04
WTW : 0.000315091153577624 0.012942085528657252 0.01775080712468095653814 - val_loss: 4.0185e-04
WTW : 0.000315091153577624 0.012942085528657252 0.01775080712468095653814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/500315091153577624 0.012942085528657252 0.01775080712468095653814 - val_loss: 4.0185e-04
Epoch 12/50315091153577624 0.012942085528657252 0.01775080712468095653814 - val_loss: 4.0185e-04
Epoch 18/50315091153577624 0.012942085528657252 0.01775080712468095653814 - val_loss: 4.0185e-04
CDW : 0.0001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
CDW : 0.0001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 3/5001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 6/5001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 6/5001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 9/5001399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 11/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 13/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 13/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 16/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 18/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 20/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 20/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 23/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 23/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 23/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
Epoch 23/501399264863126792 0.009371212779033529 0.0118290526380044153814 - val_loss: 4.0185e-04
MU : 72.2225556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
MU : 72.2225556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 4/5025556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 6/5025556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 8/5025556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 11/505556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 13/505556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 15/505556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
Epoch 15/505556311457 13.933815389871597 -58.2887402412741526380044153814 - val_loss: 4.0185e-04
COR : 194.839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
COR : 194.839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 3/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 6/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 6/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 9/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 9/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 9/50839996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 13/5039996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
Epoch 13/5039996 94.90555595809282 -99.9344400419072412741526380044153814 - val_loss: 4.0185e-04
TYL : 0.00019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
TYL : 0.00019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 7/50019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 13/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 16/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 19/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 25/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 28/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 31/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
Epoch 34/5019270577976531508 0.011533490474643342 0.013881850732712663814 - val_loss: 4.0185e-04
27/27 [==============================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
27/27 [==============================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
27/27 [==============================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 7/50===========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step013881850732712663814 - val_loss: 4.0185e-04
 1/50 [..............................] - ETA: 35step013881850732712663814 - val_loss: 4.0185e-04
DHI : 119.55454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
DHI : 119.55454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 3/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 3/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 3/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 3/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 9/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 9/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
Epoch 9/5055454036499638 36.318227022491335 -83.2363133425050432712663814 - val_loss: 4.0185e-04
PNR : 0.00021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
PNR : 0.00021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PNR : 0.00021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
PNR : 0.00021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 5/50021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 7/50021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 7/50021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 10/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 10/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 13/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 13/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 16/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
Epoch 16/5021838988824884596 0.009578656448103486 0.014778020444188253814 - val_loss: 4.0185e-04
COO : 0.00040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
COO : 0.00040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 3/50040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 6/50040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 6/50040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 9/50040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 11/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 11/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 14/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 16/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 16/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
Epoch 16/5040809409493419984 0.017314912514042668 0.020201338939144598814 - val_loss: 4.0185e-04
UHS : 131.66947897499077 133.20894062519073 1.539461650199967839144598814 - val_loss: 4.0185e-04
UHS : 131.66947897499077 133.20894062519073 1.539461650199967839144598814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5066947897499077 133.20894062519073 1.539461650199967839144598814 - val_loss: 4.0185e-04
Epoch 10/506947897499077 133.20894062519073 1.539461650199967839144598814 - val_loss: 4.0185e-04
Epoch 13/506947897499077 133.20894062519073 1.539461650199967839144598814 - val_loss: 4.0185e-04
22/22 [==============================] - 0s 1ms/step650199967839144598814 - val_loss: 4.0185e-04
22/22 [==============================] - 0s 1ms/step650199967839144598814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 5/50===========================] - 0s 1ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step650199967839144598814 - val_loss: 4.0185e-04
17/17 [==============================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
17/17 [==============================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step650199967839144598814 - val_loss: 4.0185e-04
RMD : 0.0002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
RMD : 0.0002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 3/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 3/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 3/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 7/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 7/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 7/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 7/5002283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 12/502283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 12/502283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
Epoch 12/502283626376465316 0.012485935142209312 0.0151116722319712728814 - val_loss: 4.0185e-04
EXC : 5.49062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
EXC : 5.49062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 9/50062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 15/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 18/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 24/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 27/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 33/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 39/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
Epoch 42/5062128073263e-05 0.005542179863468956 0.00740987265796965628814 - val_loss: 4.0185e-04
21/21 [==============================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
21/21 [==============================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step740987265796965628814 - val_loss: 4.0185e-04
PG : 0.0002993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
PG : 0.0002993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/502993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 7/502993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 9/502993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 12/50993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 18/50993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 18/50993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
Epoch 18/50993532796908088 0.01333491984032871 0.017301828796136228628814 - val_loss: 4.0185e-04
32/32 [==============================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
32/32 [==============================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 3/50===========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 19/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 23/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 23/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 26/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 26/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
Epoch 26/50==========================] - 0s 2ms/step301828796136228628814 - val_loss: 4.0185e-04
BK : 7.39721983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
BK : 7.39721983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5021983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
Epoch 6/5021983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
Epoch 9/5021983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
Epoch 12/501983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
Epoch 12/501983359514e-05 0.006589281917002399 0.008600709176338398628814 - val_loss: 4.0185e-04
STX : 0.00020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
STX : 0.00020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 3/50020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 5/50020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 5/50020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 8/50020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 10/5020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
Epoch 10/5020123058849837392 0.009601391643753747 0.014185576777077973814 - val_loss: 4.0185e-04
LHX : 0.00010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
LHX : 0.00010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
LHX : 0.00010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 4/50010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 4/50010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 7/50010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 7/50010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 10/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 10/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 13/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 13/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 16/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 18/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
Epoch 18/5010796099806463341 0.008379637066359927 0.010390428194479447814 - val_loss: 4.0185e-04
AOS : 0.00023414306700908182 0.010225156812306239 0.015301734117709725814 - val_loss: 4.0185e-04
AOS : 0.00023414306700908182 0.010225156812306239 0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 9/50023414306700908182 0.010225156812306239 0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 9/50023414306700908182 0.010225156812306239 0.015301734117709725814 - val_loss: 4.0185e-04
2/2 [==============================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 2/50=========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 2/50=========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 5/50=========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 8/50=========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
Epoch 11/50========================] - 0s 2ms/step0.015301734117709725814 - val_loss: 4.0185e-04
44/44 [==============================] - 0s 1ms/step015301734117709725814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step015301734117709725814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step015301734117709725814 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step015301734117709725814 - val_loss: 4.0185e-04
CFG : 0.0002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
CFG : 0.0002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
Epoch 3/5002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
Epoch 6/5002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
Epoch 9/5002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
Epoch 9/5002972379447646444 0.013469404625304963 0.0172405900352814035814 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
50/50 [==============================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/50===========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 17/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 8/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 8/500==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step172405900352814035814 - val_loss: 4.0185e-04
LLY : 8.507417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
LLY : 8.507417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/507417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 15/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 21/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 30/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 36/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 42/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
Epoch 48/50417776304633e-05 0.005154205057410891 0.0092235664340344155814 - val_loss: 4.0185e-04
13/13 [==============================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
13/13 [==============================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 2/50===========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 8/50===========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 14/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 20/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 26/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 4/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 10/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
Epoch 16/50==========================] - 0s 2ms/step092235664340344155814 - val_loss: 4.0185e-04
DUK : 0.00025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
DUK : 0.00025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
DUK : 0.00025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
DUK : 0.00025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 3/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 3/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 3/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
Epoch 6/50025458486226304214 0.01167822111240843 0.0159557156612620255814 - val_loss: 4.0185e-04
MMM : 9.678179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
MMM : 9.678179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/508179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
Epoch 5/508179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
Epoch 8/508179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
Epoch 10/50179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
Epoch 13/50179158193175e-05 0.006412146713238862 0.0098377737106487545814 - val_loss: 4.0185e-04
EQIX : 0.00017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
EQIX : 0.00017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/500017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 9/500017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 15/50017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 1/500017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 1/500017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/500017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 10/50017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 13/50017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 16/50017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
Epoch 16/50017036105557718037 0.009970477289971368 0.01305224331588943214 - val_loss: 4.0185e-04
CHTR : 0.0001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
CHTR : 0.0001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
CHTR : 0.0001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
Epoch 3/50001390372798613559 0.00932445209339634 0.0117914070348434633214 - val_loss: 4.0185e-04
IP : 0.0001759286729428208 0.010249522207691767 0.01326381064938808633214 - val_loss: 4.0185e-04
IP : 0.0001759286729428208 0.010249522207691767 0.01326381064938808633214 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 7/501759286729428208 0.010249522207691767 0.01326381064938808633214 - val_loss: 4.0185e-04
Epoch 16/50759286729428208 0.010249522207691767 0.01326381064938808633214 - val_loss: 4.0185e-04
Epoch 22/50759286729428208 0.010249522207691767 0.01326381064938808633214 - val_loss: 4.0185e-04
APTV : 0.00018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
APTV : 0.00018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
APTV : 0.00018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 3/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 3/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 6/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 6/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 9/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 9/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
Epoch 9/500018164302015071234 0.010555957202815774 0.01347750051570068914 - val_loss: 4.0185e-04
IFF : 0.0002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
IFF : 0.0002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
Epoch 5/5002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
Epoch 7/5002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
Epoch 9/5002351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
Epoch 12/502351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
Epoch 14/502351423375311231 0.012186788220653399 0.0153343515523520948914 - val_loss: 4.0185e-04
MCK : 0.00010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
MCK : 0.00010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
Epoch 5/50010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
Epoch 8/50010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
Epoch 10/5010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
Epoch 13/5010094845250715213 0.007536930225016702 0.010047310710192668914 - val_loss: 4.0185e-04
CHRW : 0.00014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
CHRW : 0.00014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/500014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
Epoch 7/500014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
Epoch 10/50014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
Epoch 12/50014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
Epoch 15/50014416100299662366 0.009166494842066044 0.01200670658409805914 - val_loss: 4.0185e-04
31/31 [==============================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
31/31 [==============================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 6/50===========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 9/50===========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 15/50==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 18/50==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 1/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 5/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 7/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 9/500==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 11/50==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
Epoch 13/50==========================] - 0s 1ms/step.01200670658409805914 - val_loss: 4.0185e-04
AZO : 0.00020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
AZO : 0.00020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 5/50020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 8/50020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 10/5020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 12/5020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 15/5020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
Epoch 17/5020317162965660676 0.011284142974014628 0.014253828596437056914 - val_loss: 4.0185e-04
IVZ : 0.00010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
IVZ : 0.00010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
IVZ : 0.00010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 4/50010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 4/50010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 7/50010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 9/50010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 9/50010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 12/5010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 12/5010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
Epoch 12/5010972594091429198 0.008066577055792848 0.010475015079430291914 - val_loss: 4.0185e-04
AMGN : 271.41388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
AMGN : 271.41388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
Epoch 4/50.41388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
Epoch 7/50.41388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
Epoch 10/5041388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
Epoch 13/5041388228912786 158.11835949968392 -113.29552278944394430291914 - val_loss: 4.0185e-04
UPS : 0.00024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
UPS : 0.00024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 5/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 7/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 9/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 9/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 12/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 14/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 16/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 18/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 20/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 22/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 24/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 26/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 28/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 30/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 32/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 34/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 34/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 34/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 34/5024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
Epoch 4/50024103078524080962 0.012416769322408525 0.015525166190440913914 - val_loss: 4.0185e-04
176/176 [==============================] - 1s 5ms/step - loss: 4.4137e-04 - val_loss: 5.6654e-04
Epoch 9/50=============================] - 1s 5ms/step - loss: 4.4137e-04 - val_loss: 5.6654e-04
Epoch 11/50============================] - 1s 5ms/step - loss: 4.4137e-04 - val_loss: 5.6654e-04
Epoch 14/50============================] - 1s 5ms/step - loss: 4.4137e-04 - val_loss: 5.6654e-04
DVA : 0.0002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
DVA : 0.0002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 5/5002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 7/5002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 9/5002461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 11/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 13/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 13/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 16/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 18/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 20/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 22/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 24/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 24/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 27/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
Epoch 29/502461729239158482 0.011399283709089979 0.015689898786029444e-04 - val_loss: 5.6654e-04
JCI : 2.5260319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
JCI : 2.5260319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5060319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 3/5060319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 6/5060319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 8/5060319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 8/5060319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 11/500319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
Epoch 11/500319974229396e-05 0.003920270033450054 0.005025964581473828-04 - val_loss: 5.6654e-04
81/81 [==============================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
81/81 [==============================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 1/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 1/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 1/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 1/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step005025964581473828-04 - val_loss: 5.6654e-04
IBM : 0.00014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
IBM : 0.00014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 3/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 3/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 7/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 9/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 9/50014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 12/5014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 14/5014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 14/5014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
Epoch 14/5014507347723199027 0.00934646448134589 0.0120446451683721378-04 - val_loss: 5.6654e-04
PCG : 16.868748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
PCG : 16.868748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 3/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 6/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 6/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 9/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 9/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 9/5068748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 13/508748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
Epoch 13/508748807814352 46.25717536877572 29.388426560961367683721378-04 - val_loss: 5.6654e-04
 1/69 [..............................] - ETA: 26s426560961367683721378-04 - val_loss: 5.6654e-04
 1/69 [..............................] - ETA: 26s426560961367683721378-04 - val_loss: 5.6654e-04
NUE : 151.28222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
NUE : 151.28222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 3/5028222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 5/5028222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 7/5028222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 7/5028222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 10/508222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
Epoch 10/508222629868026 49.298669560465484 -101.983556738214773721378-04 - val_loss: 5.6654e-04
NVR : 0.0002681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
NVR : 0.0002681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5002681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
Epoch 7/5002681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
Epoch 10/502681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
Epoch 13/502681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
Epoch 19/502681271767629518 0.01207384367883037 0.01637458936165886878-04 - val_loss: 5.6654e-04
ICE : 0.00019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
ICE : 0.00019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ICE : 0.00019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
ICE : 0.00019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 5/50019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 7/50019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 7/50019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 10/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 10/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 13/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 15/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 15/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
Epoch 15/5019132614043879912 0.01020787012247318 0.0138320692753759418-04 - val_loss: 5.6654e-04
AAPL : 176.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
AAPL : 176.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
Epoch 5/50.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
Epoch 7/50.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
Epoch 9/50.461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
Epoch 11/50461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
Epoch 13/50461228434522 33.008498601256015 -143.4527298332662753759418-04 - val_loss: 5.6654e-04
FAST : 0.0001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
FAST : 0.0001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
FAST : 0.0001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 4/50001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 4/50001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 7/50001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 7/50001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 10/5001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 10/5001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 13/5001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
Epoch 15/5001238160806594399 0.008111030049667149 0.011127267439018436-04 - val_loss: 5.6654e-04
HD : 0.00047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
HD : 0.00047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
HD : 0.00047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 4/5047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 4/5047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 7/5047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 9/5047065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 11/507065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 11/507065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 14/507065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
Epoch 16/507065262948308334 0.017359905492868172 0.0216945299438149476-04 - val_loss: 5.6654e-04
RJF : 0.00018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
RJF : 0.00018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
RJF : 0.00018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
RJF : 0.00018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
RJF : 0.00018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
Epoch 5/50018684867295735348 0.010422890067169923 0.013669260146670466-04 - val_loss: 5.6654e-04
RTX : 0.0003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
RTX : 0.0003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
RTX : 0.0003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 4/5003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 6/5003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 6/5003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 6/5003795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 10/503795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 12/503795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
Epoch 12/503795819007318768 0.015506922110233539 0.0194828617182352576-04 - val_loss: 5.6654e-04
CSX : 0.00016255841488940932 0.009369951301625755 0.012749839798578229-04 - val_loss: 5.6654e-04
CSX : 0.00016255841488940932 0.009369951301625755 0.012749839798578229-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50016255841488940932 0.009369951301625755 0.012749839798578229-04 - val_loss: 5.6654e-04
Epoch 6/50016255841488940932 0.009369951301625755 0.012749839798578229-04 - val_loss: 5.6654e-04
Epoch 12/5016255841488940932 0.009369951301625755 0.012749839798578229-04 - val_loss: 5.6654e-04
29/29 [==============================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
29/29 [==============================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
Epoch 14/50==========================] - 0s 1ms/step012749839798578229-04 - val_loss: 5.6654e-04
DLR : 0.00026618045076589496 0.012521672708618406 0.016315037565568229-04 - val_loss: 5.6654e-04
DLR : 0.00026618045076589496 0.012521672708618406 0.016315037565568229-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50026618045076589496 0.012521672708618406 0.016315037565568229-04 - val_loss: 5.6654e-04
Epoch 6/50026618045076589496 0.012521672708618406 0.016315037565568229-04 - val_loss: 5.6654e-04
Epoch 12/5026618045076589496 0.012521672708618406 0.016315037565568229-04 - val_loss: 5.6654e-04
26/26 [==============================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
26/26 [==============================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 2/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
Epoch 12/50==========================] - 0s 1ms/step016315037565568229-04 - val_loss: 5.6654e-04
PNC : 0.0004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
PNC : 0.0004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
Epoch 3/5004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
Epoch 6/5004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
Epoch 9/5004225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
Epoch 11/504225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
Epoch 11/504225560839564661 0.014668551636215077 0.0205561689999976929-04 - val_loss: 5.6654e-04
ESS : 217.4643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
ESS : 217.4643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ESS : 217.4643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
ESS : 217.4643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
Epoch 5/504643243246871 242.48049751178328 25.016173187096179999976929-04 - val_loss: 5.6654e-04
MSI : 298.50000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
MSI : 298.50000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5050000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 5/5050000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 7/5050000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 9/5050000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 11/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 13/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 15/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 17/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 17/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
Epoch 17/500000000000006 45.31032706797123 -253.1896729320288399976929-04 - val_loss: 5.6654e-04
GEN : 17.26111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
GEN : 17.26111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 4/506111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 7/506111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 10/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 13/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 13/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 1/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 1/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 1/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 5/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 5/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 8/500111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 10/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 10/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 13/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
Epoch 13/50111967419663 23.577813024520875 6.3166933503242438399976929-04 - val_loss: 5.6654e-04
JPM : 0.0007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
JPM : 0.0007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
JPM : 0.0007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 4/5007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 4/5007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 7/5007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 7/5007719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 10/507719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 12/507719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
Epoch 12/507719056005215864 0.02288920329215488 0.02778318917118023929-04 - val_loss: 5.6654e-04
AMAT : 0.00018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
AMAT : 0.00018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
AMAT : 0.00018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 4/500018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 4/500018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 7/500018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 7/500018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 10/50018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
Epoch 10/50018645086549367123 0.009349237472838475 0.01365470122315648604 - val_loss: 5.6654e-04
TT : 0.00018182816070226197 0.011469489679979303 0.0134843672711129458604 - val_loss: 5.6654e-04
TT : 0.00018182816070226197 0.011469489679979303 0.0134843672711129458604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5018182816070226197 0.011469489679979303 0.0134843672711129458604 - val_loss: 5.6654e-04
Epoch 10/508182816070226197 0.011469489679979303 0.0134843672711129458604 - val_loss: 5.6654e-04
25/25 [==============================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
25/25 [==============================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 14/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 20/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 23/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 26/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 29/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 35/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
27/27 [==============================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
27/27 [==============================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 5/50===========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
Epoch 17/50==========================] - 0s 1ms/step134843672711129458604 - val_loss: 5.6654e-04
ZTS : 0.00012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
ZTS : 0.00012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 6/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 6/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 9/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 12/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 15/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 1/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 1/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 4/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 4/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 7/50012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 10/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 12/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 14/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 16/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 18/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 20/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 22/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 24/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
Epoch 24/5012487467944310694 0.008984714005938822 0.011174733976390988604 - val_loss: 5.6654e-04
EOG : 127.07112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
EOG : 127.07112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 3/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 6/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 8/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 8/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 8/5007112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
Epoch 12/507112802820428 102.95033790450543 -24.1207901236988536390988604 - val_loss: 5.6654e-04
HSY : 0.0001785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
HSY : 0.0001785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5001785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
Epoch 6/5001785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
Epoch 9/5001785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
Epoch 12/501785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
Epoch 15/501785517447078356 0.008247201925079061 0.0133623255725878648604 - val_loss: 5.6654e-04
PFG : 0.00017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
PFG : 0.00017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PFG : 0.00017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 4/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 4/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 4/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 4/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 9/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 9/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
Epoch 9/50017270736731001958 0.009894088275902123 0.013141817504060068604 - val_loss: 5.6654e-04
BDX : 0.00016972829627640135 0.009520816118420154 0.013027981281702909604 - val_loss: 5.6654e-04
BDX : 0.00016972829627640135 0.009520816118420154 0.013027981281702909604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50016972829627640135 0.009520816118420154 0.013027981281702909604 - val_loss: 5.6654e-04
Epoch 9/50016972829627640135 0.009520816118420154 0.013027981281702909604 - val_loss: 5.6654e-04
Epoch 12/5016972829627640135 0.009520816118420154 0.013027981281702909604 - val_loss: 5.6654e-04
WBD : 0.00039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
WBD : 0.00039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
WBD : 0.00039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
WBD : 0.00039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 5/50039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 7/50039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 7/50039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 7/50039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 11/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 11/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 14/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 16/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 16/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 19/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 19/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 22/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 22/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 25/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 27/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 27/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 30/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
Epoch 30/5039896025037123797 0.012926080471254063 0.019973989345427167604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
INTC : 7.474286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 11/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 11/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 11/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 11/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 11/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 1/5004286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 1/5004286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 4/5004286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 7/5004286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 9/5004286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
Epoch 12/504286150592797e-05 0.006277910686070826 0.008645395393267337604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
PKG : 0.00015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
Epoch 9/50015885322712316409 0.010156518432076189 0.012603698946069924604 - val_loss: 5.6654e-04
MRO : 26.376944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
MRO : 26.376944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 4/5076944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 6/5076944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 9/5076944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 12/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 14/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 17/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 19/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 22/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 25/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 27/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 30/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 33/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
Epoch 35/506944180242987 33.1137415661034 6.73679738586041398946069924604 - val_loss: 5.6654e-04
FE : 9.756084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
FE : 9.756084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
FE : 9.756084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
FE : 9.756084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
FE : 9.756084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
Epoch 6/50084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
Epoch 6/50084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
Epoch 9/50084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
Epoch 9/50084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
Epoch 9/50084793507612e-05 0.005935099078527739 0.00987728950345569824604 - val_loss: 5.6654e-04
WMT : 0.0001956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
WMT : 0.0001956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5001956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
Epoch 6/5001956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
Epoch 9/5001956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
Epoch 11/501956273444856077 0.011074370200623764 0.0139866845422926344604 - val_loss: 5.6654e-04
50/50 [==============================] - 0s 1ms/step139866845422926344604 - val_loss: 5.6654e-04
50/50 [==============================] - 0s 1ms/step139866845422926344604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 9/50===========================] - 0s 1ms/step139866845422926344604 - val_loss: 5.6654e-04
Epoch 39/50==========================] - 0s 1ms/step139866845422926344604 - val_loss: 5.6654e-04
3/3 [==============================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
3/3 [==============================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 5/50=========================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
Epoch 8/50=========================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
Epoch 11/50========================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
Epoch 11/50========================] - 0s 2ms/stepep139866845422926344604 - val_loss: 5.6654e-04
PODD : 0.0003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
PODD : 0.0003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 3/50003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 6/50003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 9/50003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 12/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 15/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 18/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 18/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 21/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 24/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 27/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 30/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 33/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 33/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 36/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 39/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 42/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 45/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 48/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
Epoch 48/5003079427581984474 0.013411359805241474 0.017548297871829262604 - val_loss: 5.6654e-04
42/42 [==============================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
42/42 [==============================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 2/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
29/29 [==============================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
29/29 [==============================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 0s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
41/41 [==============================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
41/41 [==============================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 1s 1ms/step017548297871829262604 - val_loss: 5.6654e-04
DLTR : 0.00016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
DLTR : 0.00016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
DLTR : 0.00016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 3/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 3/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 6/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 6/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 9/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 9/500016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 12/50016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 12/50016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 15/50016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 15/50016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
Epoch 15/50016950971328806244 0.010088844326034107 0.01301958959752812604 - val_loss: 5.6654e-04
GIS : 65.48343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
GIS : 65.48343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/508343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
Epoch 5/508343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
Epoch 8/508343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
Epoch 10/50343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
Epoch 12/50343152412947 53.178302474032755 -12.30512905009671759752812604 - val_loss: 5.6654e-04
KMX : 0.0002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
KMX : 0.0002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 6/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 9/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 15/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 21/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 1/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 1/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 7/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 10/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 13/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 1/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 1/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 7/5002667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 10/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 13/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 16/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
Epoch 19/502667664798469136 0.011290718849205078 0.0163329874746450982604 - val_loss: 5.6654e-04
35/35 [==============================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
35/35 [==============================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 2/50===========================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 5/50===========================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 8/50===========================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 11/50==========================] - 0s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
32/32 [==============================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
32/32 [==============================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
32/32 [==============================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 4/50===========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 6/50===========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 6/50===========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 6/50===========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 10/50==========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 12/50==========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 12/50==========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 12/50==========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
Epoch 12/50==========================] - 1s 1ms/step163329874746450982604 - val_loss: 5.6654e-04
CAG : 27.59313643427913 26.958966013221858 -0.634170421057273646450982604 - val_loss: 5.6654e-04
