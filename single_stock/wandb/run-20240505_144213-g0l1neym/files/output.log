EIX
X_train shape: (10152, 8, 1)
y_train shape: (10152,)
X_test shape: (2538, 8, 1)
y_test shape: (2538,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
318/318 [==============================] - 4s 6ms/step - loss: 0.0035 - val_loss: 2.1456e-04
Epoch 2/50
318/318 [==============================] - 2s 5ms/step - loss: 6.3706e-04 - val_loss: 3.9424e-04
Epoch 3/50
318/318 [==============================] - 1s 5ms/step - loss: 5.2414e-04 - val_loss: 0.0028
Epoch 4/50
318/318 [==============================] - 2s 5ms/step - loss: 4.3868e-04 - val_loss: 9.9066e-04
Epoch 5/50
318/318 [==============================] - 1s 5ms/step - loss: 4.0726e-04 - val_loss: 1.4530e-04
Epoch 6/50
318/318 [==============================] - 2s 5ms/step - loss: 3.6044e-04 - val_loss: 1.1553e-04
Epoch 7/50
318/318 [==============================] - 2s 5ms/step - loss: 3.3595e-04 - val_loss: 8.9547e-04
Epoch 8/50
318/318 [==============================] - 2s 5ms/step - loss: 3.1234e-04 - val_loss: 0.0012
Epoch 9/50
318/318 [==============================] - 2s 5ms/step - loss: 3.1655e-04 - val_loss: 6.9451e-04
Epoch 10/50
318/318 [==============================] - 1s 5ms/step - loss: 2.7629e-04 - val_loss: 0.0014
Epoch 11/50
318/318 [==============================] - 2s 5ms/step - loss: 2.9059e-04 - val_loss: 4.6272e-04
Epoch 12/50
318/318 [==============================] - 1s 5ms/step - loss: 2.8275e-04 - val_loss: 4.8748e-04
Epoch 13/50
318/318 [==============================] - 2s 5ms/step - loss: 2.7480e-04 - val_loss: 0.0010
Epoch 14/50
318/318 [==============================] - 2s 5ms/step - loss: 2.6094e-04 - val_loss: 9.9178e-04
Epoch 15/50
318/318 [==============================] - 2s 5ms/step - loss: 2.8067e-04 - val_loss: 0.0073
Epoch 16/50
318/318 [==============================] - 2s 5ms/step - loss: 2.7677e-04 - val_loss: 0.0017
80/80 [==============================] - 0s 1ms/step
80/80 [==============================] - 0s 1ms/step
EIX : 0.00011553072590398845 0.007469909461639587 0.010748522033469925
EIX : 63.35751310329923 44.49019209991956 -18.867321003379672
Data for EIX appended successfully.
KDP
X_train shape: (3082, 8, 1)
y_train shape: (3082,)
X_test shape: (771, 8, 1)
y_test shape: (771,)
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50
97/97 [==============================] - 3s 9ms/step - loss: 0.0163 - val_loss: 5.6910e-04
Epoch 2/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 4.9125e-04
Epoch 3/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 4.4096e-04
Epoch 4/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 3.6714e-04
Epoch 5/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 3.1161e-04
Epoch 6/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 2.5084e-04
Epoch 7/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 2.1703e-04
Epoch 8/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 2.0277e-04
Epoch 9/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 1.5860e-04
Epoch 10/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 1.3195e-04
Epoch 11/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 2.3158e-04
Epoch 12/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 1.2080e-04
Epoch 13/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 1.2443e-04
Epoch 14/50
97/97 [==============================] - 0s 5ms/step - loss: 9.5435e-04 - val_loss: 1.0702e-04
Epoch 15/50
97/97 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 3.1022e-04
Epoch 16/50
97/97 [==============================] - 1s 5ms/step - loss: 8.9184e-04 - val_loss: 1.0487e-04
Epoch 17/50
97/97 [==============================] - 0s 5ms/step - loss: 8.3046e-04 - val_loss: 9.8519e-05
Epoch 18/50
97/97 [==============================] - 0s 5ms/step - loss: 8.7500e-04 - val_loss: 2.4524e-04
Epoch 19/50
97/97 [==============================] - 0s 5ms/step - loss: 8.3439e-04 - val_loss: 2.0621e-04
Epoch 20/50
97/97 [==============================] - 0s 5ms/step - loss: 8.3026e-04 - val_loss: 1.1313e-04
Epoch 21/50
97/97 [==============================] - 1s 5ms/step - loss: 8.6585e-04 - val_loss: 7.9265e-05
Epoch 22/50
97/97 [==============================] - 0s 5ms/step - loss: 8.3936e-04 - val_loss: 6.0288e-04
Epoch 23/50
97/97 [==============================] - 0s 5ms/step - loss: 8.4384e-04 - val_loss: 1.8469e-04
Epoch 24/50
97/97 [==============================] - 0s 5ms/step - loss: 7.1333e-04 - val_loss: 1.9547e-04
Epoch 25/50
97/97 [==============================] - 0s 5ms/step - loss: 7.6807e-04 - val_loss: 9.6349e-05
Epoch 26/50
97/97 [==============================] - 0s 5ms/step - loss: 7.9902e-04 - val_loss: 1.6591e-04
Epoch 27/50
97/97 [==============================] - 0s 5ms/step - loss: 6.8170e-04 - val_loss: 7.3471e-05
Epoch 28/50
97/97 [==============================] - 0s 5ms/step - loss: 6.8884e-04 - val_loss: 2.8158e-04
Epoch 29/50
97/97 [==============================] - 0s 5ms/step - loss: 7.4967e-04 - val_loss: 6.8716e-05
Epoch 30/50
Epoch 34/50==========================] - 0s 5ms/step - loss: 6.7496e-04 - val_loss: 1.2754e-04
Epoch 31/50
97/97 [==============================] - 1s 6ms/step - loss: 6.7567e-04 - val_loss: 7.9098e-05
Epoch 32/50
97/97 [==============================] - 1s 5ms/step - loss: 6.1151e-04 - val_loss: 1.1107e-04
Epoch 33/50
97/97 [==============================] - 1s 5ms/step - loss: 6.0798e-04 - val_loss: 7.2405e-05
Epoch 34/50==========================] - 0s 5ms/step - loss: 6.7496e-04 - val_loss: 1.2754e-04
12/97 [==>...........................] - ETA: 0s - loss: 7.4743e-04e-04 - val_loss: 2.5256e-04
Epoch 35/50
97/97 [==============================] - 0s 5ms/step - loss: 6.3205e-04 - val_loss: 5.7782e-04
Epoch 36/50
97/97 [==============================] - 0s 5ms/step - loss: 6.0138e-04 - val_loss: 6.4735e-05
Epoch 37/50
97/97 [==============================] - 1s 5ms/step - loss: 5.6494e-04 - val_loss: 1.2041e-04
Epoch 38/50
25/97 [======>.......................] - ETA: 0s - loss: 5.4892e-04e-04 - val_loss: 8.1333e-05
Epoch 39/50
97/97 [==============================] - 0s 5ms/step - loss: 5.4453e-04 - val_loss: 1.2418e-04
Epoch 40/50
97/97 [==============================] - 1s 5ms/step - loss: 5.5728e-04 - val_loss: 8.5849e-05
Epoch 41/50
97/97 [==============================] - 0s 5ms/step - loss: 5.4118e-04 - val_loss: 7.3237e-05
Epoch 42/50
49/97 [==============>...............] - ETA: 0s - loss: 5.0707e-04e-04 - val_loss: 8.9039e-05
Epoch 43/50
97/97 [==============================] - 0s 5ms/step - loss: 5.1161e-04 - val_loss: 6.7607e-05
Epoch 44/50
97/97 [==============================] - 1s 5ms/step - loss: 4.6639e-04 - val_loss: 2.5009e-04
Epoch 45/50
97/97 [==============================] - 0s 5ms/step - loss: 5.1174e-04 - val_loss: 1.1229e-04
Epoch 46/50
97/97 [==============================] - 0s 5ms/step - loss: 5.1252e-04 - val_loss: 6.0415e-05
Epoch 47/50
97/97 [==============================] - 0s 5ms/step - loss: 5.4651e-04 - val_loss: 1.1157e-04
Epoch 48/50
97/97 [==============================] - 0s 5ms/step - loss: 6.0144e-04 - val_loss: 8.8780e-05
Epoch 49/50
69/97 [====================>.........] - ETA: 0s - loss: 5.0464e-04
y_train shape: (7733,)===============] - 0s 5ms/step - loss: 5.1893e-04 - val_loss: 5.8242e-05
25/25 [==============================] - 0s 2ms/step
25/25 [==============================] - 0s 1ms/step
KDP : 5.824218176680315e-05 0.004778496305942108 0.007631656554562918
KDP : 31.73653911431579 30.49080605285042 -1.2457330614653692
Data for KDP appended successfully.
BBY
X_train shape: (7733, 8, 1)
y_train shape: (7733,)===============] - 0s 5ms/step - loss: 5.1893e-04 - val_loss: 5.8242e-05
X_test shape: (1934, 8, 1)
y_test shape: (1934,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
242/242 [==============================] - 3s 7ms/step - loss: 0.0022 - val_loss: 4.3119e-04
Epoch 2/50
242/242 [==============================] - 1s 5ms/step - loss: 4.0767e-04 - val_loss: 4.9215e-04
Epoch 3/50
242/242 [==============================] - 1s 5ms/step - loss: 3.1862e-04 - val_loss: 1.8193e-04
Epoch 4/50
242/242 [==============================] - 1s 5ms/step - loss: 2.6730e-04 - val_loss: 3.5441e-04
Epoch 5/50
242/242 [==============================] - 1s 5ms/step - loss: 2.3903e-04 - val_loss: 3.2188e-04
Epoch 6/50
242/242 [==============================] - 1s 5ms/step - loss: 2.0678e-04 - val_loss: 0.0016
Epoch 7/50
242/242 [==============================] - 2s 10ms/step - loss: 2.0086e-04 - val_loss: 0.0011
Epoch 8/50
242/242 [==============================] - 2s 7ms/step - loss: 1.9016e-04 - val_loss: 0.0040
Epoch 9/50
242/242 [==============================] - 1s 5ms/step - loss: 1.8408e-04 - val_loss: 0.0030
Epoch 10/50
242/242 [==============================] - 1s 5ms/step - loss: 1.7938e-04 - val_loss: 0.0036
Epoch 11/50
242/242 [==============================] - 1s 5ms/step - loss: 1.6926e-04 - val_loss: 0.0062
Epoch 12/50
242/242 [==============================] - 1s 5ms/step - loss: 1.6920e-04 - val_loss: 0.0063
Epoch 13/50
242/242 [==============================] - 1s 5ms/step - loss: 1.6170e-04 - val_loss: 0.0059
61/61 [==============================] - 0s 1ms/step
61/61 [==============================] - 0s 1ms/step
BBY : 0.00018192720871736923 0.01110151505321969 0.0134880394690025
BBY : 66.81068546087744 30.749748773613913 -36.06093668726352
Data for BBY appended successfully.
GEN
X_train shape: (6888, 8, 1)
y_train shape: (6888,)
X_test shape: (1722, 8, 1)
y_test shape: (1722,)
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50
216/216 [==============================] - 3s 7ms/step - loss: 0.0093 - val_loss: 7.7699e-04
Epoch 2/50
216/216 [==============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 0.0011
Epoch 3/50
216/216 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 0.0011
Epoch 4/50
216/216 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 1.3960e-04
Epoch 5/50
216/216 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 2.5138e-04
Epoch 6/50
216/216 [==============================] - 1s 5ms/step - loss: 9.7074e-04 - val_loss: 1.6980e-04
Epoch 7/50
216/216 [==============================] - 1s 5ms/step - loss: 9.5538e-04 - val_loss: 7.3506e-04
Epoch 8/50
216/216 [==============================] - 1s 5ms/step - loss: 9.3106e-04 - val_loss: 8.3185e-04
Epoch 9/50
216/216 [==============================] - 1s 5ms/step - loss: 8.1912e-04 - val_loss: 2.6642e-04
Epoch 10/50
216/216 [==============================] - 1s 5ms/step - loss: 7.6615e-04 - val_loss: 8.7443e-04
Epoch 11/50
216/216 [==============================] - 1s 5ms/step - loss: 7.9818e-04 - val_loss: 1.3664e-04
Epoch 12/50
216/216 [==============================] - 1s 5ms/step - loss: 7.1317e-04 - val_loss: 4.6069e-04
Epoch 13/50
216/216 [==============================] - 1s 5ms/step - loss: 6.5817e-04 - val_loss: 3.1196e-04
Epoch 14/50
216/216 [==============================] - 1s 5ms/step - loss: 6.7093e-04 - val_loss: 7.2452e-04
Epoch 15/50
216/216 [==============================] - 1s 5ms/step - loss: 6.0027e-04 - val_loss: 1.3811e-04
Epoch 16/50
216/216 [==============================] - 1s 5ms/step - loss: 6.0838e-04 - val_loss: 3.3607e-04
Epoch 17/50
216/216 [==============================] - 1s 6ms/step - loss: 5.8882e-04 - val_loss: 2.0885e-04
Epoch 18/50
216/216 [==============================] - 1s 5ms/step - loss: 5.9497e-04 - val_loss: 2.9503e-04
Epoch 19/50
216/216 [==============================] - 1s 6ms/step - loss: 5.6480e-04 - val_loss: 8.2973e-04
Epoch 20/50
216/216 [==============================] - 1s 5ms/step - loss: 5.4986e-04 - val_loss: 4.1267e-04
Epoch 21/50
216/216 [==============================] - 1s 5ms/step - loss: 5.3760e-04 - val_loss: 2.1439e-04
54/54 [==============================] - 0s 1ms/step
54/54 [==============================] - 0s 1ms/step
GEN : 0.00013663833320012734 0.008661155057027066 0.011689240060847726
GEN : 17.26111967419663 23.689018322229384 6.427898648032752
Data for GEN appended successfully.
WBA
X_train shape: (8763, 8, 1)
y_train shape: (8763,)
X_test shape: (2191, 8, 1)
y_test shape: (2191,)
Epoch 1/50
 12/274 [>.............................] - ETA: 1s - loss: 0.0426

274/274 [==============================] - 3s 6ms/step - loss: 0.0042 - val_loss: 5.3760e-04
Epoch 2/50
274/274 [==============================] - 1s 5ms/step - loss: 8.0114e-04 - val_loss: 0.0011
Epoch 3/50
274/274 [==============================] - 1s 5ms/step - loss: 6.4427e-04 - val_loss: 4.5594e-04
Epoch 4/50
274/274 [==============================] - 1s 5ms/step - loss: 5.5972e-04 - val_loss: 9.4207e-04
Epoch 5/50
274/274 [==============================] - 1s 5ms/step - loss: 4.8902e-04 - val_loss: 0.0012
Epoch 6/50
274/274 [==============================] - 1s 5ms/step - loss: 4.9203e-04 - val_loss: 1.9664e-04
Epoch 7/50
274/274 [==============================] - 1s 5ms/step - loss: 4.5048e-04 - val_loss: 4.0258e-04
Epoch 8/50
274/274 [==============================] - 1s 5ms/step - loss: 4.1055e-04 - val_loss: 0.0015
Epoch 9/50
274/274 [==============================] - 1s 5ms/step - loss: 3.5581e-04 - val_loss: 1.9015e-04
Epoch 10/50
274/274 [==============================] - 1s 5ms/step - loss: 3.8525e-04 - val_loss: 4.5083e-04
Epoch 11/50
274/274 [==============================] - 1s 5ms/step - loss: 4.3107e-04 - val_loss: 4.4412e-04
Epoch 12/50
274/274 [==============================] - 1s 5ms/step - loss: 3.4665e-04 - val_loss: 0.0013
Epoch 13/50
274/274 [==============================] - 1s 5ms/step - loss: 3.4358e-04 - val_loss: 4.0146e-04
Epoch 14/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2755e-04 - val_loss: 1.4151e-04
Epoch 15/50
274/274 [==============================] - 1s 5ms/step - loss: 3.6335e-04 - val_loss: 3.1988e-04
Epoch 16/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3432e-04 - val_loss: 2.9310e-04
Epoch 17/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3535e-04 - val_loss: 1.6618e-04
Epoch 18/50
274/274 [==============================] - 1s 5ms/step - loss: 3.0353e-04 - val_loss: 0.0017
Epoch 19/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1422e-04 - val_loss: 2.2405e-04
Epoch 20/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2668e-04 - val_loss: 6.0828e-04
Epoch 21/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2119e-04 - val_loss: 9.4768e-04
Epoch 22/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2259e-04 - val_loss: 2.8313e-04
Epoch 23/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1491e-04 - val_loss: 1.4116e-04
Epoch 24/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3091e-04 - val_loss: 1.3956e-04
Epoch 25/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3151e-04 - val_loss: 2.7150e-04
Epoch 26/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2659e-04 - val_loss: 7.6485e-04
Epoch 27/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1291e-04 - val_loss: 1.5932e-04
Epoch 28/50
274/274 [==============================] - 1s 5ms/step - loss: 3.2671e-04 - val_loss: 1.6694e-04
Epoch 29/50
274/274 [==============================] - 1s 5ms/step - loss: 3.0196e-04 - val_loss: 1.8779e-04
Epoch 30/50
274/274 [==============================] - 1s 5ms/step - loss: 2.9867e-04 - val_loss: 1.2641e-04
Epoch 31/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3480e-04 - val_loss: 2.9343e-04
Epoch 32/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1092e-04 - val_loss: 6.0916e-04
Epoch 33/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1573e-04 - val_loss: 3.0347e-04
Epoch 34/50
274/274 [==============================] - 1s 5ms/step - loss: 3.0720e-04 - val_loss: 6.2541e-04
Epoch 35/50
274/274 [==============================] - 1s 5ms/step - loss: 3.3874e-04 - val_loss: 5.1156e-04
Epoch 36/50
274/274 [==============================] - 2s 5ms/step - loss: 3.1444e-04 - val_loss: 2.7205e-04
Epoch 37/50
274/274 [==============================] - 1s 5ms/step - loss: 3.1923e-04 - val_loss: 2.1757e-04
Epoch 38/50
274/274 [==============================] - 1s 5ms/step - loss: 3.0495e-04 - val_loss: 1.7074e-04
Epoch 39/50
274/274 [==============================] - 1s 5ms/step - loss: 3.0936e-04 - val_loss: 3.0811e-04
Epoch 40/50
274/274 [==============================] - 2s 5ms/step - loss: 3.1972e-04 - val_loss: 6.6169e-04
69/69 [==============================] - 0s 1ms/step
69/69 [==============================] - 0s 2ms/step
WBA : 0.00012641484191651457 0.008090899195642114 0.01124343550328433
WBA : 22.006653472467608 79.14284122440863 57.13618775194102
Data for WBA appended successfully.
LVS
X_train shape: (3764, 8, 1)
y_train shape: (3764,)
X_test shape: (942, 8, 1)
y_test shape: (942,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
118/118 [==============================] - 3s 9ms/step - loss: 0.0123 - val_loss: 6.3247e-04
Epoch 2/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0024 - val_loss: 2.9818e-04
Epoch 3/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0017 - val_loss: 1.3559e-04
Epoch 4/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 1.8341e-04
Epoch 5/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 1.8934e-04
Epoch 6/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 7.9636e-05
Epoch 7/50
118/118 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 8.7431e-05
Epoch 8/50
118/118 [==============================] - 1s 5ms/step - loss: 9.2320e-04 - val_loss: 1.8196e-04
Epoch 9/50
118/118 [==============================] - 1s 5ms/step - loss: 9.3460e-04 - val_loss: 1.3033e-04
Epoch 10/50
118/118 [==============================] - 1s 5ms/step - loss: 9.0898e-04 - val_loss: 9.8575e-05
Epoch 11/50
118/118 [==============================] - 1s 5ms/step - loss: 8.1113e-04 - val_loss: 1.7149e-04
Epoch 12/50
118/118 [==============================] - 1s 5ms/step - loss: 7.8973e-04 - val_loss: 3.2720e-04
Epoch 13/50
118/118 [==============================] - 1s 6ms/step - loss: 8.4044e-04 - val_loss: 9.9451e-05
Epoch 14/50
118/118 [==============================] - 1s 5ms/step - loss: 7.0667e-04 - val_loss: 5.9455e-05
Epoch 15/50
118/118 [==============================] - 1s 5ms/step - loss: 6.7660e-04 - val_loss: 6.1677e-05
Epoch 16/50
118/118 [==============================] - 1s 5ms/step - loss: 6.2006e-04 - val_loss: 5.8034e-05
Epoch 17/50
118/118 [==============================] - 1s 5ms/step - loss: 5.9964e-04 - val_loss: 7.6363e-05
Epoch 18/50
118/118 [==============================] - 1s 5ms/step - loss: 5.5230e-04 - val_loss: 7.6857e-05
Epoch 19/50
118/118 [==============================] - 1s 5ms/step - loss: 5.7877e-04 - val_loss: 3.4569e-04
Epoch 20/50
118/118 [==============================] - 1s 5ms/step - loss: 5.5393e-04 - val_loss: 6.0685e-05
Epoch 21/50
118/118 [==============================] - 1s 5ms/step - loss: 5.0882e-04 - val_loss: 7.3888e-05
Epoch 22/50
118/118 [==============================] - 1s 5ms/step - loss: 4.8581e-04 - val_loss: 1.0373e-04
Epoch 23/50
118/118 [==============================] - 1s 5ms/step - loss: 4.9972e-04 - val_loss: 2.2098e-04
Epoch 24/50
118/118 [==============================] - 1s 5ms/step - loss: 4.9796e-04 - val_loss: 8.5627e-05
Epoch 25/50
118/118 [==============================] - 1s 5ms/step - loss: 5.5915e-04 - val_loss: 2.2838e-04
Epoch 26/50
118/118 [==============================] - 1s 5ms/step - loss: 4.3960e-04 - val_loss: 1.0784e-04
30/30 [==============================] - 1s 1ms/step
30/30 [==============================] - 0s 1ms/step
LVS : 5.8034112110175595e-05 0.005904576580019503 0.007618012346417903
LVS : 48.01639240081587 66.21818750895285 18.201795108136984
Data for LVS appended successfully.
HCA
X_train shape: (2509, 8, 1)
y_train shape: (2509,)
X_test shape: (628, 8, 1)
y_test shape: (628,)
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50
79/79 [==============================] - 2s 10ms/step - loss: 0.0082 - val_loss: 0.0014
Epoch 2/50
79/79 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 6.0234e-04
Epoch 3/50
79/79 [==============================] - 0s 5ms/step - loss: 9.2747e-04 - val_loss: 0.0029
Epoch 4/50
79/79 [==============================] - 0s 5ms/step - loss: 8.4987e-04 - val_loss: 0.0014
Epoch 5/50
79/79 [==============================] - 0s 5ms/step - loss: 6.6941e-04 - val_loss: 0.0037
Epoch 6/50
79/79 [==============================] - 0s 5ms/step - loss: 5.5766e-04 - val_loss: 0.0012
Epoch 7/50
79/79 [==============================] - 0s 5ms/step - loss: 5.0180e-04 - val_loss: 1.8504e-04
Epoch 8/50
79/79 [==============================] - 0s 5ms/step - loss: 4.9493e-04 - val_loss: 0.0013
Epoch 9/50
79/79 [==============================] - 0s 5ms/step - loss: 5.3760e-04 - val_loss: 1.8254e-04
Epoch 10/50
79/79 [==============================] - 0s 5ms/step - loss: 4.1415e-04 - val_loss: 2.5073e-04
Epoch 11/50
79/79 [==============================] - 0s 5ms/step - loss: 4.8249e-04 - val_loss: 3.3126e-04
Epoch 12/50
79/79 [==============================] - 0s 6ms/step - loss: 3.8448e-04 - val_loss: 0.0010
Epoch 13/50
79/79 [==============================] - 0s 5ms/step - loss: 4.0240e-04 - val_loss: 0.0036
Epoch 14/50
79/79 [==============================] - 0s 5ms/step - loss: 3.6227e-04 - val_loss: 9.4285e-04
Epoch 15/50
79/79 [==============================] - 0s 5ms/step - loss: 3.2473e-04 - val_loss: 0.0013
Epoch 16/50
79/79 [==============================] - 0s 5ms/step - loss: 3.5553e-04 - val_loss: 2.8784e-04
Epoch 17/50
79/79 [==============================] - 0s 5ms/step - loss: 3.8412e-04 - val_loss: 6.2341e-04
Epoch 18/50
79/79 [==============================] - 0s 5ms/step - loss: 2.9810e-04 - val_loss: 8.6694e-04
Epoch 19/50
79/79 [==============================] - 0s 5ms/step - loss: 3.6091e-04 - val_loss: 0.0031
20/20 [==============================] - 0s 1ms/step
20/20 [==============================] - 0s 1ms/step
HCA : 0.0001825433908375928 0.010043301210706861 0.013510861957609989
HCA : 231.19578916960435 215.35165147046794 -15.844137699136411
Data for HCA appended successfully.
AJG
X_train shape: (7896, 8, 1)
y_train shape: (7896,)
X_test shape: (1975, 8, 1)
y_test shape: (1975,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
247/247 [==============================] - 3s 6ms/step - loss: 5.4814e-04 - val_loss: 0.0024
Epoch 2/50
247/247 [==============================] - 1s 5ms/step - loss: 8.3652e-05 - val_loss: 0.0017
Epoch 3/50
247/247 [==============================] - 1s 5ms/step - loss: 6.3803e-05 - val_loss: 0.0040
Epoch 4/50
247/247 [==============================] - 1s 5ms/step - loss: 4.7738e-05 - val_loss: 0.0033
Epoch 5/50
247/247 [==============================] - 1s 5ms/step - loss: 4.0067e-05 - val_loss: 0.0031
Epoch 6/50
247/247 [==============================] - 1s 5ms/step - loss: 3.6713e-05 - val_loss: 0.0033
Epoch 7/50
247/247 [==============================] - 1s 5ms/step - loss: 3.5756e-05 - val_loss: 0.0073
Epoch 8/50
247/247 [==============================] - 1s 5ms/step - loss: 3.5185e-05 - val_loss: 0.0066
Epoch 9/50
247/247 [==============================] - 1s 5ms/step - loss: 3.3213e-05 - val_loss: 0.0099
Epoch 10/50
247/247 [==============================] - 1s 5ms/step - loss: 3.1838e-05 - val_loss: 0.0088
Epoch 11/50
247/247 [==============================] - 1s 5ms/step - loss: 3.2940e-05 - val_loss: 0.0103
Epoch 12/50
247/247 [==============================] - 1s 5ms/step - loss: 3.5240e-05 - val_loss: 0.0115
62/62 [==============================] - 0s 1ms/step
62/62 [==============================] - 0s 1ms/step
AJG : 0.0017273450702445785 0.026615429863279944 0.04156134105445322
AJG : 243.13000500000004 39.427724573229256 -203.70228042677078
Data for AJG appended successfully.
DTE
X_train shape: (12415, 8, 1)
y_train shape: (12415,)
X_test shape: (3104, 8, 1)
y_test shape: (3104,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.

388/388 [==============================] - 4s 7ms/step - loss: 0.0014 - val_loss: 0.0217
Epoch 2/50
388/388 [==============================] - 2s 5ms/step - loss: 5.6581e-04 - val_loss: 0.0081
Epoch 3/50
388/388 [==============================] - 2s 5ms/step - loss: 1.8615e-04 - val_loss: 0.0122
Epoch 4/50
388/388 [==============================] - 2s 5ms/step - loss: 1.4145e-04 - val_loss: 0.0144
Epoch 5/50
388/388 [==============================] - 2s 5ms/step - loss: 1.2341e-04 - val_loss: 0.0248
Epoch 6/50
388/388 [==============================] - 2s 5ms/step - loss: 1.0917e-04 - val_loss: 0.0239
Epoch 7/50
388/388 [==============================] - 2s 5ms/step - loss: 1.0175e-04 - val_loss: 0.0320
Epoch 8/50
388/388 [==============================] - 2s 5ms/step - loss: 9.4775e-05 - val_loss: 0.0368
Epoch 9/50
388/388 [==============================] - 2s 5ms/step - loss: 9.3026e-05 - val_loss: 0.0340
Epoch 10/50
388/388 [==============================] - 2s 5ms/step - loss: 8.3628e-05 - val_loss: 0.0316
Epoch 11/50
388/388 [==============================] - 2s 5ms/step - loss: 8.5025e-05 - val_loss: 0.0383
Epoch 12/50
388/388 [==============================] - 2s 5ms/step - loss: 8.1242e-05 - val_loss: 0.0504
97/97 [==============================] - 0s 1ms/step
97/97 [==============================] - 0s 1ms/step
DTE : 0.00805522134144144 0.07080038602253054 0.08975088490617483
DTE : 98.24388199168936 39.86072668521837 -58.383155306471
Data for DTE appended successfully.
C
X_train shape: (9410, 8, 1)
y_train shape: (9410,)
X_test shape: (2353, 8, 1)
y_test shape: (2353,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
295/295 [==============================] - 3s 6ms/step - loss: 0.0070 - val_loss: 2.7974e-04
Epoch 2/50
295/295 [==============================] - 1s 5ms/step - loss: 0.0015 - val_loss: 3.0615e-04
Epoch 3/50
295/295 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 1.5503e-04
Epoch 4/50
295/295 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 6.2786e-05
Epoch 5/50
295/295 [==============================] - 1s 5ms/step - loss: 9.9371e-04 - val_loss: 1.2606e-04
Epoch 6/50
295/295 [==============================] - 1s 5ms/step - loss: 9.3914e-04 - val_loss: 2.6731e-05
Epoch 7/50
295/295 [==============================] - 1s 5ms/step - loss: 9.0041e-04 - val_loss: 9.4787e-05
Epoch 8/50
295/295 [==============================] - 1s 5ms/step - loss: 8.4947e-04 - val_loss: 6.8705e-05
Epoch 9/50
295/295 [==============================] - 1s 5ms/step - loss: 8.9008e-04 - val_loss: 5.1888e-05
Epoch 10/50
295/295 [==============================] - 1s 5ms/step - loss: 7.9552e-04 - val_loss: 2.1340e-05
Epoch 11/50
295/295 [==============================] - 1s 5ms/step - loss: 7.3887e-04 - val_loss: 1.3505e-04
Epoch 12/50
295/295 [==============================] - 1s 5ms/step - loss: 7.9430e-04 - val_loss: 2.4996e-05
Epoch 13/50
295/295 [==============================] - 1s 5ms/step - loss: 7.9839e-04 - val_loss: 2.4211e-04
Epoch 14/50
295/295 [==============================] - 1s 5ms/step - loss: 8.0555e-04 - val_loss: 5.8849e-05
Epoch 15/50
295/295 [==============================] - 1s 5ms/step - loss: 8.0006e-04 - val_loss: 2.4989e-05
Epoch 16/50
295/295 [==============================] - 2s 5ms/step - loss: 7.8986e-04 - val_loss: 1.8275e-04
Epoch 17/50
295/295 [==============================] - 1s 5ms/step - loss: 7.5119e-04 - val_loss: 3.6589e-05
Epoch 18/50
295/295 [==============================] - 1s 5ms/step - loss: 7.9013e-04 - val_loss: 1.9759e-05
Epoch 19/50
295/295 [==============================] - 2s 5ms/step - loss: 7.9747e-04 - val_loss: 2.2187e-05
Epoch 20/50
295/295 [==============================] - 2s 5ms/step - loss: 7.1471e-04 - val_loss: 1.0958e-04
Epoch 21/50
295/295 [==============================] - 1s 5ms/step - loss: 6.8734e-04 - val_loss: 4.5946e-05
Epoch 22/50
295/295 [==============================] - 2s 8ms/step - loss: 7.3407e-04 - val_loss: 4.5846e-05
Epoch 23/50
295/295 [==============================] - 1s 5ms/step - loss: 7.2612e-04 - val_loss: 3.7460e-05
Epoch 24/50
295/295 [==============================] - 1s 5ms/step - loss: 7.5853e-04 - val_loss: 1.2016e-04
Epoch 25/50
295/295 [==============================] - 1s 5ms/step - loss: 7.3052e-04 - val_loss: 1.8628e-04
Epoch 26/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9924e-04 - val_loss: 1.8548e-05
Epoch 27/50
295/295 [==============================] - 2s 8ms/step - loss: 7.0225e-04 - val_loss: 1.8114e-05
Epoch 28/50
295/295 [==============================] - 2s 6ms/step - loss: 6.8668e-04 - val_loss: 1.1467e-04
Epoch 29/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9614e-04 - val_loss: 3.4005e-05
Epoch 30/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9104e-04 - val_loss: 2.7632e-05
Epoch 31/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9561e-04 - val_loss: 1.8506e-04
Epoch 32/50
295/295 [==============================] - 1s 5ms/step - loss: 7.0994e-04 - val_loss: 9.9868e-05
Epoch 33/50
295/295 [==============================] - 1s 5ms/step - loss: 7.1973e-04 - val_loss: 1.0623e-05
Epoch 34/50
295/295 [==============================] - 2s 5ms/step - loss: 7.2011e-04 - val_loss: 1.3490e-05
Epoch 35/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9891e-04 - val_loss: 5.7256e-05
Epoch 36/50
295/295 [==============================] - 1s 5ms/step - loss: 6.8673e-04 - val_loss: 1.0089e-05
Epoch 37/50
295/295 [==============================] - 1s 5ms/step - loss: 6.8066e-04 - val_loss: 5.3451e-05
Epoch 38/50
295/295 [==============================] - 1s 5ms/step - loss: 7.1394e-04 - val_loss: 1.5300e-05
Epoch 39/50
295/295 [==============================] - 1s 5ms/step - loss: 7.1631e-04 - val_loss: 1.3440e-05
Epoch 40/50
295/295 [==============================] - 1s 5ms/step - loss: 6.7073e-04 - val_loss: 1.3256e-04
Epoch 41/50
295/295 [==============================] - 1s 5ms/step - loss: 6.6709e-04 - val_loss: 1.9743e-05
Epoch 42/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9486e-04 - val_loss: 7.1379e-05
Epoch 43/50
295/295 [==============================] - 1s 5ms/step - loss: 7.0087e-04 - val_loss: 1.5030e-05
Epoch 44/50
295/295 [==============================] - 1s 5ms/step - loss: 7.2427e-04 - val_loss: 7.0596e-05
Epoch 45/50
295/295 [==============================] - 1s 5ms/step - loss: 6.7392e-04 - val_loss: 6.7753e-05
Epoch 46/50
295/295 [==============================] - 1s 5ms/step - loss: 6.9117e-04 - val_loss: 3.9990e-05
74/74 [==============================] - 0s 1ms/step
74/74 [==============================] - 0s 1ms/step
C : 1.008863362390842e-05 0.002446548150735271 0.003176260950222513
C : 42.12146201348199 47.27703737393022 5.155575360448232
Data for C appended successfully.
T
X_train shape: (8017, 8, 1)
y_train shape: (8017,)
X_test shape: (2005, 8, 1)
y_test shape: (2005,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
251/251 [==============================] - 3s 6ms/step - loss: 0.0104 - val_loss: 8.6784e-04
Epoch 2/50
251/251 [==============================] - 1s 5ms/step - loss: 0.0018 - val_loss: 4.3421e-04
Epoch 3/50
251/251 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0011
Epoch 4/50
251/251 [==============================] - 1s 5ms/step - loss: 0.0013 - val_loss: 5.9672e-04
Epoch 5/50
251/251 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 5.4590e-04
Epoch 6/50
251/251 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 2.3253e-04
Epoch 7/50
251/251 [==============================] - 1s 5ms/step - loss: 9.9403e-04 - val_loss: 2.8358e-04
Epoch 8/50
251/251 [==============================] - 1s 5ms/step - loss: 9.3568e-04 - val_loss: 3.1929e-04
Epoch 9/50
251/251 [==============================] - 1s 5ms/step - loss: 8.7410e-04 - val_loss: 1.9663e-04
Epoch 10/50
251/251 [==============================] - 1s 5ms/step - loss: 8.3708e-04 - val_loss: 7.0602e-04
Epoch 11/50
251/251 [==============================] - 1s 5ms/step - loss: 7.1740e-04 - val_loss: 1.6654e-04
Epoch 12/50
251/251 [==============================] - 1s 5ms/step - loss: 6.7942e-04 - val_loss: 1.6092e-04
Epoch 13/50
251/251 [==============================] - 1s 5ms/step - loss: 6.7240e-04 - val_loss: 2.6827e-04
Epoch 14/50
251/251 [==============================] - 1s 5ms/step - loss: 6.2988e-04 - val_loss: 7.3244e-04
Epoch 15/50
251/251 [==============================] - 1s 5ms/step - loss: 5.4206e-04 - val_loss: 5.2459e-04
Epoch 16/50
251/251 [==============================] - 1s 5ms/step - loss: 5.5810e-04 - val_loss: 2.0274e-04
Epoch 17/50
251/251 [==============================] - 1s 5ms/step - loss: 5.3285e-04 - val_loss: 4.9767e-04
Epoch 18/50
251/251 [==============================] - 1s 5ms/step - loss: 5.1787e-04 - val_loss: 6.5023e-04
Epoch 19/50
251/251 [==============================] - 1s 5ms/step - loss: 5.3497e-04 - val_loss: 5.1642e-04
Epoch 20/50
251/251 [==============================] - 1s 5ms/step - loss: 5.0434e-04 - val_loss: 1.7524e-04
Epoch 21/50
251/251 [==============================] - 1s 5ms/step - loss: 5.1495e-04 - val_loss: 1.8413e-04
Epoch 22/50
251/251 [==============================] - 1s 5ms/step - loss: 4.4771e-04 - val_loss: 1.7996e-04
63/63 [==============================] - 0s 1ms/step
63/63 [==============================] - 0s 1ms/step
T : 0.00016092279142250124 0.008232106339143402 0.01268553473143727
T : 13.62515678989555 23.308812090942382 9.683655301046832
Data for T appended successfully.
CF
X_train shape: (3632, 8, 1)
y_train shape: (3632,)
X_test shape: (909, 8, 1)
y_test shape: (909,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
114/114 [==============================] - 3s 8ms/step - loss: 0.0065 - val_loss: 0.0012
Epoch 2/50
114/114 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 2.7186e-04
Epoch 3/50
114/114 [==============================] - 1s 5ms/step - loss: 7.7950e-04 - val_loss: 5.8947e-04
Epoch 4/50
114/114 [==============================] - 1s 5ms/step - loss: 7.0110e-04 - val_loss: 2.6238e-04
Epoch 5/50
114/114 [==============================] - 1s 5ms/step - loss: 6.0353e-04 - val_loss: 3.7887e-04
Epoch 6/50
114/114 [==============================] - 1s 5ms/step - loss: 5.5892e-04 - val_loss: 2.4943e-04
Epoch 7/50
114/114 [==============================] - 1s 5ms/step - loss: 5.5995e-04 - val_loss: 3.5833e-04
Epoch 8/50
114/114 [==============================] - 1s 5ms/step - loss: 4.6709e-04 - val_loss: 2.8855e-04
Epoch 9/50
114/114 [==============================] - 1s 5ms/step - loss: 4.6454e-04 - val_loss: 5.2134e-04
Epoch 10/50
114/114 [==============================] - 1s 5ms/step - loss: 4.2387e-04 - val_loss: 3.9285e-04
Epoch 11/50
114/114 [==============================] - 1s 5ms/step - loss: 4.3582e-04 - val_loss: 0.0011
Epoch 12/50
114/114 [==============================] - 1s 6ms/step - loss: 3.6105e-04 - val_loss: 3.8795e-04
Epoch 13/50
114/114 [==============================] - 1s 5ms/step - loss: 3.7321e-04 - val_loss: 2.5365e-04
Epoch 14/50
114/114 [==============================] - 1s 5ms/step - loss: 3.8190e-04 - val_loss: 0.0012
Epoch 15/50
114/114 [==============================] - 1s 5ms/step - loss: 3.4596e-04 - val_loss: 8.1761e-04
Epoch 16/50
114/114 [==============================] - 1s 5ms/step - loss: 3.1197e-04 - val_loss: 0.0011
29/29 [==============================] - 0s 1ms/step
29/29 [==============================] - 0s 1ms/step
CF : 0.0002494308212252776 0.011283410557190031 0.015793379031267426
CF : 81.27183341978882 27.195752506852152 -54.07608091293667
Data for CF appended successfully.
MGM
X_train shape: (7109, 8, 1)
y_train shape: (7109,)
X_test shape: (1778, 8, 1)
y_test shape: (1778,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
223/223 [==============================] - 3s 7ms/step - loss: 0.0033 - val_loss: 2.8211e-04
Epoch 2/50
223/223 [==============================] - 1s 5ms/step - loss: 7.3025e-04 - val_loss: 5.3232e-04
Epoch 3/50
223/223 [==============================] - 1s 5ms/step - loss: 5.9678e-04 - val_loss: 4.5927e-04
Epoch 4/50
223/223 [==============================] - 1s 5ms/step - loss: 4.9009e-04 - val_loss: 9.5302e-04
Epoch 5/50
223/223 [==============================] - 1s 5ms/step - loss: 5.0991e-04 - val_loss: 9.9191e-04
Epoch 6/50
223/223 [==============================] - 1s 5ms/step - loss: 4.1798e-04 - val_loss: 9.0740e-04
Epoch 7/50
223/223 [==============================] - 1s 5ms/step - loss: 4.2478e-04 - val_loss: 1.7796e-04
Epoch 8/50
223/223 [==============================] - 1s 5ms/step - loss: 3.9134e-04 - val_loss: 2.9770e-04
Epoch 9/50
223/223 [==============================] - 1s 5ms/step - loss: 3.6409e-04 - val_loss: 1.1769e-04
Epoch 10/50
223/223 [==============================] - 1s 5ms/step - loss: 3.8812e-04 - val_loss: 3.8664e-04
Epoch 11/50
223/223 [==============================] - 1s 5ms/step - loss: 3.9487e-04 - val_loss: 1.0702e-04
Epoch 12/50
223/223 [==============================] - 1s 5ms/step - loss: 3.6908e-04 - val_loss: 1.6075e-04
Epoch 13/50
223/223 [==============================] - 1s 5ms/step - loss: 3.1597e-04 - val_loss: 2.9079e-04
Epoch 14/50
223/223 [==============================] - 1s 5ms/step - loss: 3.1934e-04 - val_loss: 1.4324e-04
Epoch 15/50
223/223 [==============================] - 1s 5ms/step - loss: 3.1772e-04 - val_loss: 1.1293e-04
Epoch 16/50
223/223 [==============================] - 1s 5ms/step - loss: 3.9379e-04 - val_loss: 2.0571e-04
Epoch 17/50
223/223 [==============================] - 1s 5ms/step - loss: 3.1162e-04 - val_loss: 1.0719e-04
Epoch 18/50
223/223 [==============================] - 1s 5ms/step - loss: 4.2202e-04 - val_loss: 9.6478e-05
Epoch 19/50
223/223 [==============================] - 1s 5ms/step - loss: 2.7382e-04 - val_loss: 2.5448e-04
Epoch 20/50
223/223 [==============================] - 1s 5ms/step - loss: 2.8462e-04 - val_loss: 1.2550e-04
Epoch 21/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0606e-04 - val_loss: 1.8271e-04
Epoch 22/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0430e-04 - val_loss: 9.9912e-05
Epoch 23/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0853e-04 - val_loss: 2.5290e-04
Epoch 24/50
223/223 [==============================] - 1s 5ms/step - loss: 3.4739e-04 - val_loss: 4.8464e-04
Epoch 25/50
223/223 [==============================] - 1s 5ms/step - loss: 3.4591e-04 - val_loss: 1.9316e-04
Epoch 26/50
223/223 [==============================] - 1s 5ms/step - loss: 2.4593e-04 - val_loss: 1.0525e-04
Epoch 27/50
223/223 [==============================] - 1s 5ms/step - loss: 3.2041e-04 - val_loss: 7.8902e-05
Epoch 28/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0038e-04 - val_loss: 8.5386e-05
Epoch 29/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0252e-04 - val_loss: 1.3634e-04
Epoch 30/50
223/223 [==============================] - 1s 5ms/step - loss: 3.7798e-04 - val_loss: 1.1721e-04
Epoch 31/50
223/223 [==============================] - 1s 5ms/step - loss: 3.2274e-04 - val_loss: 1.1186e-04
Epoch 32/50
223/223 [==============================] - 1s 5ms/step - loss: 3.3061e-04 - val_loss: 7.0046e-05
Epoch 33/50
223/223 [==============================] - 1s 5ms/step - loss: 2.9267e-04 - val_loss: 1.0643e-04
Epoch 34/50
223/223 [==============================] - 1s 5ms/step - loss: 3.4455e-04 - val_loss: 1.4160e-04
Epoch 35/50
223/223 [==============================] - 1s 5ms/step - loss: 2.8111e-04 - val_loss: 8.3735e-05
Epoch 36/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0467e-04 - val_loss: 1.2177e-04
Epoch 37/50
223/223 [==============================] - 1s 5ms/step - loss: 2.8614e-04 - val_loss: 1.4518e-04
Epoch 38/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0468e-04 - val_loss: 9.0081e-05
Epoch 39/50
223/223 [==============================] - 1s 5ms/step - loss: 2.9246e-04 - val_loss: 6.5005e-05
Epoch 40/50
223/223 [==============================] - 1s 5ms/step - loss: 2.6660e-04 - val_loss: 2.4842e-04
Epoch 41/50
223/223 [==============================] - 1s 5ms/step - loss: 2.9654e-04 - val_loss: 1.6934e-04
Epoch 42/50
223/223 [==============================] - 1s 5ms/step - loss: 2.6755e-04 - val_loss: 1.9563e-04
Epoch 43/50
223/223 [==============================] - 1s 5ms/step - loss: 2.7228e-04 - val_loss: 9.8904e-05
Epoch 44/50
223/223 [==============================] - 1s 5ms/step - loss: 2.9086e-04 - val_loss: 2.2493e-04
Epoch 45/50
223/223 [==============================] - 1s 5ms/step - loss: 2.8568e-04 - val_loss: 1.6317e-04
Epoch 46/50
223/223 [==============================] - 1s 5ms/step - loss: 2.6966e-04 - val_loss: 2.0760e-04
Epoch 47/50
223/223 [==============================] - 1s 5ms/step - loss: 2.5566e-04 - val_loss: 9.4637e-05
Epoch 48/50
223/223 [==============================] - 1s 5ms/step - loss: 2.7553e-04 - val_loss: 6.8221e-05
Epoch 49/50
223/223 [==============================] - 1s 5ms/step - loss: 2.6748e-04 - val_loss: 4.8546e-05
Epoch 50/50
223/223 [==============================] - 1s 5ms/step - loss: 3.0072e-04 - val_loss: 5.3702e-05
56/56 [==============================] - 0s 1ms/step
56/56 [==============================] - 0s 1ms/step
MGM : 5.3701813034062234e-05 0.004959242335079836 0.007328152088627953
MGM : 36.81112201103617 24.747177172899246 -12.063944838136923
Data for MGM appended successfully.
HUM
X_train shape: (8400, 8, 1)
y_train shape: (8400,)
X_test shape: (2101, 8, 1)
y_test shape: (2101,)
Epoch 1/50
  1/263 [..............................] - ETA: 7:20 - loss: 0.0042
263/263 [==============================] - 3s 6ms/step - loss: 4.7020e-04 - val_loss: 0.0058
Epoch 2/50
263/263 [==============================] - 1s 5ms/step - loss: 8.8437e-05 - val_loss: 0.0037
Epoch 3/50
263/263 [==============================] - 1s 5ms/step - loss: 6.6903e-05 - val_loss: 1.2474e-04
Epoch 4/50
263/263 [==============================] - 1s 5ms/step - loss: 6.0290e-05 - val_loss: 0.0012
Epoch 5/50
263/263 [==============================] - 1s 5ms/step - loss: 5.3170e-05 - val_loss: 3.4724e-04
Epoch 6/50
263/263 [==============================] - 1s 5ms/step - loss: 4.8549e-05 - val_loss: 0.0040
Epoch 7/50
263/263 [==============================] - 1s 5ms/step - loss: 4.6971e-05 - val_loss: 0.0067
Epoch 8/50
263/263 [==============================] - 1s 5ms/step - loss: 5.5424e-05 - val_loss: 7.4898e-04
Epoch 9/50
263/263 [==============================] - 1s 5ms/step - loss: 4.3242e-05 - val_loss: 0.0087
Epoch 10/50
263/263 [==============================] - 1s 5ms/step - loss: 4.7256e-05 - val_loss: 0.0064
Epoch 11/50
263/263 [==============================] - 1s 5ms/step - loss: 4.7476e-05 - val_loss: 0.0040
Epoch 12/50
263/263 [==============================] - 1s 5ms/step - loss: 4.0415e-05 - val_loss: 0.0137
Epoch 13/50
263/263 [==============================] - 1s 5ms/step - loss: 4.0446e-05 - val_loss: 0.0076
66/66 [==============================] - 0s 1ms/step
66/66 [==============================] - 0s 1ms/step
HUM : 0.00012473600466975042 0.007621945454435258 0.011168527417244872
HUM : 479.76902371585317 192.03042097575963 -287.73860274009354
Data for HUM appended successfully.
CBOE
X_train shape: (2658, 8, 1)
y_train shape: (2658,)
X_test shape: (665, 8, 1)
y_test shape: (665,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
84/84 [==============================] - 2s 9ms/step - loss: 0.0169 - val_loss: 0.0014
Epoch 2/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0022 - val_loss: 3.7963e-04
Epoch 3/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 5.1180e-04
Epoch 4/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0012
Epoch 5/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0022
Epoch 6/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 2.3529e-04
Epoch 7/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 4.4572e-04
Epoch 8/50
84/84 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 3.5382e-04
Epoch 9/50
84/84 [==============================] - 0s 5ms/step - loss: 9.7808e-04 - val_loss: 0.0011
Epoch 10/50
84/84 [==============================] - 0s 5ms/step - loss: 9.4434e-04 - val_loss: 0.0017
Epoch 11/50
84/84 [==============================] - 0s 5ms/step - loss: 8.7817e-04 - val_loss: 2.2272e-04
Epoch 12/50
84/84 [==============================] - 0s 5ms/step - loss: 8.4529e-04 - val_loss: 6.6390e-04
Epoch 13/50
84/84 [==============================] - 0s 5ms/step - loss: 8.5961e-04 - val_loss: 3.4231e-04
Epoch 14/50
84/84 [==============================] - 0s 5ms/step - loss: 7.5910e-04 - val_loss: 3.7876e-04
Epoch 15/50
84/84 [==============================] - 0s 5ms/step - loss: 8.0994e-04 - val_loss: 2.4625e-04
Epoch 16/50
84/84 [==============================] - 0s 5ms/step - loss: 8.6769e-04 - val_loss: 0.0011
Epoch 17/50
84/84 [==============================] - 0s 5ms/step - loss: 7.2741e-04 - val_loss: 2.6536e-04
Epoch 18/50
84/84 [==============================] - 0s 5ms/step - loss: 6.8182e-04 - val_loss: 5.4072e-04
Epoch 19/50
84/84 [==============================] - 0s 5ms/step - loss: 7.3259e-04 - val_loss: 2.8268e-04
Epoch 20/50
84/84 [==============================] - 0s 5ms/step - loss: 7.6817e-04 - val_loss: 0.0012
Epoch 21/50
84/84 [==============================] - 0s 5ms/step - loss: 6.0971e-04 - val_loss: 0.0017
21/21 [==============================] - 0s 1ms/step
21/21 [==============================] - 0s 1ms/step
CBOE : 0.0002227246833041313 0.012088135818163346 0.0149239633912755
CBOE : 165.860001 103.39236819131845 -62.46763280868156
Data for CBOE appended successfully.
CFG
X_train shape: (1796, 8, 1)
y_train shape: (1796,)
X_test shape: (450, 8, 1)
y_test shape: (450,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
57/57 [==============================] - 2s 12ms/step - loss: 0.0365 - val_loss: 0.0032
Epoch 2/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 6.2137e-04
Epoch 3/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 9.6258e-04
Epoch 4/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 6.1837e-04
Epoch 5/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 3.7137e-04
Epoch 6/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 8.1198e-04
Epoch 7/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 3.3546e-04
Epoch 8/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 3.5800e-04
Epoch 9/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 3.0189e-04
Epoch 10/50
49/57 [========================>.....] - ETA: 0s - loss: 0.00140017 - val_loss: 6.2099e-04
Epoch 11/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 3.3485e-04
Epoch 12/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.0012
Epoch 13/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 5.0931e-04
Epoch 14/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 5.3970e-04
Epoch 15/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 6.1559e-04
Epoch 16/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 3.1592e-04
Epoch 17/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 4.0796e-04
Epoch 18/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 2.8315e-04
Epoch 19/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 3.4906e-04
Epoch 20/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 2.8521e-04
Epoch 21/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 2.6867e-04
Epoch 22/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 5.9122e-04
Epoch 23/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 3.0045e-04
Epoch 24/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 4.0796e-04
Epoch 25/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 4.5198e-04
Epoch 26/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 2.7590e-04
Epoch 27/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 7.2303e-04
Epoch 28/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 2.7628e-04
Epoch 29/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 3.5657e-04
Epoch 30/50
57/57 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 3.2320e-04
Epoch 31/50
57/57 [==============================] - 0s 5ms/step - loss: 9.9172e-04 - val_loss: 2.7208e-04
CFG : 25.721887226267963 51.09386635359633 25.37197912732837 0.0013 - val_loss: 4.5198e-04
Data for CFG appended successfully.
APH
X_train shape: (6406, 8, 1)
y_train shape: (6406,)
X_test shape: (1602, 8, 1)
y_test shape: (1602,)
Epoch 1/50
CFG : 25.721887226267963 51.09386635359633 25.37197912732837 0.0013 - val_loss: 4.5198e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CFG : 25.721887226267963 51.09386635359633 25.37197912732837 0.0013 - val_loss: 4.5198e-04
189/201 [===========================>..] - ETA: 0s - loss: 1.5488e-04 val_loss: 4.5198e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.5641e-04 - val_loss: 7.6869e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.5641e-04 - val_loss: 7.6869e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.5641e-04 - val_loss: 7.6869e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
201/201 [==============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
201/201 [==============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 5/50=============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 7/50=============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 7/50=============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 10/50============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 12/50============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
Epoch 12/50============================] - 1s 5ms/step - loss: 1.2365e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
SYY : 0.0005565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
Epoch 10/505565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
Epoch 10/505565865550044495 0.017498893756307515 0.023592086703054682e-04 - val_loss: 0.0053e-04
MSI : 7.261811783372055e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
y_train shape: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
y_train shape: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50ape: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 7/50ape: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 9/50ape: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 12/50pe: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 14/50pe: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 16/50pe: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 19/50pe: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
Epoch 21/50pe: (5666,)5e-05 0.006779186741373516 0.008521626478185992e-04 - val_loss: 0.0053e-04
FCX : 35.18734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
FCX : 35.18734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 3/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 6/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 6/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 9/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 9/508734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 12/50734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
Epoch 12/50734983208799 18.319603979587555 -16.8677458525004348185992e-04 - val_loss: 0.0053e-04
ADM : 0.0002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
ADM : 0.0002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
Epoch 5/5002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
Epoch 7/5002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
Epoch 9/5002591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
Epoch 11/502591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
Epoch 13/502591282597253591 0.01130504427843919 0.0160974612819959692e-04 - val_loss: 0.0053e-04
LH : 0.0001457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
LH : 0.0001457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/501457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 7/501457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 11/50457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 15/50457241896460602 0.00824959242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Data for BLDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Data for BLDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 3/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 3/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 7/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 7/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 7/50LDR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 11/50DR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 11/50DR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
Epoch 11/50DR appended successfully.9242697563 0.01207162746468181292e-04 - val_loss: 0.0053e-04
LNT : 0.00011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
LNT : 0.00011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
LNT : 0.00011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
LNT : 0.00011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 5/50011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 5/50011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 5/50011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 5/50011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 10/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 10/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 10/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 14/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 14/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 14/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 14/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 19/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 19/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 19/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 19/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 24/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 24/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 24/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 24/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 29/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 29/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 29/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 29/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 29/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 35/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 35/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 35/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
Epoch 35/5011709441330563116 0.007981786359175298 0.010821017202908014-04 - val_loss: 0.0053e-04
BAC : 4.833092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
BAC : 4.833092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/503092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
Epoch 8/503092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
Epoch 14/50092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
Epoch 20/50092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
Epoch 26/50092816179251e-05 0.004894194952212618 0.0069520448906629274-04 - val_loss: 0.0053e-04
PSX : 0.0004163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
PSX : 0.0004163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5004163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
Epoch 6/5004163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
Epoch 9/5004163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
Epoch 12/504163407980210193 0.015023432261353286 0.0204044308428590974-04 - val_loss: 0.0053e-04
GPN : 0.00021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
GPN : 0.00021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
GPN : 0.00021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
Epoch 7/50021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
Epoch 9/50021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
Epoch 11/5021818814679766684 0.011170189533785471 0.014771193140625674-04 - val_loss: 0.0053e-04
46/46 [==============================] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
y_train shape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
y_train shape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
y_train shape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
Epoch 5/50ape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
Epoch 5/50ape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
Epoch 8/50ape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
Epoch 8/50ape: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
Epoch 11/50pe: (8763,)===============] - 0s 1ms/step014771193140625674-04 - val_loss: 0.0053e-04
PPG : 0.00017242928032285714 0.010403537627595213 0.013131233008474762-04 - val_loss: 0.0053e-04
PPG : 0.00017242928032285714 0.010403537627595213 0.013131233008474762-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PPG : 0.00017242928032285714 0.010403537627595213 0.013131233008474762-04 - val_loss: 0.0053e-04
106/215 [=============>................] - ETA: 0s - loss: 7.0595e-052-04 - val_loss: 0.0053e-04
Epoch 6/50============>................] - ETA: 0s - loss: 7.0595e-052-04 - val_loss: 0.0053e-04
Epoch 8/50============>................] - ETA: 0s - loss: 7.0595e-052-04 - val_loss: 0.0053e-04
Epoch 10/50===========>................] - ETA: 0s - loss: 7.0595e-052-04 - val_loss: 0.0053e-04
Epoch 12/50===========>................] - ETA: 0s - loss: 7.0595e-052-04 - val_loss: 0.0053e-04
TECH : 0.0001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
TECH : 0.0001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
Epoch 5/50001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
Epoch 8/50001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
Epoch 10/5001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
Epoch 13/5001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
Epoch 15/5001239188159277585 0.0079002171570065 0.01113188285636165452-04 - val_loss: 0.0053e-04
44/44 [==============================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
y_train shape: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
y_train shape: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 3/50ape: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 5/50ape: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 12/50pe: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 17/50pe: (2074,)===============] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
17/17 [==============================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
17/17 [==============================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
Epoch 12/50==========================] - 0s 1ms/step113188285636165452-04 - val_loss: 0.0053e-04
ESS : 0.00016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
ESS : 0.00016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 8/50016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 11/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 15/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 19/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 22/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 26/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
Epoch 29/5016957005409873498 0.010420134960338499 0.013021906699816852-04 - val_loss: 0.0053e-04
29/29 [==============================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
29/29 [==============================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
29/29 [==============================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 14/50==========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
Epoch 16/50==========================] - 0s 1ms/step013021906699816852-04 - val_loss: 0.0053e-04
HAL : 0.000162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
HAL : 0.000162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
HAL : 0.000162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
Epoch 3/500162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
Epoch 5/500162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
Epoch 7/500162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
Epoch 10/50162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
Epoch 12/50162890138501385 0.008779976051048213 0.01276284210124786452-04 - val_loss: 0.0053e-04
STZ : 0.0001138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
STZ : 0.0001138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5001138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 6/5001138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 9/5001138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 12/501138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 14/501138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 17/501138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
Epoch 20/501138736661739237 0.008179371400235107 0.0106711604886218352-04 - val_loss: 0.0053e-04
35/35 [==============================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
35/35 [==============================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step106711604886218352-04 - val_loss: 0.0053e-04
PARA : 0.00044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
PARA : 0.00044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PARA : 0.00044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 4/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 4/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 7/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 9/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 9/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 9/500044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 13/50044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 13/50044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
Epoch 16/50044403734925001795 0.01343684896018401 0.021072193745550512-04 - val_loss: 0.0053e-04
ADI : 0.0002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
ADI : 0.0002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 3/5002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 6/5002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 6/5002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 9/5002643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 11/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 11/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 14/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 16/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 16/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 19/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 19/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 22/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 24/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 24/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 27/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 27/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 30/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 32/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
Epoch 32/502643853297703483 0.01280789649688441 0.01625993018959024512-04 - val_loss: 0.0053e-04
F : 7.01521076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
F : 7.01521076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
F : 7.01521076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 4/501076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 6/501076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 8/501076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 10/50076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 12/50076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
Epoch 12/50076969831e-05 0.005800346894046162 0.0083756855060934024512-04 - val_loss: 0.0053e-04
ADBE : 0.0011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
ADBE : 0.0011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
Epoch 6/50011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
Epoch 8/50011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
Epoch 11/5011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
Epoch 14/5011564432591005419 0.021440181772097472 0.034006517891435782-04 - val_loss: 0.0053e-04
43/43 [==============================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
43/43 [==============================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
43/43 [==============================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
Epoch 3/50===========================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step034006517891435782-04 - val_loss: 0.0053e-04
SMCI : 0.005228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
SMCI : 0.005228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5005228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
Epoch 6/5005228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
Epoch 8/5005228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
Epoch 10/505228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
Epoch 13/505228253499840372 0.030196232047124245 0.0723066629007339382-04 - val_loss: 0.0053e-04
CPRT : 0.0003971028951437294 0.013554025739797288 0.019927440757501438-04 - val_loss: 0.0053e-04
CPRT : 0.0003971028951437294 0.013554025739797288 0.019927440757501438-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50003971028951437294 0.013554025739797288 0.019927440757501438-04 - val_loss: 0.0053e-04
Epoch 7/50003971028951437294 0.013554025739797288 0.019927440757501438-04 - val_loss: 0.0053e-04
Epoch 10/5003971028951437294 0.013554025739797288 0.019927440757501438-04 - val_loss: 0.0053e-04
28/28 [==============================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
28/28 [==============================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
28/28 [==============================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 12/50==========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
Epoch 12/50==========================] - 0s 1ms/step019927440757501438-04 - val_loss: 0.0053e-04
TFX : 210.67181576805376 125.3198847173766 -85.35193105067717757501438-04 - val_loss: 0.0053e-04
TFX : 210.67181576805376 125.3198847173766 -85.35193105067717757501438-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5067181576805376 125.3198847173766 -85.35193105067717757501438-04 - val_loss: 0.0053e-04
Epoch 8/5067181576805376 125.3198847173766 -85.35193105067717757501438-04 - val_loss: 0.0053e-04
Epoch 13/507181576805376 125.3198847173766 -85.35193105067717757501438-04 - val_loss: 0.0053e-04
25/25 [==============================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
25/25 [==============================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step105067717757501438-04 - val_loss: 0.0053e-04
ARE : 0.00031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
ARE : 0.00031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ARE : 0.00031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
ARE : 0.00031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 5/50031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 7/50031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 7/50031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 10/5031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 10/5031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 13/5031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
Epoch 13/5031608953753928086 0.012909454272646392 0.017778907096311653-04 - val_loss: 0.0053e-04
69/69 [==============================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
y_train shape: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
y_train shape: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 1/500pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 1/500pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 5/500pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 7/500pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 10/50pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
Epoch 12/50pe: (63,)=================] - 0s 1ms/step017778907096311653-04 - val_loss: 0.0053e-04
CB : 9.961002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
CB : 9.961002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CB : 9.961002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 4/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 4/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 7/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 9/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 9/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 9/50002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 13/5002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
Epoch 13/5002082492326e-05 0.007835016912776025 0.00998048199361750553-04 - val_loss: 0.0053e-04
TSN : 0.00034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
TSN : 0.00034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
Epoch 9/50034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
Epoch 14/5034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
Epoch 18/5034215085997574453 0.012967615877289387 0.018497320345816163-04 - val_loss: 0.0053e-04
22/22 [==============================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
22/22 [==============================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 3/50===========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step018497320345816163-04 - val_loss: 0.0053e-04
PEP : 0.0007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
PEP : 0.0007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PEP : 0.0007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
PEP : 0.0007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 5/5007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 5/5007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 8/5007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 8/5007043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 11/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 11/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 14/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 16/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 16/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
Epoch 19/507043731930210938 0.018434388423177087 0.0265400300116841263-04 - val_loss: 0.0053e-04
PEG : 0.000200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
PEG : 0.000200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
PEG : 0.000200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
Epoch 3/500200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
Epoch 8/500200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
Epoch 14/50200091683310137 0.011508116365472647 0.01414537674684336663-04 - val_loss: 0.0053e-04
NOW : 0.00026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
NOW : 0.00026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
Epoch 3/50026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
Epoch 6/50026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
Epoch 8/50026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
Epoch 8/50026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
Epoch 11/5026217325810540726 0.013029276219210295 0.016191765132480378-04 - val_loss: 0.0053e-04
LLY : 0.00024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
LLY : 0.00024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
LLY : 0.00024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 4/50024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 6/50024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 8/50024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 10/5024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 12/5024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 12/5024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 15/5024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
Epoch 17/5024244474061967583 0.009262512802732102 0.015570637129535703-04 - val_loss: 0.0053e-04
COST : 0.0005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
COST : 0.0005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 5/50005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 7/50005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 9/50005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 12/5005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 14/5005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 16/5005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
Epoch 18/5005646526439990951 0.015531406554371081 0.023762420836250988-04 - val_loss: 0.0053e-04
REG : 62.54457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
REG : 62.54457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 3/504457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 6/504457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 12/50457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 18/50457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 1/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 1/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 5/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 5/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 8/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 8/500457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 11/50457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 13/50457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
Epoch 13/50457054295119 67.04538071762717 4.50081017467597620836250988-04 - val_loss: 0.0053e-04
LOW : 0.0006413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
LOW : 0.0006413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5006413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
Epoch 6/5006413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
Epoch 10/506413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
Epoch 13/506413524887969969 0.018947451079974146 0.0253249380808126988-04 - val_loss: 0.0053e-04
MDLZ : 68.88661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
MDLZ : 68.88661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5088661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
Epoch 7/5088661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
Epoch 9/5088661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
Epoch 11/508661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
Epoch 14/508661188098656 51.94715081427098 -16.93946106671558708126988-04 - val_loss: 0.0053e-04
39/39 [==============================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
39/39 [==============================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
39/39 [==============================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/50===========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 15/50==========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
Epoch 15/50==========================] - 0s 1ms/step106671558708126988-04 - val_loss: 0.0053e-04
ZBRA : 0.0009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
ZBRA : 0.0009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 3/50009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 3/50009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 3/50009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 7/50009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 7/50009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 10/5009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
Epoch 10/5009800141520261403 0.016625341716256882 0.031305177719127238-04 - val_loss: 0.0053e-04
FMC : 55.150166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
FMC : 55.150166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
FMC : 55.150166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
FMC : 55.150166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
FMC : 55.150166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 6/5050166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 6/5050166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 6/5050166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 10/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 10/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
Epoch 13/500166071608524 54.281746109104006 -0.86841996250451819127238-04 - val_loss: 0.0053e-04
XEL : 59.91811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
XEL : 59.91811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
Epoch 4/501811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
Epoch 8/501811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
Epoch 11/50811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
Epoch 15/50811252624673 25.419213231915712 -34.49889929433102819127238-04 - val_loss: 0.0053e-04
AIZ : 0.0001797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
AIZ : 0.0001797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 4/5001797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
Epoch 7/5001797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
Epoch 10/501797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
Epoch 12/501797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
Epoch 12/501797074530703745 0.009847015987254352 0.0134055008511571288-04 - val_loss: 0.0053e-04
MET : 0.00017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
MET : 0.00017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 4/50017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 20/5017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 28/5017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 36/5017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 44/5017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
Epoch 44/5017862840612311516 0.010753862431871798 0.013365193830360828-04 - val_loss: 0.0053e-04
FTV : 0.0006805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
FTV : 0.0006805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 3/5006805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 6/5006805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 10/506805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 13/506805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 17/506805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 17/506805175454406558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
y_train shape: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
y_train shape: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 4/50ape: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 7/50ape: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 9/50ape: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 12/50pe: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
Epoch 12/50pe: (5628,)06558 0.02253910656479102 0.02608673121417583528-04 - val_loss: 0.0053e-04
ACGL : 0.0001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
ACGL : 0.0001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 3/50001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 5/50001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 7/50001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 9/50001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 11/5001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 11/5001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
Epoch 11/5001120454540986423 0.0067707658057969125 0.01058515253072162-04 - val_loss: 0.0053e-04
FAST : 0.00022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
FAST : 0.00022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 3/500022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 5/500022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 5/500022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 8/500022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 10/50022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 12/50022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 14/50022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
Epoch 14/50022207553869917272 0.011865681532923639 0.01490219912292050704 - val_loss: 0.0053e-04
TJX : 0.0001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
TJX : 0.0001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
TJX : 0.0001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
TJX : 0.0001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 5/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 5/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 5/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 9/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 9/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 9/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
Epoch 9/5001177382111331753 0.008029576471178253 0.0108507239911987110704 - val_loss: 0.0053e-04
SNA : 270.06530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
Epoch 3/5006530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
Epoch 5/5006530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
Epoch 10/506530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
Epoch 16/506530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
Epoch 16/506530115691163 94.47062407205374 -175.5946770848579911987110704 - val_loss: 0.0053e-04
MPC : 152.033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
Epoch 3/50033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
Epoch 5/50033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
Epoch 8/50033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
Epoch 12/5033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
Epoch 12/5033933805656 64.50842951145982 -87.5255042941961879911987110704 - val_loss: 0.0053e-04
BR : 177.0698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
BR : 177.0698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
BR : 177.0698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
BR : 177.0698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 5/50698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 7/50698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 7/50698124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 10/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 10/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 13/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 13/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 16/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 18/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 18/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 21/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 21/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 24/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 24/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 27/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 29/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 29/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 32/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 32/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 35/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 35/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 38/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 38/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 41/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
Epoch 41/5098124221888 130.8909170737115 -46.1788953484772879911987110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
D : 0.0001776477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
Epoch 11/5076477341849217 0.009223774553464879 0.013328455806466168110704 - val_loss: 0.0053e-04
MRK : 0.0001439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
MRK : 0.0001439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 3/5001439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 6/5001439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 9/5001439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 13/501439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 16/501439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
Epoch 16/501439901633981995 0.008718836327392156 0.0119995901345920760704 - val_loss: 0.0053e-04
STX : 72.25711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
STX : 72.25711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 3/505711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 3/505711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 6/505711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 8/505711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 8/505711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 11/50711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
Epoch 11/50711862289748 55.362309556363584 -16.89480906653389545920760704 - val_loss: 0.0053e-04
NOC : 0.0006974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
NOC : 0.0006974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 3/5006974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 6/5006974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 8/5006974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 11/506974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 13/506974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 16/506974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
Epoch 18/506974521173164556 0.020425059679370642 0.0264093187590376050704 - val_loss: 0.0053e-04
42/42 [==============================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
42/42 [==============================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50===========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 18/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 26/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 34/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 41/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 5/500==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 7/500==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 7/500==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 15/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 15/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 18/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 18/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 21/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 21/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 24/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 26/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 26/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 29/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 29/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 32/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 32/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 35/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 37/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 37/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 40/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 40/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 43/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 43/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 46/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 46/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
Epoch 49/50==========================] - 0s 1ms/step264093187590376050704 - val_loss: 0.0053e-04
IPG : 0.00011921708591912414 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 11/5011921708591912414 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 35/5011921708591912414 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 35/5011921708591912414 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
X_test shape: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 9/50pe: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 9/50pe: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 12/50e: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 12/50e: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
Epoch 12/50e: (2202, 8, 1)14 0.007995173863437656 0.010918657697680799704 - val_loss: 0.0053e-04
UNP : 0.001031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 3/501031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 5/501031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 12/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 19/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 25/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 32/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 39/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 45/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
Epoch 45/50031493080507175 0.02593776396533053 0.032116865982022280799704 - val_loss: 0.0053e-04
ALLE : 0.00028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
Epoch 3/500028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
Epoch 7/500028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
Epoch 15/50028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
Epoch 22/50028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
Epoch 22/50028996610668389306 0.012489062543983666 0.01702839119482204804 - val_loss: 0.0053e-04
17/17 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
17/17 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 3/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 12/50==========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
60/60 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
60/60 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
60/60 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
60/60 [==============================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step.01702839119482204804 - val_loss: 0.0053e-04
ECL : 0.0006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
ECL : 0.0006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
ECL : 0.0006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 4/5006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 4/5006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 7/5006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 9/5006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 9/5006639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 12/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 12/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 15/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 17/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 17/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 20/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 22/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 22/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
Epoch 22/506639119016829787 0.016888313411633712 0.0257664879578684284804 - val_loss: 0.0053e-04
ETR : 0.00015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
ETR : 0.00015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
Epoch 3/50015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
Epoch 6/50015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
Epoch 9/50015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
Epoch 12/5015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
Epoch 12/5015057855273642744 0.008731627163979523 0.012271045299257412804 - val_loss: 0.0053e-04
EBAY : 0.0002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
EBAY : 0.0002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 3/50002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 6/50002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 8/50002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 10/5002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 10/5002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
Epoch 10/5002718858004329338 0.012095964229478994 0.016488959956071633804 - val_loss: 0.0053e-04
SBUX : 102.66627856354864 50.803839423739916 -51.862439139808736071633804 - val_loss: 0.0053e-04
Epoch 3/50.66627856354864 50.803839423739916 -51.862439139808736071633804 - val_loss: 0.0053e-04
Epoch 16/5066627856354864 50.803839423739916 -51.862439139808736071633804 - val_loss: 0.0053e-04
Epoch 16/5066627856354864 50.803839423739916 -51.862439139808736071633804 - val_loss: 0.0053e-04
IR : 0.00021009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
IR : 0.00021009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 3/5021009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 5/5021009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 8/5021009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 10/501009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 13/501009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
Epoch 13/501009660466038494 0.011444112204973132 0.0144947095403938653804 - val_loss: 0.0053e-04
AMT : 0.00018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
AMT : 0.00018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 4/50018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 6/50018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 8/50018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 10/5018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 12/5018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 15/5018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
Epoch 15/5018919999049013804 0.011210249254589523 0.013754998745552034804 - val_loss: 0.0053e-04
INTU : 0.0004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
INTU : 0.0004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 3/50004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 7/50004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 11/5004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 13/5004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 13/5004934308680579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
y_test shape: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 3/50pe: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 9/50pe: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 16/50e: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 23/50e: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 30/50e: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
Epoch 30/50e: (472,)80579445 0.014745955619095105 0.022213303852825328804 - val_loss: 0.0053e-04
PAYC : 160.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
PAYC : 160.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 3/50.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 5/50.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 5/50.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 8/50.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 8/50.1888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 11/501888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 11/501888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 14/501888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 14/501888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
Epoch 14/501888778268878 409.816965970236 249.628088143348243852825328804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
CMA : 0.00018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
Epoch 8/50018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
Epoch 8/50018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
Epoch 8/50018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
Epoch 8/50018799605339785136 0.009734455988189273 0.013711165282274566804 - val_loss: 0.0053e-04
PG : 0.00026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
PG : 0.00026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PG : 0.00026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
PG : 0.00026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
Epoch 4/5026220925666324026 0.010927606115099379 0.0161928767259940656804 - val_loss: 0.0053e-04
CAT : 239.99999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
CAT : 239.99999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 4/5099999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 6/5099999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 8/5099999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 10/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 12/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 14/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 16/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 18/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 20/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
Epoch 20/509999682983045 115.24173017730936 -124.758266652521099940656804 - val_loss: 0.0053e-04
ODFL : 0.0013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
ODFL : 0.0013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
ODFL : 0.0013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
ODFL : 0.0013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
MNSTh 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
MNSTh 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 3/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 5/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 7/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 9/50013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
Epoch 11/5013784263155649673 0.031005889366144675 0.037127164119616886804 - val_loss: 0.0053e-04
MNST : 55.83760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
MNST : 55.83760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
Epoch 4/5083760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
Epoch 7/5083760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
Epoch 9/5083760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
Epoch 12/503760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
Epoch 12/503760373014462 26.360295190990744 -29.4773085391538789616886804 - val_loss: 0.0053e-04
AMZN : 139.06769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 11/5006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 11/5006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 3/50006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 6/50006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 9/50006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 11/5006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 14/5006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
Epoch 16/5006769686650466 95.42680145539826 -43.6408954111064789616886804 - val_loss: 0.0053e-04
EG : 385.09926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
EG : 385.09926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
EG : 385.09926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 3/509926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 3/509926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 6/509926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 6/509926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 9/509926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 11/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 11/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 14/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 14/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 17/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 17/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 20/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 20/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 23/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 25/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 25/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 28/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 28/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 31/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 31/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 34/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 34/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
Epoch 34/50926950542626 256.2427643411638 -128.85650516426244789616886804 - val_loss: 0.0053e-04
INTC : 38.52225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
INTC : 38.52225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
INTC : 38.52225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 4/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 4/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 4/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 8/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 8/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
Epoch 8/5052225168891221 34.25217238813639 -4.270079300775826789616886804 - val_loss: 0.0053e-04
38/80 [=============>................] - ETA: 0s 079300775826789616886804 - val_loss: 0.0053e-04
38/80 [=============>................] - ETA: 0s 079300775826789616886804 - val_loss: 0.0053e-04
PNR : 61.2184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
PNR : 61.2184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
PNR : 61.2184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 5/50184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 7/50184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 7/50184090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 10/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 10/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 13/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 15/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 15/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
Epoch 18/5084090559767 41.97854922100663 -19.2398598349700726789616886804 - val_loss: 0.0053e-04
GLW : 2.0518119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
GLW : 2.0518119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
GLW : 2.0518119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
GLW : 2.0518119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
GLW : 2.0518119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 6/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 6/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 9/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 9/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 9/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
Epoch 9/5018119887963065e-05 0.0036377550498612157 0.00452969313397310204 - val_loss: 0.0053e-04
BDX : 0.0001963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
BDX : 0.0001963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 4/5001963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 9/5001963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 13/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 19/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 24/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 29/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 35/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 40/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 45/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
Epoch 45/501963018296632278 0.010350822582695334 0.0140107754840061510204 - val_loss: 0.0053e-04
20/20 [==============================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
20/20 [==============================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 3/50===========================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
40/40 [==============================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
40/40 [==============================] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
y_train shape: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 3/50ape: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 5/50ape: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 7/50ape: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 9/50ape: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 12/50pe: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 15/50pe: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
Epoch 15/50pe: (5140,)===============] - 0s 1ms/step140107754840061510204 - val_loss: 0.0053e-04
PWR : 0.0005528812009948896 0.012504984282150166 0.0235134259731518510204 - val_loss: 0.0053e-04
Epoch 3/5005528812009948896 0.012504984282150166 0.0235134259731518510204 - val_loss: 0.0053e-04
Epoch 6/5005528812009948896 0.012504984282150166 0.0235134259731518510204 - val_loss: 0.0053e-04
Epoch 13/505528812009948896 0.012504984282150166 0.0235134259731518510204 - val_loss: 0.0053e-04
Epoch 13/505528812009948896 0.012504984282150166 0.0235134259731518510204 - val_loss: 0.0053e-04
APTV : 78.11703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
APTV : 78.11703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 3/5011703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 5/5011703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 5/5011703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 8/5011703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 10/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 10/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 13/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 13/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 16/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
Epoch 16/501703230557609 156.62030644836744 78.50327414279135731518510204 - val_loss: 0.0053e-04
BBWI : 0.0002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
BBWI : 0.0002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 3/50002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 7/50002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 10/5002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 14/5002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 18/5002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 21/5002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 21/5002492597094100359 0.01120385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Data for DXCM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Data for DXCM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 4/50XCM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 8/50XCM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 11/50CM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
Epoch 11/50CM appended successfully.385029613388 0.0157879609009534820204 - val_loss: 0.0053e-04
EXR : 107.07789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
EXR : 107.07789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 3/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 3/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 6/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 8/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 8/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 8/5007789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 12/507789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 14/507789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 14/507789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
Epoch 14/507789482453448 115.42923427462577 8.351339450091288009534820204 - val_loss: 0.0053e-04
WELL : 0.0002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
WELL : 0.0002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
Epoch 3/50002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
Epoch 5/50002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
Epoch 7/50002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
Epoch 9/50002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
Epoch 11/5002628412412642947 0.010720560175829097 0.016212379259821635204 - val_loss: 0.0053e-04
53/53 [==============================] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
53/53 [==============================] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Data for HOLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Data for HOLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 4/50OLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 4/50OLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 7/50OLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 9/50OLX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 11/50LX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 11/50LX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
Epoch 11/50LX appended successfully.=] - 0s 1ms/step016212379259821635204 - val_loss: 0.0053e-04
EXPD : 0.00040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
EXPD : 0.00040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
Epoch 4/500040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
Epoch 9/500040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
Epoch 14/50040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
Epoch 14/50040503493981199664 0.013411076326090766 0.02012547986538449504 - val_loss: 0.0053e-04
GM : 0.00031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
GM : 0.00031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 3/5031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 3/5031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 6/5031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 6/5031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 9/5031573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 11/501573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 11/501573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 11/501573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
Epoch 11/501573162747857613 0.013476394680795657 0.0177688386643183949504 - val_loss: 0.0053e-04
TXN : 150.42363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 3/5042363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 5/5042363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 10/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 15/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 15/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 15/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 4/5002363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 6/5002363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 8/5002363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 10/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 10/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 13/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
Epoch 13/502363872203288 37.83873234206824 -112.5849063799646443183949504 - val_loss: 0.0053e-04
FI : 119.22504138823756 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
FI : 119.22504138823756 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
y_train shape: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
y_train shape: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 4/50ape: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 7/50ape: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 9/50ape: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 11/50pe: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 14/50pe: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 16/50pe: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
Epoch 18/50pe: (5804,)6 52.78541446503997 -66.439626923197586443183949504 - val_loss: 0.0053e-04
46/46 [==============================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
46/46 [==============================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
46/46 [==============================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
46/46 [==============================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step923197586443183949504 - val_loss: 0.0053e-04
TMO : 0.0001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
TMO : 0.0001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 3/5001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 3/5001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 6/5001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 8/5001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 8/5001162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 11/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 13/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 13/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 16/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 16/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 19/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 21/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 21/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 24/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 24/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 27/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 29/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 29/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 29/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
Epoch 29/501162471393082049 0.009526421858407375 0.0107817966642023499504 - val_loss: 0.0053e-04
OXY : 62.684752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
OXY : 62.684752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 4/5084752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 7/5084752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 9/5084752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 12/504752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 14/504752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 17/504752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 20/504752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
Epoch 20/504752629671024 74.82086830560999 12.136115675938967642023499504 - val_loss: 0.0053e-04
RL : 0.00011734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
RL : 0.00011734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 3/5011734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 6/5011734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 8/5011734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 10/501734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 12/501734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 15/501734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 17/501734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
Epoch 17/501734642758353704 0.008136464204753642 0.0108326556108618659504 - val_loss: 0.0053e-04
DECK : 0.0006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
DECK : 0.0006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 3/50006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 6/50006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 9/50006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 11/5006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 14/5006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 16/5006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 19/5006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
Epoch 19/5006410068597848828 0.019261715164122672 0.025318113274588279504 - val_loss: 0.0053e-04
CCI : 98.52720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
CCI : 98.52720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
CCI : 98.52720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
CCI : 98.52720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
CCI : 98.52720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
Epoch 6/502720069542652 112.18299905538558 13.655798359959064274588279504 - val_loss: 0.0053e-04
MMM : 77.02139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
MMM : 77.02139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 3/502139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 5/502139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 7/502139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 9/502139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 11/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 11/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 14/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 16/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 18/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
Epoch 20/50139082445244 82.03594337326783 5.01455254881538864274588279504 - val_loss: 0.0053e-04
40/57 [====================>.........] - ETA: 0s 254881538864274588279504 - val_loss: 0.0053e-04
40/57 [====================>.........] - ETA: 0s 254881538864274588279504 - val_loss: 0.0053e-04
MOS : 33.64959045212538 24.213969031395955 -9.435621420729426274588279504 - val_loss: 0.0053e-04
Epoch 3/504959045212538 24.213969031395955 -9.435621420729426274588279504 - val_loss: 0.0053e-04
Epoch 6/504959045212538 24.213969031395955 -9.435621420729426274588279504 - val_loss: 0.0053e-04
Epoch 11/50959045212538 24.213969031395955 -9.435621420729426274588279504 - val_loss: 0.0053e-04
22/22 [==============================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
22/22 [==============================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
22/22 [==============================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 7/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 9/50===========================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
69/69 [==============================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
69/69 [==============================] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Data for HSY appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 3/50SY appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 5/50SY appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 7/50SY appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 10/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 12/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 16/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 19/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 21/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 24/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 27/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
Epoch 27/50Y appended successfully.==] - 0s 1ms/step420729426274588279504 - val_loss: 0.0053e-04
JNPR : 8.092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
JNPR : 8.092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 3/5092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 5/5092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 7/5092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 9/5092986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 11/502986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 13/502986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 15/502986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
Epoch 15/502986911777288e-06 0.0023927805081206794 0.00284481755333752244 - val_loss: 0.0053e-04
DHI : 119.55454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
DHI : 119.55454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
DHI : 119.55454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
DHI : 119.55454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
DHI : 119.55454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 6/5055454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 6/5055454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 6/5055454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 6/5055454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
Epoch 10/505454036499638 34.139343598963976 -85.4151967660324155333752244 - val_loss: 0.0053e-04
ED : 0.00015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
ED : 0.00015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
ED : 0.00015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 4/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 4/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 4/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 4/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 9/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 9/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 9/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
Epoch 9/5015942490046484258 0.009302528086917183 0.0126263573711836062244 - val_loss: 0.0053e-04
ES : 0.0003133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
ES : 0.0003133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 3/503133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 5/503133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 5/503133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 8/503133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 10/50133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
Epoch 12/50133835453557377 0.014899145153462181 0.01770264232694480262244 - val_loss: 0.0053e-04
ADSK : 0.0006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
ADSK : 0.0006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ADSK : 0.0006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 4/50006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 4/50006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 7/50006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 7/50006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 10/5006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 10/5006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
Epoch 10/5006991363445028873 0.021181449168037995 0.026441186518439134244 - val_loss: 0.0053e-04
GL : 116.67549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 3/507549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 7/507549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 15/50549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 21/50549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 29/50549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
Epoch 29/50549752087628 51.22708932212752 -65.448408198748756518439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
INVH : 32.36328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 11/506328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 11/506328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 11/506328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 11/506328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 11/506328889855225 35.464645731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Data for IP appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 3/50P appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 5/50P appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 9/50P appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 12/50 appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 16/50 appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 16/50 appended successfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
y_train shape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
y_train shape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
y_train shape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
Epoch 4/50ape: (12415,)ccessfully.731079344 3.101356832527095318439134244 - val_loss: 0.0053e-04
KO : 0.00014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
KO : 0.00014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
KO : 0.00014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
KO : 0.00014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 5/5014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 5/5014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 5/5014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 9/5014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 9/5014083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 12/504083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
Epoch 12/504083942016355336 0.008083327156204696 0.0118675785299088444244 - val_loss: 0.0053e-04
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
PCAR : 8.096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 7/5096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 7/5096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 7/5096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 7/5096807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 12/506807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 12/506807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 12/506807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
Epoch 12/506807005582522e-05 0.00741581118916131 0.0089982259393630044244 - val_loss: 0.0053e-04
RVTY : 0.00011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
RVTY : 0.00011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
RVTY : 0.00011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 4/500011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 4/500011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 7/500011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 7/500011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 10/50011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 10/50011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 13/50011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
Epoch 13/50011614445063910248 0.007953088236778228 0.01077703348046680544 - val_loss: 0.0053e-04
WDC : 0.0001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
WDC : 0.0001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 3/5001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 11/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 19/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 26/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 34/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 42/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 42/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 50/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 50/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 50/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 50/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 50/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 6/5001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 6/5001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 6/5001704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 10/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 10/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 10/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 14/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 14/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 17/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 17/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 17/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
Epoch 17/501704932439101291 0.009376073688850153 0.0130573061505859270544 - val_loss: 0.0053e-04
NEE : 0.0006981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
NEE : 0.0006981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 3/5006981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 6/5006981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 9/5006981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 11/506981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 14/506981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
Epoch 14/506981450147315176 0.021112833273061776 0.0264224339289838640544 - val_loss: 0.0053e-04
UPS : 0.00023377926965515577 0.011515367858926394 0.015289842041537112544 - val_loss: 0.0053e-04
UPS : 0.00023377926965515577 0.011515367858926394 0.015289842041537112544 - val_loss: 0.0053e-04
Epoch 4/50023377926965515577 0.011515367858926394 0.015289842041537112544 - val_loss: 0.0053e-04
Epoch 7/50023377926965515577 0.011515367858926394 0.015289842041537112544 - val_loss: 0.0053e-04
Epoch 10/5023377926965515577 0.011515367858926394 0.015289842041537112544 - val_loss: 0.0053e-04
35/35 [==============================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
35/35 [==============================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
35/35 [==============================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 4/50===========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
Epoch 10/50==========================] - 0s 1ms/step015289842041537112544 - val_loss: 0.0053e-04
EMR : 0.0005343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
EMR : 0.0005343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 3/5005343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 6/5005343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 8/5005343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 10/505343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 12/505343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
Epoch 12/505343633839661699 0.018553885638106858 0.0231163012604994822544 - val_loss: 0.0053e-04
MSFT : 0.002793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
MSFT : 0.002793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
Epoch 3/5002793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
Epoch 6/5002793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
Epoch 8/5002793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
Epoch 11/502793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
Epoch 13/502793316756718741 0.03302672719366008 0.05285183777995559822544 - val_loss: 0.0053e-04
ANSS : 0.0007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
ANSS : 0.0007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
ANSS : 0.0007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 4/50007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 6/50007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 8/50007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 8/50007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 11/5007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 11/5007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
Epoch 11/5007286390020505398 0.019796582509977442 0.026993314024968105544 - val_loss: 0.0053e-04
CTAS : 0.0002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
CTAS : 0.0002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
CTAS : 0.0002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 4/50002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 6/50002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 6/50002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 9/50002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 9/50002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 12/5002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 12/5002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
Epoch 15/5002161100121327248 0.011834635693376698 0.014700680669027704544 - val_loss: 0.0053e-04
BIO : 0.0005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
BIO : 0.0005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
BIO : 0.0005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 4/5005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 4/5005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 7/5005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 9/5005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 9/5005265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 12/505265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 12/505265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
Epoch 12/505265756955508758 0.01394286760892515 0.02294723720953953704544 - val_loss: 0.0053e-04
UDR : 0.00019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 3/50019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 5/50019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 10/5019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 17/5019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 24/5019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 24/5019974167941803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
y_train shape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
y_train shape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
y_train shape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 5/50ape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 5/50ape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 8/50ape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 8/50ape: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 11/50pe: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 11/50pe: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
Epoch 11/50pe: (8763,)803192 0.0117808951763684 0.01413299966100728704544 - val_loss: 0.0053e-04
WEC : 0.00013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 3/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 16/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 32/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 48/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 48/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 1/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 1/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 1/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 1/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 1/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 8/50013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 10/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
Epoch 12/5013047953959036221 0.00880798035327062 0.0114227640958903744544 - val_loss: 0.0053e-04
AME : 7.429725167247957e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
AME : 7.429725167247957e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
y_train shape: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
y_train shape: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
Epoch 4/50ape: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
Epoch 6/50ape: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
Epoch 8/50ape: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
Epoch 10/50pe: (6021,)7e-05 0.006766749045365093 0.0086195853538601024544 - val_loss: 0.0053e-04
48/48 [==============================] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
48/48 [==============================] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Data for IT appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Data for IT appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Data for IT appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 5/50T appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 7/50T appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 7/50T appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 10/50 appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 12/50 appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 12/50 appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
Epoch 12/50 appended successfully.===] - 0s 1ms/step086195853538601024544 - val_loss: 0.0053e-04
DD : 0.0002626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
DD : 0.0002626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
Epoch 4/502626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
Epoch 8/502626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
Epoch 11/50626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
Epoch 15/50626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
Epoch 15/50626565702463115 0.011856796086578171 0.01620668288843560624544 - val_loss: 0.0053e-04
ACN : 0.0002523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
ACN : 0.0002523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 3/5002523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 5/5002523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 8/5002523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 10/502523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 13/502523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 16/502523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 18/502523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
Epoch 18/502523392927551256 0.011530035407821396 0.0158851909889407874544 - val_loss: 0.0053e-04
VRSN : 9.462640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
VRSN : 9.462640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
Epoch 3/5062640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
Epoch 6/5062640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
Epoch 9/5062640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
Epoch 12/502640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
Epoch 15/502640015760575e-05 0.0076572275279454165 0.00972761019765932844 - val_loss: 0.0053e-04
EW : 0.0002841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
EW : 0.0002841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/502841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
Epoch 7/502841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
Epoch 10/50841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
Epoch 10/50841231311482347 0.01235210161067032 0.016855952395169925932844 - val_loss: 0.0053e-04
28/28 [==============================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
28/28 [==============================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 3/50===========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 6/50===========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 15/50==========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 20/50==========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 24/50==========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
Epoch 28/50==========================] - 0s 1ms/step855952395169925932844 - val_loss: 0.0053e-04
AWK : 130.14657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
AWK : 130.14657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
AWK : 130.14657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
Epoch 4/5014657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
Epoch 6/5014657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
Epoch 6/5014657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
Epoch 9/5014657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
Epoch 9/5014657909806758 156.68799192261338 26.54141282454585169925932844 - val_loss: 0.0053e-04
64/64 [==============================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
64/64 [==============================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
SHW64 [==============================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
SHW64 [==============================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
SHW64 [==============================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 5/50===========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 8/50===========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 11/50==========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
Epoch 13/50==========================] - 0s 1ms/step282454585169925932844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
SHW : 0.0002570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 12/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 25/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 25/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 25/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
Epoch 25/502570542353199186 0.01203853294776737 0.01603291100580049232844 - val_loss: 0.0053e-04
HPQ : 27.2365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
HPQ : 27.2365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 3/50365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 5/50365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 5/50365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 8/50365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 8/50365911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 11/5065911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 11/5065911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 11/5065911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
Epoch 11/5065911030794 16.813167734818936 -10.423423368260465580049232844 - val_loss: 0.0053e-04
AMAT : 138.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
AMAT : 138.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 3/50.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 5/50.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 7/50.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 9/50.363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 11/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 13/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 15/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 17/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 17/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 20/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 22/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 24/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
Epoch 26/50363081607929 23.73083081291318 -114.63225079501585580049232844 - val_loss: 0.0053e-04
CCL : 0.00047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
CCL : 0.00047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
CCL : 0.00047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
Epoch 4/50047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
Epoch 7/50047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
Epoch 9/50047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
Epoch 11/5047425784525528364 0.015436773736293137 0.021777461864397413844 - val_loss: 0.0053e-04
MLM : 0.00033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
MLM : 0.00033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
MLM : 0.00033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
MLM : 0.00033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 5/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 5/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 5/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 9/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 9/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 9/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
Epoch 9/50033737061217290365 0.012914854885216957 0.018367651242684885844 - val_loss: 0.0053e-04
AVY : 0.0001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
AVY : 0.0001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
AVY : 0.0001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
AVY : 0.0001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 5/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 5/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 5/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 9/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 9/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 9/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 9/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
Epoch 9/5001692819970104405 0.010495960995974569 0.0130108415181509585844 - val_loss: 0.0053e-04
EVRG : 48.62522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
EVRG : 48.62522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 4/5062522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 6/5062522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 8/5062522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 10/502522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 10/502522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
Epoch 10/502522129264038 27.32855194505033 -21.29666934759005481509585844 - val_loss: 0.0053e-04
EA : 0.0014948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
EA : 0.0014948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 3/504948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 3/504948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 6/504948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 8/504948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 8/504948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 11/50948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
Epoch 13/50948944407351384 0.035516263237449275 0.03866386479304853685844 - val_loss: 0.0053e-04
DE : 0.00020061769692379322 0.012347759853454389 0.0141639576716323685844 - val_loss: 0.0053e-04
DE : 0.00020061769692379322 0.012347759853454389 0.0141639576716323685844 - val_loss: 0.0053e-04
DE : 0.00020061769692379322 0.012347759853454389 0.0141639576716323685844 - val_loss: 0.0053e-04