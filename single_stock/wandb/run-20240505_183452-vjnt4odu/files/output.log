VLTO
Data for VLTO is empty after dropping NaN values. Skipping this stock.
SPG
X_train shape: (5982, 8, 1)
y_train shape: (5982,)
X_test shape: (1496, 8, 1)
y_test shape: (1496,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
187/187 [==============================] - 3s 7ms/step - loss: 0.0100 - val_loss: 9.5464e-04
Epoch 2/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 5.2850e-04
Epoch 3/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0015 - val_loss: 5.5553e-04
Epoch 4/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0012 - val_loss: 2.9759e-04
Epoch 5/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 6.2179e-04
Epoch 6/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0011 - val_loss: 4.2019e-04
Epoch 7/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 2.4868e-04
Epoch 8/50
187/187 [==============================] - 1s 5ms/step - loss: 0.0010 - val_loss: 2.6363e-04
Epoch 9/50
187/187 [==============================] - 1s 5ms/step - loss: 9.4013e-04 - val_loss: 3.0123e-04
Epoch 10/50
187/187 [==============================] - 1s 5ms/step - loss: 8.9241e-04 - val_loss: 4.9418e-04
Epoch 11/50
187/187 [==============================] - 1s 5ms/step - loss: 8.9771e-04 - val_loss: 2.6239e-04
Epoch 12/50
187/187 [==============================] - 1s 5ms/step - loss: 9.0989e-04 - val_loss: 9.9799e-04
Epoch 13/50
187/187 [==============================] - 1s 5ms/step - loss: 8.0848e-04 - val_loss: 4.9911e-04
Epoch 14/50
187/187 [==============================] - 1s 5ms/step - loss: 8.2653e-04 - val_loss: 4.6776e-04
Epoch 15/50
187/187 [==============================] - 1s 5ms/step - loss: 7.3110e-04 - val_loss: 4.2513e-04
Epoch 16/50
187/187 [==============================] - 1s 5ms/step - loss: 7.8567e-04 - val_loss: 2.1691e-04
Epoch 17/50
187/187 [==============================] - 1s 5ms/step - loss: 7.6884e-04 - val_loss: 2.8628e-04
Epoch 18/50
187/187 [==============================] - 1s 5ms/step - loss: 7.3275e-04 - val_loss: 2.0892e-04
Epoch 19/50
187/187 [==============================] - 1s 5ms/step - loss: 6.9661e-04 - val_loss: 3.1037e-04
Epoch 20/50
187/187 [==============================] - 1s 5ms/step - loss: 7.3480e-04 - val_loss: 2.3382e-04
Epoch 21/50
187/187 [==============================] - 1s 5ms/step - loss: 6.4508e-04 - val_loss: 2.1636e-04
Epoch 22/50
187/187 [==============================] - 1s 5ms/step - loss: 6.6821e-04 - val_loss: 5.9025e-04
Epoch 23/50
187/187 [==============================] - 1s 5ms/step - loss: 6.7007e-04 - val_loss: 4.9295e-04
Epoch 24/50
187/187 [==============================] - 1s 5ms/step - loss: 6.0850e-04 - val_loss: 2.1992e-04
Epoch 25/50
187/187 [==============================] - 1s 5ms/step - loss: 6.9867e-04 - val_loss: 2.5488e-04
Epoch 26/50
187/187 [==============================] - 1s 5ms/step - loss: 6.4362e-04 - val_loss: 1.8772e-04
Epoch 27/50
187/187 [==============================] - 1s 5ms/step - loss: 6.1174e-04 - val_loss: 8.8307e-04
Epoch 28/50
187/187 [==============================] - 1s 5ms/step - loss: 6.4661e-04 - val_loss: 2.3643e-04
Epoch 29/50
187/187 [==============================] - 1s 5ms/step - loss: 6.0315e-04 - val_loss: 2.1159e-04
Epoch 30/50
187/187 [==============================] - 1s 5ms/step - loss: 6.0743e-04 - val_loss: 5.7018e-04
Epoch 31/50
187/187 [==============================] - 1s 5ms/step - loss: 6.4692e-04 - val_loss: 3.3028e-04
Epoch 32/50
187/187 [==============================] - 1s 5ms/step - loss: 6.8727e-04 - val_loss: 1.8235e-04
Epoch 33/50
187/187 [==============================] - 1s 5ms/step - loss: 6.3834e-04 - val_loss: 2.1687e-04
Epoch 34/50
187/187 [==============================] - 1s 5ms/step - loss: 6.2044e-04 - val_loss: 6.9476e-04
Epoch 35/50
187/187 [==============================] - 1s 5ms/step - loss: 6.2274e-04 - val_loss: 2.3444e-04
Epoch 36/50
187/187 [==============================] - 1s 5ms/step - loss: 6.2440e-04 - val_loss: 3.2971e-04
Epoch 37/50
187/187 [==============================] - 1s 5ms/step - loss: 6.1460e-04 - val_loss: 2.1060e-04
Epoch 38/50
187/187 [==============================] - 1s 5ms/step - loss: 5.9305e-04 - val_loss: 4.5926e-04
Epoch 39/50
187/187 [==============================] - 1s 5ms/step - loss: 6.2794e-04 - val_loss: 2.6598e-04
Epoch 40/50
187/187 [==============================] - 1s 5ms/step - loss: 6.0999e-04 - val_loss: 2.5716e-04
Epoch 41/50
187/187 [==============================] - 1s 5ms/step - loss: 6.2447e-04 - val_loss: 2.8336e-04
Epoch 42/50
187/187 [==============================] - 1s 5ms/step - loss: 5.8768e-04 - val_loss: 1.8718e-04
47/47 [==============================] - 0s 1ms/step
47/47 [==============================] - 0s 1ms/step
SPG : 0.0001823497913661005 0.008665092988495614 0.0135036954707258
SPG : 117.448280697991 158.82926230567955 41.380981607688554
Data for SPG appended successfully.
AMD
X_train shape: (8763, 8, 1)
y_train shape: (8763,)
X_test shape: (2191, 8, 1)
y_test shape: (2191,)
Epoch 1/50
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
274/274 [==============================] - 3s 6ms/step - loss: 4.3405e-04 - val_loss: 3.9954e-04
Epoch 2/50
274/274 [==============================] - 1s 5ms/step - loss: 9.7188e-05 - val_loss: 8.6131e-04
Epoch 3/50
274/274 [==============================] - 1s 5ms/step - loss: 8.2300e-05 - val_loss: 5.5471e-04
Epoch 4/50
253/274 [==========================>...] - ETA: 0s - loss: 7.6022e-05
274/274 [==============================] - 1s 5ms/step - loss: 7.1029e-05 - val_loss: 6.6901e-04
Epoch 6/50
109/274 [==========>...................] - ETA: 0s - loss: 6.1262e-05
268/274 [============================>.] - ETA: 0s - loss: 6.2422e-05e-05 - val_loss: 6.6901e-04
144/274 [==============>...............] - ETA: 0s - loss: 6.0298e-05e-05 - val_loss: 0.0011e-04
274/274 [==============================] - 1s 5ms/step - loss: 5.8493e-05 - val_loss: 0.0033e-04
274/274 [==============================] - 1s 5ms/step - loss: 5.8042e-05 - val_loss: 0.0070e-04
y_train shape: (4112,)91 1.7738459021738293 -110.919808340732085.8042e-05 - val_loss: 0.0070e-04
X_test shape: (1029, 8, 1)
y_test shape: (1029,)
Epoch 1/50
y_train shape: (4112,)91 1.7738459021738293 -110.919808340732085.8042e-05 - val_loss: 0.0070e-04
129/129 [==============================] - 3s 11ms/step - loss: 6.8553e-04 - val_loss: 0.0034-04
Epoch 2/50
 37/129 [=======>......................] - ETA: 0s - loss: 2.2684e-04
 85/129 [==================>...........] - ETA: 0s - loss: 1.2608e-04e-04 - val_loss: 0.00104-04
Epoch 4/50
129/129 [==============================] - 1s 5ms/step - loss: 1.3558e-04 - val_loss: 0.0019
Epoch 5/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0969e-04 - val_loss: 7.7976e-04
Epoch 6/50
129/129 [==============================] - 1s 5ms/step - loss: 1.1772e-04 - val_loss: 8.8912e-04
Epoch 7/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0969e-04 - val_loss: 7.7976e-04
Epoch 8/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0152e-04 - val_loss: 9.0377e-04
Epoch 9/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0273e-04 - val_loss: 0.0010
Epoch 10/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0534e-04 - val_loss: 9.7065e-04
Epoch 11/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0152e-04 - val_loss: 9.0377e-04
Epoch 12/50
129/129 [==============================] - 1s 5ms/step - loss: 9.9503e-05 - val_loss: 0.0011
Epoch 13/50
129/129 [==============================] - 1s 5ms/step - loss: 1.0166e-04 - val_loss: 0.0012
Epoch 14/50
129/129 [==============================] - 1s 5ms/step - loss: 9.5856e-05 - val_loss: 0.0018
Epoch 15/50
 73/129 [===============>..............] - ETA: 0s - loss: 1.0112e-04
129/129 [==============================] - 1s 5ms/step - loss: 9.3711e-05 - val_loss: 0.0023
Epoch 17/50
129/129 [==============================] - 1s 5ms/step - loss: 9.4100e-05 - val_loss: 0.0017
33/33 [==============================] - 0s 1ms/step
33/33 [==============================] - 0s 1ms/step
KLAC : 0.0007797631070436445 0.019794463987463137 0.027924238701236684
KLAC : 50.243228787627295 18.354493676684797 -31.8887351109425
NDAQ129 [==============================] - 1s 5ms/step - loss: 9.3711e-05 - val_loss: 0.0023
X_train shape: (4260, 8, 1)
y_train shape: (4260,)
X_test shape: (1066, 8, 1)
y_test shape: (1066,)
Epoch 1/50
NDAQ129 [==============================] - 1s 5ms/step - loss: 9.3711e-05 - val_loss: 0.0023
134/134 [==============================] - 1s 5ms/step - loss: 6.1085e-04 - val_loss: 0.0021
Epoch 3/50
 48/134 [=========>....................] - ETA: 0s - loss: 4.4960e-04
 85/134 [==================>...........] - ETA: 0s - loss: 3.0784e-04e-04 - val_loss: 0.0021
134/134 [==============================] - 1s 5ms/step - loss: 2.8273e-04 - val_loss: 9.4410e-04
134/134 [==============================] - 1s 5ms/step - loss: 2.2576e-04 - val_loss: 0.0034e-04
URI/134 [==============================] - 1s 5ms/step - loss: 2.2576e-04 - val_loss: 0.0034e-04
URI/134 [==============================] - 1s 5ms/step - loss: 2.2576e-04 - val_loss: 0.0034e-04
162/162 [==============================] - 1s 5ms/step - loss: 2.3534e-04 - val_loss: 1.7550e-04
162/162 [==============================] - 1s 5ms/step - loss: 2.3534e-04 - val_loss: 1.7550e-04
162/162 [==============================] - 1s 5ms/step - loss: 1.2519e-04 - val_loss: 2.6909e-04
151/162 [==========================>...] - ETA: 0s - loss: 9.2626e-05e-04 - val_loss: 2.6909e-04
162/162 [==============================] - 1s 5ms/step - loss: 9.6314e-05 - val_loss: 0.0012e-04
162/162 [==============================] - 1s 5ms/step - loss: 8.0863e-05 - val_loss: 0.0012e-04
RTX/162 [==============================] - 1s 5ms/step - loss: 8.0863e-05 - val_loss: 0.0012e-04
RTX/162 [==============================] - 1s 5ms/step - loss: 8.0863e-05 - val_loss: 0.0012e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
301/387 [======================>.......] - ETA: 0s - loss: 2.9476e-04e-05 - val_loss: 0.0012e-04
387/387 [==============================] - 2s 5ms/step - loss: 2.8075e-04 - val_loss: 0.0022e-04
387/387 [==============================] - 2s 5ms/step - loss: 2.8075e-04 - val_loss: 0.0022e-04
  1/387 [..............................] - ETA: 1s - loss: 1.2769e-04e-04 - val_loss: 0.0022e-04
387/387 [==============================] - 2s 5ms/step - loss: 2.0690e-04 - val_loss: 9.0600e-04
387/387 [==============================] - 2s 5ms/step - loss: 2.0690e-04 - val_loss: 9.0600e-04
387/387 [==============================] - 2s 5ms/step - loss: 2.0690e-04 - val_loss: 9.0600e-04
 37/387 [=>............................] - ETA: 1s - loss: 1.6676e-04e-04 - val_loss: 9.0600e-04
387/387 [==============================] - 2s 5ms/step - loss: 1.7757e-04 - val_loss: 0.0112e-04
387/387 [==============================] - 2s 5ms/step - loss: 1.7757e-04 - val_loss: 0.0112e-04
387/387 [==============================] - 2s 5ms/step - loss: 1.7757e-04 - val_loss: 0.0112e-04
97/97 [==============================] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
y_train shape: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
y_train shape: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 3/50ape: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 6/50ape: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 14/50pe: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 18/50pe: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 22/50pe: (2628,)===============] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
X_train shape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
X_train shape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50ape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 3/50ape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 3/50ape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 7/50ape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 7/50ape: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 10/50pe: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 10/50pe: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
Epoch 13/50pe: (9614, 8, 1)==========] - 0s 1ms/stepep - loss: 1.7757e-04 - val_loss: 0.0112e-04
PNC : 0.0001826666921816302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
y_train shape: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
y_train shape: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 3/50ape: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 5/50ape: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 7/50ape: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 10/50pe: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 12/50pe: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
Epoch 14/50pe: (5347,)16302 0.00988825340921909 0.0135154242323957487e-04 - val_loss: 0.0112e-04
KMX : 66.98781860706764 76.65645643083126 9.6686378237636182323957487e-04 - val_loss: 0.0112e-04
KMX : 66.98781860706764 76.65645643083126 9.6686378237636182323957487e-04 - val_loss: 0.0112e-04
23/52 [============>.................] - ETA: 0s - loss: 0.0035.0389 - val_loss: 0.0074.0112e-04
13/52 [======>.......................] - ETA: 0s - loss: 0.00190025 - val_loss: 7.8895e-0412e-04
52/52 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 4.8962e-0412e-04
52/52 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 5.9788e-0412e-04
52/52 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 5.0107e-0412e-04
52/52 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 6.0433e-0412e-04
52/52 [==============================] - 0s 6ms/step - loss: 0.0010 - val_loss: 3.8906e-0412e-04
MTCH2 [==============================] - 0s 6ms/step - loss: 0.0010 - val_loss: 3.8906e-0412e-04
MTCH2 [==============================] - 0s 6ms/step - loss: 0.0010 - val_loss: 3.8906e-0412e-04
 48/193 [======>.......................] - ETA: 0s - loss: 8.1704e-05 val_loss: 3.8906e-0412e-04
193/193 [==============================] - 1s 5ms/step - loss: 5.8116e-05 - val_loss: 0.0085e-04
109/193 [===============>..............] - ETA: 0s - loss: 2.9422e-05e-05 - val_loss: 0.0085e-04
193/193 [==============================] - 1s 5ms/step - loss: 2.4760e-05 - val_loss: 0.0032e-04
181/193 [===========================>..] - ETA: 0s - loss: 1.8410e-05e-05 - val_loss: 0.0032e-04
193/193 [==============================] - 1s 5ms/step - loss: 1.8905e-05 - val_loss: 0.0090e-04
193/193 [==============================] - 1s 5ms/step - loss: 1.5503e-05 - val_loss: 0.0098e-04
193/193 [==============================] - 1s 5ms/step - loss: 1.5503e-05 - val_loss: 0.0098e-04
MTCH : 28.92273941172113 23.619979114503444 -5.302760297217684 1.5503e-05 - val_loss: 0.0098e-04
MTCH : 28.92273941172113 23.619979114503444 -5.302760297217684 1.5503e-05 - val_loss: 0.0098e-04
202/202 [==============================] - 3s 7ms/step - loss: 0.0055 - val_loss: 4.8799e-04e-04
202/202 [==============================] - 3s 7ms/step - loss: 0.0055 - val_loss: 4.8799e-04e-04
202/202 [==============================] - 1s 5ms/step - loss: 6.4038e-04 - val_loss: 1.2286e-04
202/202 [==============================] - 1s 5ms/step - loss: 6.4038e-04 - val_loss: 1.2286e-04
202/202 [==============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 12/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 14/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 16/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 18/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 20/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 22/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 24/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 26/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 28/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 30/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 33/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 35/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 35/50============================] - 1s 5ms/step - loss: 4.8277e-04 - val_loss: 0.0019e-04
Epoch 1/5043693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 1/5043693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 3/5043693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 5/5043693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 8/5043693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 10/503693968687826e-05 0.006589480816960177 0.00907947904270274e-04 - val_loss: 0.0019e-04
Epoch 1/50008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 1/50008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 6/50008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 9/50008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 11/5008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 14/5008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 17/5008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 19/5008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
Epoch 19/5008429499191703081 0.02148323015917448 0.029033599831407544e-04 - val_loss: 0.0019e-04
CHRW : 81.83391059424014 95.77431445144087 13.94040385720073531407544e-04 - val_loss: 0.0019e-04
192/199 [===========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 3/50==========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 5/50==========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 8/50==========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 10/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 12/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 14/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 16/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 18/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
Epoch 18/50=========================>..] - ETA: 0s - loss: 0.0017  44e-04 - val_loss: 0.0019e-04
ROP : 503.1181684399018 241.08540691406037 -262.03276152584147017  44e-04 - val_loss: 0.0019e-04
ROP : 503.1181684399018 241.08540691406037 -262.03276152584147017  44e-04 - val_loss: 0.0019e-04
203/203 [==============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 4/50=============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 6/50=============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 8/50=============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 10/50============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 12/50============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 12/50============================] - 3s 7ms/step - loss: 3.9438e-04 - val_loss: 0.0045e-04
Epoch 1/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 1/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 1/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 4/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 4/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 4/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 4/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 9/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 9/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 9/5002095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 13/502095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 13/502095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 13/502095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 13/502095767740478577 0.030528883060557763 0.045779555922688658e-04 - val_loss: 0.0045e-04
Epoch 1/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 1/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 3/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 3/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 6/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 8/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 8/505301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 11/50301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 11/50301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 11/50301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 15/50301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
Epoch 15/50301597672225e-05 0.005494360573512839 0.007317992619340515e-04 - val_loss: 0.0045e-04
HES : 145.1992356069184 74.0374707013215 -71.161764905596992619340515e-04 - val_loss: 0.0045e-04
HES : 145.1992356069184 74.0374707013215 -71.161764905596992619340515e-04 - val_loss: 0.0045e-04
260/264 [============================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 3/50===========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 5/50===========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 5/50===========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 8/50===========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 10/50==========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 10/50==========================>.] - ETA: 0s - loss: 7.2450e-04e-04 - val_loss: 0.0045e-04
Epoch 1/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 1/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 5/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 7/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 9/5016393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 12/506393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
Epoch 14/506393871108828925 0.009872466342232416 0.012803855321280744e-04 - val_loss: 0.0045e-04
47/47 [==============================] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
47/47 [==============================] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
y_train shape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
y_train shape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
y_train shape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 5/50ape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 7/50ape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 7/50ape: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 10/50pe: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 12/50pe: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 12/50pe: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 15/50pe: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
Epoch 17/50pe: (8400,)===============] - 0s 1ms/step12803855321280744e-04 - val_loss: 0.0045e-04
VLO : 0.00014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 1/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 1/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 5/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 5/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 8/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 8/50014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 11/5014837184392086945 0.008538149032930445 0.012180798164359734-04 - val_loss: 0.0045e-04
Epoch 1/5004317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
Epoch 1/5004317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 8/5004317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
Epoch 12/504317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
Epoch 20/504317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
Epoch 20/504317564315868731 0.01635255889479399 0.02077874951932558734-04 - val_loss: 0.0045e-04
Epoch 1/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 1/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 1/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 1/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 5/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 5/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 8/50016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 10/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 10/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 13/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 13/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 16/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 16/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 16/5016254715636855962 0.009249785593087577 0.012749398274764172-04 - val_loss: 0.0045e-04
Epoch 1/50031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 1/50031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 3/50031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 11/5031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 15/5031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 15/5031203144471792627 0.013307264271530825 0.017664411813528532-04 - val_loss: 0.0045e-04
Epoch 1/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 1/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 1/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 4/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 4/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 7/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 7/5016240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 10/506240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 12/506240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 12/506240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
Epoch 15/506240768729449754 0.00980152863910531 0.01274392746740570332-04 - val_loss: 0.0045e-04
TYL : 0.0002361127387749372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
TYL : 0.0002361127387749372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
y_train shape: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
y_train shape: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 4/50ape: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 6/50ape: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 9/50ape: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 11/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 14/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 16/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 19/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50pe: (5576,)49372 0.01126749108988863 0.01536596039220904832-04 - val_loss: 0.0045e-04
X_test shape: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
X_test shape: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
X_test shape: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
X_test shape: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
X_test shape: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 6/50pe: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 6/50pe: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 9/50pe: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 9/50pe: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 9/50pe: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 13/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 13/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 13/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 17/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 17/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 17/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 21/50e: (2548, 8, 1)===========] - 0s 1ms/step536596039220904832-04 - val_loss: 0.0045e-04
Epoch 1/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 1/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 3/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 3/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 6/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 6/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 9/503624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 11/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 11/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 14/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 14/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 17/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 17/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 17/50624563115407e-05 0.006521405093900947 0.0089964573933940292-04 - val_loss: 0.0045e-04
Epoch 1/5004227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 1/5004227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 3/5004227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 5/5004227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 8/5004227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 10/504227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 12/504227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 15/504227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
Epoch 17/504227710999505184 0.015686682431188394 0.0205613982975506422-04 - val_loss: 0.0045e-04
X_test shape: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
X_test shape: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
X_test shape: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 4/50pe: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 6/50pe: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 8/50pe: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 10/50e: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 10/50e: (1811, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
X_test shape: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
X_test shape: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 3/50pe: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 6/50pe: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 9/50pe: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 12/50e: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 14/50e: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 14/50e: (1400, 8, 1)===========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
X_train shape: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
X_train shape: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 4/50ape: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 6/50ape: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 8/50ape: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 11/50pe: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 13/50pe: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 13/50pe: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 16/50pe: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 16/50pe: (6530, 8, 1)==========] - 0s 1ms/step205613982975506422-04 - val_loss: 0.0045e-04
Epoch 1/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 1/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 3/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 5/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 5/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 8/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 8/50027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 11/5027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 11/5027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
Epoch 11/5027867187924658627 0.013265999004746768 0.016693468161127762-04 - val_loss: 0.0045e-04
AXP : 152.08724444569145 72.21946912480497 -79.86777532088648161127762-04 - val_loss: 0.0045e-04
48/50 [===========================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
Epoch 7/50========================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
Epoch 15/50=======================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
Epoch 23/50=======================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
Epoch 31/50=======================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
Epoch 39/50=======================>..] - ETA: 0s - loss: 0.0635  27762-04 - val_loss: 0.0045e-04
HPE : 0.00024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
HPE : 0.00024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 1/50024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 1/50024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 5/50024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 9/50024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 13/5024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 17/5024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 21/5024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
Epoch 21/5024223193261372602 0.012038753466711351 0.015563801997382453-04 - val_loss: 0.0045e-04
X_test shape: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
X_test shape: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
X_test shape: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 4/50pe: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 4/50pe: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 8/50pe: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 8/50pe: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 8/50pe: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 12/50e: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 12/50e: (2191, 8, 1)===========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
X_train shape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
X_train shape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
X_train shape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 4/50ape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 4/50ape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 7/50ape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 9/50ape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 9/50ape: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 12/50pe: (8400, 8, 1)==========] - 0s 1ms/step015563801997382453-04 - val_loss: 0.0045e-04
Epoch 1/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 1/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
Epoch 1/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 1/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 5/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 5/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 5/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 9/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 9/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 9/5022923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 13/502923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 13/502923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
Epoch 13/502923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
ZBHch 13/502923772136420294 0.0118533133019196 0.015140598448020572453-04 - val_loss: 0.0045e-04
139/139 [==============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
Epoch 5/50=============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
Epoch 8/50=============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
Epoch 11/50============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
Epoch 14/50============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
Epoch 14/50============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 3.7448e-04e-04
ZBH : 109.00472545520994 118.45687909561813 9.452153640408198: 0.0019 - val_loss: 3.7448e-04e-04
131/131 [==============================] - 3s 8ms/step - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 5/50=============================] - 3s 8ms/step - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 8/50=============================] - 3s 8ms/step - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 11/50============================] - 3s 8ms/step - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 14/50============================] - 3s 8ms/step - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
X_test shape: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 17/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 23/50e: (3104, 8, 1)===========] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Data for XOM appended successfully.==] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 2/50OM appended successfully.==] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 5/50OM appended successfully.==] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
Epoch 8/50OM appended successfully.==] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
29/29 [==============================] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04
29/29 [==============================] - 0s 1ms/stepep - loss: 0.0110 - val_loss: 0.0030e-04e-04