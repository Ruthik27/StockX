{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e32dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478093b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beadc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#import csv\n",
    "#import os\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "#from selenium.webdriver.common.action_chains import ActionChains\n",
    "#from selenium.common.exceptions import StaleElementReferenceException\n",
    "#import logging\n",
    "#from selenium.common.exceptions import TimeoutException\n",
    "#\n",
    "#import time\n",
    "#from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "#\n",
    "#OUTPUT_FILE_TEMPLATE = \"aapl_historical_data_{time_period}_{show}_{frequency}.csv\"\n",
    "#\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "#\n",
    "## Constants\n",
    "#BASE_URL = \"https://finance.yahoo.com/quote/AAPL/history\"\n",
    "#OUTPUT_FILE = \"aapl_historical_data.csv\"\n",
    "#DATE_RANGE_ID = 'date-range-selector'  # Update this with the actual ID or selector for the date range element\n",
    "#FREQUENCY_SELECTOR = 'frequency-selector'  # Update this with the actual ID or selector for the frequency dropdown\n",
    "#\n",
    "#def setup_driver():\n",
    "#    options = FirefoxOptions()\n",
    "#    driver = webdriver.Firefox(options=options)\n",
    "#    return driver\n",
    "#\n",
    "## Use logging in your functions\n",
    "#def navigate_to_page(driver, url):\n",
    "#    try:\n",
    "#        logging.info(f\"Navigating to {url}\")\n",
    "#        driver.get(url)\n",
    "#        time.sleep(5)  # Sleep to allow any pop-ups to appear that you might need to cancel manually\n",
    "#    except Exception as e:\n",
    "#        logging.error(\"An error occurred while trying to navigate to the page.\")\n",
    "#        logging.error(e)\n",
    "#\n",
    "#def select_time_period(driver, period):\n",
    "#    try:\n",
    "#        logging.info(f\"Selecting time period: {period}\")\n",
    "#        # Your code for selecting the time period...\n",
    "#    except TimeoutException as e:\n",
    "#        logging.error(f\"Timed out waiting for time period option: {period}\")\n",
    "#        logging.error(e)\n",
    "#    except Exception as e:\n",
    "#        logging.error(f\"An error occurred while selecting time period: {period}\")\n",
    "#        logging.error(e)\n",
    "#\n",
    "#def select_frequency(driver, frequency):\n",
    "#    try:\n",
    "#        logging.info(f\"Selecting frequency: {frequency}\")\n",
    "#        # Your code for selecting the frequency...\n",
    "#    except TimeoutException as e:\n",
    "#        logging.error(f\"Timed out waiting for frequency option: {frequency}\")\n",
    "#        logging.error(e)\n",
    "#    except Exception as e:\n",
    "#        logging.error(f\"An error occurred while selecting frequency: {frequency}\")\n",
    "#        logging.error(e)\n",
    "#\n",
    "##def extract_data(driver):\n",
    "##    try:\n",
    "##        logging.info(\"Extracting data from the table...\")\n",
    "##            # Wait for the table to load after applying filters\n",
    "##        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "##\n",
    "##        # Extract the data from the table\n",
    "##        data = []\n",
    "##        table = driver.find_element(By.TAG_NAME, 'table')\n",
    "##        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "##        for row in rows[1:]:  # Skip header row\n",
    "##            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "##            if cols:\n",
    "##                data_row = [col.text for col in cols]\n",
    "##                data.append(data_row)\n",
    "##    except Exception as e:\n",
    "##        logging.error(\"An error occurred while extracting data.\")\n",
    "##        logging.error(e)\n",
    "##        \n",
    "##    return data\n",
    "##\n",
    "#\n",
    "#\n",
    "#def extract_data(driver):\n",
    "#    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "#    \n",
    "#    # Scroll to the end of the page to ensure all data is loaded\n",
    "#    scroll_pause_time = 2  # You may need to adjust this value\n",
    "#    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "#    i = 1\n",
    "#\n",
    "#    while True:\n",
    "#        # Scroll one screen height each time\n",
    "#        driver.execute_script(f\"window.scrollTo(0, {screen_height}*{i});\")\n",
    "#        i += 1\n",
    "#        time.sleep(scroll_pause_time)\n",
    "#        \n",
    "#        # Try to see if there's a \"Show More\" button and click it\n",
    "#        try:\n",
    "#            show_more_button = driver.find_element(By.XPATH, '//button[contains(text(),\"Show More\")]')\n",
    "#            if show_more_button:\n",
    "#                driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "#                time.sleep(scroll_pause_time)\n",
    "#        except NoSuchElementException:\n",
    "#            pass  # If there's no button, we'll just continue scrolling\n",
    "#\n",
    "#        # After the scroll attempt, check if we're at the bottom\n",
    "#        scrolled_to_bottom = driver.execute_script(\n",
    "#            \"return document.documentElement.scrollHeight == document.documentElement.scrollTop + window.innerHeight;\")\n",
    "#        if scrolled_to_bottom:\n",
    "#            break  # If we're at the bottom, exit the loop\n",
    "#\n",
    "#    # Now that we've scrolled the entire page, we'll attempt to extract the data\n",
    "#    # Extract headers\n",
    "#    data = []\n",
    "#    try:\n",
    "#        # Re-find the table to avoid stale element issues\n",
    "#        table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'table')))\n",
    "#        headers = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'th')\n",
    "#        header_row = [header.text for header in headers]\n",
    "#        data.append(header_row)  # Save headers\n",
    "#\n",
    "#        # Extract body rows\n",
    "#        body_rows = table.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "#        for row in body_rows:\n",
    "#            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "#            if cols:\n",
    "#                data_row = [col.text for col in cols]\n",
    "#                data.append(data_row)\n",
    "#    except StaleElementReferenceException:\n",
    "#        # Handle stale elements\n",
    "#        pass  # You may want to retry finding the header and rows\n",
    "#    except Exception as e:\n",
    "#        logging.error(\"An error occurred while extracting data.\")\n",
    "#        logging.error(e)\n",
    "#\n",
    "#    return data\n",
    "#\n",
    "#\n",
    "#\n",
    "#def save_data_to_csv(data, filename):\n",
    "#    # Define the directory structure\n",
    "#    directory = \"yahoo_appl/yahoo_appl_history\"\n",
    "#\n",
    "#    # Ensure the directory exists\n",
    "#    if not os.path.exists(directory):\n",
    "#        os.makedirs(directory)\n",
    "#\n",
    "#    # Define the full file path\n",
    "#    filepath = os.path.join(directory, filename)\n",
    "#\n",
    "#    # Write the CSV file\n",
    "#    with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "#        writer = csv.writer(file)\n",
    "#        writer.writerows(data)\n",
    "#\n",
    "#\n",
    "#def main():\n",
    "#    driver = setup_driver()\n",
    "#    try:\n",
    "#        navigate_to_page(driver, BASE_URL)\n",
    "#\n",
    "#        # Add a 5-second delay after navigating to the page\n",
    "#        # This allows time for manual intervention to close any pop-ups\n",
    "#        time.sleep(5)\n",
    "#        \n",
    "#        time_periods = [\"1D\", \"5D\", \"3M\", \"6M\", \"YTD\", \"1Y\", \"5Y\", \"Max\"]\n",
    "#        show_options = [\"Historical Prices\", \"Dividends Only\", \"Stock Splits\", \"Capital Gain\"]\n",
    "#        frequencies = [\"Daily\", \"Weekly\", \"Monthly\"]\n",
    "#\n",
    "#        for period in time_periods:\n",
    "#            for show in show_options:\n",
    "#                for frequency in frequencies:\n",
    "#                    select_time_period(driver, period)\n",
    "#                    select_show_option(driver, show)\n",
    "#                    select_frequency(driver, frequency)\n",
    "#                    apply_filters(driver)\n",
    "#                    \n",
    "#                    data = extract_data(driver)\n",
    "#                    filename = OUTPUT_FILE_TEMPLATE.format(time_period=period, show=show.replace(\" \", \"_\"), frequency=frequency)\n",
    "#                    save_data_to_csv(data, filename)\n",
    "#                    \n",
    "#                    # It's a good idea to pause between requests to avoid being rate-limited or banned\n",
    "#                    time.sleep(2)\n",
    "#                    \n",
    "#    except Exception as e:\n",
    "#        logging.error(\"An error occurred in the main function.\")\n",
    "#        logging.error(e)\n",
    "#    finally:\n",
    "#        driver.quit()\n",
    "#\n",
    "#\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c9da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import logging\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "OUTPUT_FILE_TEMPLATE = \"aapl_historical_data_{time_period}_{show}_{frequency}.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://finance.yahoo.com/quote/AAPL/history\"\n",
    "OUTPUT_FILE = \"aapl_historical_data.csv\"\n",
    "DATE_RANGE_ID = 'date-range-selector'  # Update this with the actual ID or selector for the date range element\n",
    "FREQUENCY_SELECTOR = 'frequency-selector'  # Update this with the actual ID or selector for the frequency dropdown\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "# Use logging in your functions\n",
    "def navigate_to_page(driver, url):\n",
    "    try:\n",
    "        logging.info(f\"Navigating to {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Sleep to allow any pop-ups to appear that you might need to cancel manually\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while trying to navigate to the page.\")\n",
    "        logging.error(e)\n",
    "\n",
    "def select_time_period(driver, period):\n",
    "    try:\n",
    "        logging.info(f\"Selecting time period: {period}\")\n",
    "        # Click the date range dropdown to reveal the options\n",
    "        date_range_dropdown = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"svg[data-icon='CoreArrowDown']\"))\n",
    "        )\n",
    "        date_range_dropdown.click()\n",
    "        time.sleep(2)  # Wait a moment for the dropdown options to appear\n",
    "\n",
    "        # Now that the dropdown is open, click the 'Max' option\n",
    "        max_option = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[@data-value='MAX']\"))\n",
    "        )\n",
    "        max_option.click()\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find the time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting time period: {period}\")\n",
    "        logging.error(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_frequency(driver, frequency):\n",
    "    try:\n",
    "        logging.info(f\"Selecting frequency: {frequency}\")\n",
    "        # Your code for selecting the frequency...\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for frequency option: {frequency}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting frequency: {frequency}\")\n",
    "        logging.error(e)\n",
    "\n",
    "#def extract_data(driver):\n",
    "#    try:\n",
    "#        logging.info(\"Extracting data from the table...\")\n",
    "#            # Wait for the table to load after applying filters\n",
    "#        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "#\n",
    "#        # Extract the data from the table\n",
    "#        data = []\n",
    "#        table = driver.find_element(By.TAG_NAME, 'table')\n",
    "#        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "#        for row in rows[1:]:  # Skip header row\n",
    "#            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "#            if cols:\n",
    "#                data_row = [col.text for col in cols]\n",
    "#                data.append(data_row)\n",
    "#    except Exception as e:\n",
    "#        logging.error(\"An error occurred while extracting data.\")\n",
    "#        logging.error(e)\n",
    "#        \n",
    "#    return data\n",
    "#\n",
    "\n",
    "\n",
    "def extract_data(driver):\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "    \n",
    "    # Scroll to the end of the page to ensure all data is loaded\n",
    "    scroll_pause_time = 2  # You may need to adjust this value\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # Scroll one screen height each time\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height}*{i});\")\n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        \n",
    "        # Try to see if there's a \"Show More\" button and click it\n",
    "        try:\n",
    "            show_more_button = driver.find_element(By.XPATH, '//button[contains(text(),\"Show More\")]')\n",
    "            if show_more_button:\n",
    "                driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "                time.sleep(scroll_pause_time)\n",
    "        except NoSuchElementException:\n",
    "            pass  # If there's no button, we'll just continue scrolling\n",
    "\n",
    "        # After the scroll attempt, check if we're at the bottom\n",
    "        scrolled_to_bottom = driver.execute_script(\n",
    "            \"return document.documentElement.scrollHeight == document.documentElement.scrollTop + window.innerHeight;\")\n",
    "        if scrolled_to_bottom:\n",
    "            break  # If we're at the bottom, exit the loop\n",
    "\n",
    "    # Now that we've scrolled the entire page, we'll attempt to extract the data\n",
    "    # Extract headers\n",
    "    data = []\n",
    "    try:\n",
    "        # Re-find the table to avoid stale element issues\n",
    "        table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'table')))\n",
    "        headers = table.find_element(By.TAG_NAME, 'thead').find_elements(By.TAG_NAME, 'th')\n",
    "        header_row = [header.text for header in headers]\n",
    "        data.append(header_row)  # Save headers\n",
    "\n",
    "        # Extract body rows\n",
    "        body_rows = table.find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "        for row in body_rows:\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if cols:\n",
    "                data_row = [col.text for col in cols]\n",
    "                data.append(data_row)\n",
    "    except StaleElementReferenceException:\n",
    "        # Handle stale elements\n",
    "        pass  # You may want to retry finding the header and rows\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while extracting data.\")\n",
    "        logging.error(e)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def save_data_to_csv(data, filename):\n",
    "    # Define the directory structure\n",
    "    directory = \"yahoo_appl/yahoo_appl_history\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Define the full file path\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    # Write the CSV file\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        navigate_to_page(driver, BASE_URL)\n",
    "\n",
    "        # Add a 5-second delay after navigating to the page\n",
    "        # This allows time for manual intervention to close any pop-ups\n",
    "        time.sleep(5)\n",
    "        \n",
    "        time_periods = [\"Max\"]\n",
    "        show_options = [\"Historical Prices\"]\n",
    "        frequencies = [\"Daily\"]\n",
    "\n",
    "        for period in time_periods:\n",
    "            for show in show_options:\n",
    "                for frequency in frequencies:\n",
    "                    select_time_period(driver, period)\n",
    "                    select_show_option(driver, show)\n",
    "                    select_frequency(driver, frequency)\n",
    "                    apply_filters(driver)\n",
    "                    \n",
    "                    data = extract_data(driver)\n",
    "                    filename = OUTPUT_FILE_TEMPLATE.format(time_period=period, show=show.replace(\" \", \"_\"), frequency=frequency)\n",
    "                    save_data_to_csv(data, filename)\n",
    "                    \n",
    "                    # It's a good idea to pause between requests to avoid being rate-limited or banned\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in the main function.\")\n",
    "        logging.error(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094688f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f99cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c250917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 13:26:11,102 - INFO - Navigating to https://finance.yahoo.com/quote/AAPL/key-statistics?p=AAPL\n",
      "2023-11-05 13:26:12,897 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x146bfdd50>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/2825cd9f-e888-4f61-a022-93a756b3b4b2\n",
      "2023-11-05 13:26:12,899 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x146bfc890>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/2825cd9f-e888-4f61-a022-93a756b3b4b2\n",
      "2023-11-05 13:26:12,900 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x146bff490>: Failed to establish a new connection: [Errno 61] Connection refused')': /session/2825cd9f-e888-4f61-a022-93a756b3b4b2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[68], line 58\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     navigate_to_page(driver, BASE_URL)\n\u001b[1;32m     59\u001b[0m     data \u001b[38;5;241m=\u001b[39m extract_data(driver)\n\u001b[1;32m     60\u001b[0m     save_data_to_csv(data, OUTPUT_DIR, OUTPUT_FILE)\n",
      "Cell \u001b[0;32mIn[68], line 24\u001b[0m, in \u001b[0;36mnavigate_to_page\u001b[0;34m(driver, url)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnavigate_to_page\u001b[39m(driver, url):\n\u001b[1;32m     23\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNavigating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Wait for the content to load\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:342\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    340\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 342\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:294\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    292\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    293\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:315\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    312\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 315\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    316\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://finance.yahoo.com/quote/AAPL/key-statistics?p=AAPL\"\n",
    "OUTPUT_DIR = \"yahoo_appl/yahoo_statistics\"\n",
    "OUTPUT_FILE = \"aapl_key_statistics.csv\"\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "def navigate_to_page(driver, url):\n",
    "    logging.info(f\"Navigating to {url}\")\n",
    "    driver.get(url)\n",
    "    # Wait for the content to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "def extract_data(driver):\n",
    "    data = []\n",
    "    try:\n",
    "        # Assuming the data is in a table with an identifiable selector\n",
    "        table = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'table')))\n",
    "        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if cols:\n",
    "                data_row = [col.text for col in cols]\n",
    "                data.append(data_row)\n",
    "        logging.info(\"Data extracted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while extracting data from the page.\")\n",
    "        logging.error(e)\n",
    "    return data\n",
    "\n",
    "def save_data_to_csv(data, directory, filename):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "    logging.info(f\"Data saved to {filepath}\")\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        navigate_to_page(driver, BASE_URL)\n",
    "        data = extract_data(driver)\n",
    "        save_data_to_csv(data, OUTPUT_DIR, OUTPUT_FILE)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in the main function.\")\n",
    "        logging.error(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d54c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f060114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 02:28:03,884 - INFO - Starting the web driver.\n",
      "2023-11-05 02:28:06,082 - INFO - Navigating to the page.\n",
      "2023-11-05 02:28:06,083 - INFO - Navigating to https://finance.yahoo.com/quote/AAPL/analysis\n",
      "2023-11-05 02:28:12,942 - INFO - Extracting data.\n",
      "2023-11-05 02:28:12,956 - INFO - Extracting data from all tables...\n",
      "2023-11-05 02:28:13,452 - INFO - Data extraction complete.\n",
      "2023-11-05 02:28:13,452 - INFO - Data extracted: [[['Earnings Estimate', 'Current Qtr. (Sep 2023)', 'Next Qtr. (Dec 2023)', 'Current Year (2023)', 'Next Year (2024)'], ['No. of Analysts', '28', '25', '36', '36'], ['Avg. Estimate', '1.31', '1.98', '5.72', '6.19'], ['Low Estimate', '1.27', '1.62', '5.49', '5.28'], ['High Estimate', '1.37', '2.27', '5.82', '6.69'], ['Year Ago EPS', '1.29', '1.88', '6.11', '5.72']], [['Revenue Estimate', 'Current Qtr. (Sep 2023)', 'Next Qtr. (Dec 2023)', 'Current Year (2023)', 'Next Year (2024)'], ['No. of Analysts', '24', '23', '35', '35'], ['Avg. Estimate', '84.18B', '116.37B', '360.83B', '382.24B'], ['Low Estimate', '82.8B', '110.21B', '344B', '364.64B'], ['High Estimate', '86.89B', '125.38B', '363.96B', '394.25B'], ['Year Ago Sales', '90.15B', 'N/A', '394.33B', '360.83B'], ['Sales Growth (year/est)', '-6.60%', 'N/A', '-8.50%', '5.90%']], [['Earnings History', '9/29/2022', '12/30/2022', '3/30/2023', '6/29/2023'], ['EPS Est.', '1.27', '1.94', '1.43', '1.19'], ['EPS Actual', '1.29', '1.88', '1.52', '1.26'], ['Difference', '0.02', '-0.06', '0.09', '0.07'], ['Surprise %', '1.60%', '-3.10%', '6.30%', '5.90%']], [['EPS Trend', 'Current Qtr. (Sep 2023)', 'Next Qtr. (Dec 2023)', 'Current Year (2023)', 'Next Year (2024)'], ['Current Estimate', '1.31', '1.98', '5.72', '6.19'], ['7 Days Ago', '1.39', '2.11', '6.06', '6.57'], ['30 Days Ago', '1.3', '1.98', '5.7', '6.19'], ['60 Days Ago', '1.39', '2.12', '6.07', '6.61'], ['90 Days Ago', '1.22', '1.93', '5.38', '5.92']], [['EPS Revisions', 'Current Qtr. (Sep 2023)', 'Next Qtr. (Dec 2023)', 'Current Year (2023)', 'Next Year (2024)'], ['Up Last 7 Days', 'N/A', 'N/A', 'N/A', 'N/A'], ['Up Last 30 Days', 'N/A', '1', '1', 'N/A'], ['Down Last 7 Days', 'N/A', 'N/A', 'N/A', 'N/A'], ['Down Last 30 Days', 'N/A', '1', '1', '2']], [['Growth Estimates', 'AAPL', 'Industry', 'Sector(s)', 'S&P 500'], ['Current Qtr.', '1.60%', 'N/A', 'N/A', 'N/A'], ['Next Qtr.', '5.30%', 'N/A', 'N/A', 'N/A'], ['Current Year', '-6.40%', 'N/A', 'N/A', 'N/A'], ['Next Year', '8.20%', 'N/A', 'N/A', 'N/A'], ['Next 5 Years (per annum)', '6.98%', 'N/A', 'N/A', 'N/A'], ['Past 5 Years (per annum)', '22.16%', 'N/A', 'N/A', 'N/A']]]\n",
      "2023-11-05 02:28:13,453 - INFO - Writing data to table_0.csv\n",
      "2023-11-05 02:28:13,454 - INFO - Data written to table_0.csv successfully.\n",
      "2023-11-05 02:28:13,454 - INFO - Writing data to table_1.csv\n",
      "2023-11-05 02:28:13,455 - INFO - Data written to table_1.csv successfully.\n",
      "2023-11-05 02:28:13,455 - INFO - Writing data to table_2.csv\n",
      "2023-11-05 02:28:13,455 - INFO - Data written to table_2.csv successfully.\n",
      "2023-11-05 02:28:13,456 - INFO - Writing data to table_3.csv\n",
      "2023-11-05 02:28:13,456 - INFO - Data written to table_3.csv successfully.\n",
      "2023-11-05 02:28:13,456 - INFO - Writing data to table_4.csv\n",
      "2023-11-05 02:28:13,457 - INFO - Data written to table_4.csv successfully.\n",
      "2023-11-05 02:28:13,457 - INFO - Writing data to table_5.csv\n",
      "2023-11-05 02:28:13,458 - INFO - Data written to table_5.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://finance.yahoo.com/quote/AAPL/analysis\"  # Change this to the exact URL you want to scrape\n",
    "OUTPUT_FILE = \"aapl_analyst_ratings.csv\"\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    # options.add_argument('--headless')  # Uncomment if you want to run Firefox in headless mode\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "def navigate_to_page(driver, url):\n",
    "    logging.info(f\"Navigating to {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for page to load and any pop-ups to appear\n",
    "\n",
    "def extract_data(driver):\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "    all_tables_data = []\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Extracting data from all tables...\")\n",
    "        tables = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'table')))\n",
    "        \n",
    "        for table in tables:\n",
    "            data = []\n",
    "\n",
    "            # Extract headers\n",
    "            headers = table.find_elements(By.TAG_NAME, 'th')\n",
    "            header_row = [header.text for header in headers]\n",
    "            data.append(header_row)\n",
    "\n",
    "            # Extract rows\n",
    "            rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if cols:  # This checks if the row is not a header or empty row\n",
    "                    data_row = [col.text for col in cols]\n",
    "                    data.append(data_row)\n",
    "            \n",
    "            all_tables_data.append(data)\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        logging.error(\"StaleElementReferenceException occurred while extracting data\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while extracting data from all tables.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    return all_tables_data\n",
    "\n",
    "\n",
    "def save_data_to_csv(table_data, filename):\n",
    "    # Define the directory structure\n",
    "    directory = \"yahoo_appl/yahoo_analysis\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Define the full file path\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    # Write the CSV file\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in table_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting the web driver.\")\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        logging.info(\"Navigating to the page.\")\n",
    "        navigate_to_page(driver, BASE_URL)\n",
    "        # If there are filters to apply, you would do that here before extracting data\n",
    "        \n",
    "        logging.info(\"Extracting data.\")\n",
    "        data = extract_data(driver)  # This will be your list of tables\n",
    "        logging.info(\"Data extraction complete.\")\n",
    "        logging.info(f\"Data extracted: {data}\")  # Add this line to log the extracted data\n",
    "        if data:\n",
    "            for i, table_data in enumerate(data):\n",
    "                filename = f\"table_{i}.csv\"  # This gives each table a unique filename\n",
    "                logging.info(f\"Writing data to {filename}\")\n",
    "                save_data_to_csv(table_data, filename)\n",
    "                logging.info(f\"Data written to {filename} successfully.\")\n",
    "\n",
    "        else:\n",
    "            logging.info(\"No data found on the page.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in the main function.\")\n",
    "        logging.error(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a35e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77a57b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 02:40:51,598 - INFO - Navigating to https://finance.yahoo.com/quote/AAPL/options?p=AAPL\n",
      "2023-11-05 02:41:04,182 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:06,592 - INFO - Data for November 10, 2023 written to table_0_November_10,_2023.csv successfully.\n",
      "2023-11-05 02:41:11,861 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:19,084 - INFO - Data for November 17, 2023 written to table_1_November_17,_2023.csv successfully.\n",
      "2023-11-05 02:41:24,346 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:26,130 - INFO - Data for November 24, 2023 written to table_2_November_24,_2023.csv successfully.\n",
      "2023-11-05 02:41:31,380 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:32,987 - INFO - Data for December 1, 2023 written to table_3_December_1,_2023.csv successfully.\n",
      "2023-11-05 02:41:38,232 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:40,270 - INFO - Data for December 8, 2023 written to table_4_December_8,_2023.csv successfully.\n",
      "2023-11-05 02:41:45,547 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:48,421 - INFO - Data for December 15, 2023 written to table_5_December_15,_2023.csv successfully.\n",
      "2023-11-05 02:41:53,733 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:41:58,883 - INFO - Data for January 19, 2024 written to table_6_January_19,_2024.csv successfully.\n",
      "2023-11-05 02:42:04,150 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:42:07,333 - INFO - Data for February 16, 2024 written to table_7_February_16,_2024.csv successfully.\n",
      "2023-11-05 02:42:12,608 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:42:22,490 - INFO - Data for March 15, 2024 written to table_8_March_15,_2024.csv successfully.\n",
      "2023-11-05 02:42:27,909 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:42:32,186 - INFO - Data for April 19, 2024 written to table_9_April_19,_2024.csv successfully.\n",
      "2023-11-05 02:42:37,490 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:42:42,293 - INFO - Data for June 21, 2024 written to table_10_June_21,_2024.csv successfully.\n",
      "2023-11-05 02:42:47,586 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:42:54,226 - INFO - Data for September 20, 2024 written to table_11_September_20,_2024.csv successfully.\n",
      "2023-11-05 02:42:59,618 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:43:12,238 - INFO - Data for December 20, 2024 written to table_12_December_20,_2024.csv successfully.\n",
      "2023-11-05 02:43:17,684 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:43:25,012 - INFO - Data for January 17, 2025 written to table_13_January_17,_2025.csv successfully.\n",
      "2023-11-05 02:43:30,330 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:43:34,279 - INFO - Data for June 20, 2025 written to table_14_June_20,_2025.csv successfully.\n",
      "2023-11-05 02:43:39,623 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:43:42,015 - INFO - Data for September 19, 2025 written to table_15_September_19,_2025.csv successfully.\n",
      "2023-11-05 02:43:47,371 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:43:51,902 - INFO - Data for December 19, 2025 written to table_16_December_19,_2025.csv successfully.\n",
      "2023-11-05 02:43:57,188 - INFO - Extracting data from tables...\n",
      "2023-11-05 02:44:06,148 - INFO - Data for January 16, 2026 written to table_17_January_16,_2026.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://finance.yahoo.com/quote/AAPL/options?p=AAPL\"\n",
    "OUTPUT_DIR = \"yahoo_appl/yahoo_options\"\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    # options.add_argument('--headless')  # Uncomment if you want to run Firefox in headless mode\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "def navigate_to_page(driver, url):\n",
    "    logging.info(f\"Navigating to {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for page to load and any pop-ups to appear\n",
    "\n",
    "def extract_and_save_options_data(driver, date_text, index):\n",
    "    # Find the dropdown element and select the date by value\n",
    "    select = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.Cf select')))\n",
    "    for option in select.find_elements(By.TAG_NAME, 'option'):\n",
    "        if option.text == date_text:\n",
    "            option.click()  # select() in earlier versions of webdriver\n",
    "            break\n",
    "    \n",
    "    # Wait for the data to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Extract the data\n",
    "    data = extract_data(driver)\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    if data:\n",
    "        filename = f\"table_{index}_{date_text.replace(' ', '_')}.csv\"\n",
    "        save_data_to_csv(data, filename)\n",
    "        logging.info(f\"Data for {date_text} written to {filename} successfully.\")\n",
    "    else:\n",
    "        logging.error(f\"No data found for {date_text}.\")\n",
    "\n",
    "def extract_data(driver):\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table\")))\n",
    "    all_tables_data = []\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Extracting data from tables...\")\n",
    "        tables = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'table')))\n",
    "        \n",
    "        for table in tables:\n",
    "            data = []\n",
    "            # Extract headers\n",
    "            headers = table.find_elements(By.TAG_NAME, 'th')\n",
    "            header_row = [header.text for header in headers]\n",
    "            data.append(header_row)\n",
    "            # Extract rows\n",
    "            rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if cols:  # This checks if the row is not a header or empty row\n",
    "                    data_row = [col.text for col in cols]\n",
    "                    data.append(data_row)\n",
    "            all_tables_data.append(data)\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        logging.error(\"StaleElementReferenceException occurred while extracting data\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while extracting data from tables.\")\n",
    "        logging.error(e)\n",
    "    \n",
    "    return all_tables_data\n",
    "\n",
    "def save_data_to_csv(data, filename):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    # Define the full file path\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    # Write the CSV file\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for table in data:  # data is a list of tables, and each table is a list of rows\n",
    "            for row in table:\n",
    "                writer.writerow(row)\n",
    "            writer.writerow([])  # Add an empty row between tables for readability\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        navigate_to_page(driver, BASE_URL)\n",
    "        \n",
    "        # Extract the available dates from the dropdown\n",
    "        select = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.Cf select')))\n",
    "        date_values = [option.text for option in select.find_elements(By.TAG_NAME, 'option')]\n",
    "        \n",
    "        # Loop through each date and extract/save the options data\n",
    "        for index, date_text in enumerate(date_values):\n",
    "            extract_and_save_options_data(driver, date_text, index)\n",
    "            \n",
    "    except TimeoutException:\n",
    "        logging.error(\"Page took too long to load and could not find the necessary elements.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in the main function.\")\n",
    "        logging.error(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42b220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9389c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Historical_Prices_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Capital_Gain_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_YTD_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Dividends_Only_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1Y_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5Y_Stock_Splits_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Dividends_Only_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Capital_Gain_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Stock_Splits_Daily.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_3M_Stock_Splits_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_Max_Historical_Prices_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_6M_Capital_Gain_Weekly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_1D_Historical_Prices_Monthly.csv\n",
      "-----------------------------------\n",
      "\n",
      "File: yahoo_appl/yahoo_appl_history/aapl_historical_data_5D_Dividends_Only_Daily.csv\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to analyze a single CSV file\n",
    "def analyze_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        analyze_dataframe(df, file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Could not read {file_path}: {e}')\n",
    "\n",
    "# Function to analyze a single JSON file with nested structure details\n",
    "def analyze_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.json_normalize(data, sep='_')\n",
    "            analyze_dataframe(df, file_path)\n",
    "            if isinstance(data, dict):\n",
    "                print_nested_json_structure(data, level=0)\n",
    "            elif isinstance(data, list) and data:\n",
    "                print_nested_json_structure(data[0], level=0)  # Assumes first element is representative of the list\n",
    "    except Exception as e:\n",
    "        print(f'Could not read {file_path}: {e}')\n",
    "\n",
    "# Helper function to print nested JSON structure\n",
    "def print_nested_json_structure(data, level):\n",
    "    indent = '  ' * level\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict) or isinstance(value, list):\n",
    "                print(f'{indent}{key}:')\n",
    "                print_nested_json_structure(value, level + 1)\n",
    "            else:\n",
    "                print(f'{indent}{key}: {type(value).__name__}')\n",
    "    elif isinstance(data, list):\n",
    "        print(f'{indent}List of {len(data)} items: {type(data[0]).__name__}')\n",
    "        if data and isinstance(data[0], (dict, list)):\n",
    "            print_nested_json_structure(data[0], level + 1)\n",
    "\n",
    "# General function to analyze DataFrames\n",
    "def analyze_dataframe(df, file_path):\n",
    "    column_details = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    num_rows, num_columns = df.shape\n",
    "    nan_values = df.isna().sum().to_dict()\n",
    "\n",
    "    print(f'File: {file_path}')\n",
    "    #print(f'Columns and Types: {column_details}')\n",
    "    #print(f'Number of Rows: {num_rows}')\n",
    "    #print(f'Number of Columns: {num_columns}')\n",
    "    #print(f'NaN values per column: {nan_values}')\n",
    "    print('-----------------------------------\\n')\n",
    "\n",
    "# Function to traverse directories and find CSV and JSON files\n",
    "def find_files(directory):\n",
    "    pathlist = Path(directory).rglob('*.*')\n",
    "    for path in pathlist:\n",
    "        # Analyze files based on extension\n",
    "        if path.suffix.lower() == '.csv':\n",
    "            analyze_csv(str(path))\n",
    "        elif path.suffix.lower() == '.json':\n",
    "            analyze_json(str(path))\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory containing your CSV and JSON files\n",
    "your_directory_path = './yahoo_appl/yahoo_appl_history/'\n",
    "find_files(your_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5a64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c138087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 12:17:33,023 - INFO - Navigating to https://finance.yahoo.com/quote/AAPL/history\n",
      "2023-11-05 12:17:49,813 - INFO - Selecting time period: Max\n",
      "2023-11-05 12:17:52,793 - INFO - Selecting frequency: Daily\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124a956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
