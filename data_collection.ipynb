{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4adfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c04333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 17:12:25,229 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to sp500_companies.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S&P_500_companies'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# If the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the <tbody> element in the HTML where the data resides\n",
    "    table_body = soup.find('tbody')\n",
    "    \n",
    "    # Initialize a list to store your data\n",
    "    data = []\n",
    "    \n",
    "    # Find all <tr> elements\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        # Find all <td> elements in this row\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # Get the text from all the <td> elements and add to the list\n",
    "        data.append([ele.text.strip() for ele in cols])\n",
    "    \n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data, columns=['Ticker', 'Company', 'Sector', 'Sub-Industry', 'Headquarters', 'Date First Added', 'CIK', 'Founded'])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('sp500_companies.csv', index=False)\n",
    "\n",
    "    print('Data scraped and saved to sp500_companies.csv')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage. Status code:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c84efd1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for ticker in df['Ticker']:\n",
    "#    print(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a88020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Sub-Industry</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Date First Added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>0000066740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>0000091142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>0000001800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>0001551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>XYL</td>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Machinery &amp; Supplies &amp; Components</td>\n",
       "      <td>White Plains, New York</td>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>0001524472</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>1997-10-06</td>\n",
       "      <td>0001041061</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Electronic Equipment &amp; Instruments</td>\n",
       "      <td>Lincolnshire, Illinois</td>\n",
       "      <td>2019-12-23</td>\n",
       "      <td>0000877212</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Warsaw, Indiana</td>\n",
       "      <td>2001-08-07</td>\n",
       "      <td>0001136869</td>\n",
       "      <td>1927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>Parsippany, New Jersey</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>0001555280</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker             Company                  Sector  \\\n",
       "0     None                None                    None   \n",
       "1      MMM                  3M             Industrials   \n",
       "2      AOS         A. O. Smith             Industrials   \n",
       "3      ABT              Abbott             Health Care   \n",
       "4     ABBV              AbbVie             Health Care   \n",
       "..     ...                 ...                     ...   \n",
       "498    XYL          Xylem Inc.             Industrials   \n",
       "499    YUM         Yum! Brands  Consumer Discretionary   \n",
       "500   ZBRA  Zebra Technologies  Information Technology   \n",
       "501    ZBH       Zimmer Biomet             Health Care   \n",
       "502    ZTS              Zoetis             Health Care   \n",
       "\n",
       "                                     Sub-Industry             Headquarters  \\\n",
       "0                                            None                     None   \n",
       "1                        Industrial Conglomerates    Saint Paul, Minnesota   \n",
       "2                               Building Products     Milwaukee, Wisconsin   \n",
       "3                           Health Care Equipment  North Chicago, Illinois   \n",
       "4                                   Biotechnology  North Chicago, Illinois   \n",
       "..                                            ...                      ...   \n",
       "498  Industrial Machinery & Supplies & Components   White Plains, New York   \n",
       "499                                   Restaurants     Louisville, Kentucky   \n",
       "500            Electronic Equipment & Instruments   Lincolnshire, Illinois   \n",
       "501                         Health Care Equipment          Warsaw, Indiana   \n",
       "502                               Pharmaceuticals   Parsippany, New Jersey   \n",
       "\n",
       "    Date First Added         CIK      Founded  \n",
       "0               None        None         None  \n",
       "1         1957-03-04  0000066740         1902  \n",
       "2         2017-07-26  0000091142         1916  \n",
       "3         1957-03-04  0000001800         1888  \n",
       "4         2012-12-31  0001551152  2013 (1888)  \n",
       "..               ...         ...          ...  \n",
       "498       2011-11-01  0001524472         2011  \n",
       "499       1997-10-06  0001041061         1997  \n",
       "500       2019-12-23  0000877212         1969  \n",
       "501       2001-08-07  0001136869         1927  \n",
       "502       2013-06-21  0001555280         1952  \n",
       "\n",
       "[503 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91be5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_tickers = ['MMM','AOS','ABT','ABBV','ACN','ADM','ADBE','ADP','AES','AFL','A','ABNB','APD','AKAM','ALK','ALB','ARE','ALGN','ALLE','LNT','ALL','GOOGL','GOOG','MO','AMZN','AMCR','AMD','AEE','AAL','AEP','AXP','AIG','AMT','AWK','AMP','AME','AMGN','APH','ADI','ANSS','AON','APA','AAPL','AMAT','APTV','ACGL','ANET','AJG','AIZ','T','ATO','ADSK','AZO','AVB','AVY','AXON','BKR','BALL','BAC','BBWI','BAX','BDX','WRB','BRK.B','BBY','BIO','TECH','BIIB','BLK','BX','BK','BA','BKNG','BWA','BXP','BSX','BMY','AVGO','BR','BRO','BF.B','BG','CHRW','CDNS','CZR','CPT','CPB','COF','CAH','KMX','CCL','CARR','CTLT','CAT','CBOE','CBRE','CDW','CE','COR','CNC','CNP','CDAY','CF','CRL','SCHW','CHTR','CVX','CMG','CB','CHD','CI','CINF','CTAS','CSCO','C','CFG','CLX','CME','CMS','KO','CTSH','CL','CMCSA','CMA','CAG','COP','ED','STZ','CEG','COO','CPRT','GLW','CTVA','CSGP','COST','CTRA','CCI','CSX','CMI','CVS','DHI','DHR','DRI','DVA','DE','DAL','XRAY','DVN','DXCM','FANG','DLR','DFS','DIS','DG','DLTR','D','DPZ','DOV','DOW','DTE','DUK','DD','EMN','ETN','EBAY','ECL','EIX','EW','EA','ELV','LLY','EMR','ENPH','ETR','EOG','EPAM','EQT','EFX','EQIX','EQR','ESS','EL','ETSY','EG','EVRG','ES','EXC','EXPE','EXPD','EXR','XOM','FFIV','FDS','FICO','FAST','FRT','FDX','FITB','FSLR','FE','FIS','FI','FLT','FMC','F','FTNT','FTV','FOXA','FOX','BEN','FCX','GRMN','IT','GEHC','GEN','GNRC','GD','GE','GIS','GM','GPC','GILD','GL','GPN','GS','HAL','HIG','HAS','HCA','PEAK','HSIC','HSY','HES','HPE','HLT','HOLX','HD','HON','HRL','HST','HWM','HPQ','HUBB','HUM','HBAN','HII','IBM','IEX','IDXX','ITW','ILMN','INCY','IR','PODD','INTC','ICE','IFF','IP','IPG','INTU','ISRG','IVZ','INVH','IQV','IRM','JBHT','JKHY','J','JNJ','JCI','JPM','JNPR','K','KVUE','KDP','KEY','KEYS','KMB','KIM','KMI','KLAC','KHC','KR','LHX','LH','LRCX','LW','LVS','LDOS','LEN','LIN','LYV','LKQ','LMT','L','LOW','LULU','LYB','MTB','MRO','MPC','MKTX','MAR','MMC','MLM','MAS','MA','MTCH','MKC','MCD','MCK','MDT','MRK','META','MET','MTD','MGM','MCHP','MU','MSFT','MAA','MRNA','MHK','MOH','TAP','MDLZ','MPWR','MNST','MCO','MS','MOS','MSI','MSCI','NDAQ','NTAP','NFLX','NEM','NWSA','NWS','NEE','NKE','NI','NDSN','NSC','NTRS','NOC','NCLH','NRG','NUE','NVDA','NVR','NXPI','ORLY','OXY','ODFL','OMC','ON','OKE','ORCL','OTIS','PCAR','PKG','PANW','PARA','PH','PAYX','PAYC','PYPL','PNR','PEP','PFE','PCG','PM','PSX','PNW','PXD','PNC','POOL','PPG','PPL','PFG','PG','PGR','PLD','PRU','PEG','PTC','PSA','PHM','QRVO','PWR','QCOM','DGX','RL','RJF','RTX','O','REG','REGN','RF','RSG','RMD','RVTY','RHI','ROK','ROL','ROP','ROST','RCL','SPGI','CRM','SBAC','SLB','STX','SEE','SRE','NOW','SHW','SPG','SWKS','SJM','SNA','SEDG','SO','LUV','SWK','SBUX','STT','STLD','STE','SYK','SYF','SNPS','SYY','TMUS','TROW','TTWO','TPR','TRGP','TGT','TEL','TDY','TFX','TER','TSLA','TXN','TXT','TMO','TJX','TSCO','TT','TDG','TRV','TRMB','TFC','TYL','TSN','USB','UDR','ULTA','UNP','UAL','UPS','URI','UNH','UHS','VLO','VTR','VLTO','VRSN','VRSK','VZ','VRTX','VFC','VTRS','VICI','V','VMC','WAB','WBA','WMT','WBD','WM','WAT','WEC','WFC','WELL','WST','WDC','WRK','WY','WHR','WMB','WTW','GWW','WYNN','XEL','XYL','YUM','ZBRA','ZBH','ZION','ZT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8923544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp500_tickers = ['UAL','UPS','URI','UNH','UHS','VLO','VTR','VLTO','VRSN','VRSK','VZ','VRTX','VFC','VTRS','VICI','V','VMC','WAB','WBA','WMT','WBD','WM','WAT','WEC','WFC','WELL','WST','WDC','WRK','WY','WHR','WMB','WTW','GWW','WYNN','XEL','XYL','YUM','ZBRA','ZBH','ZION','ZT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a1e51b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/ruthikkale/anaconda3/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1e0335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 03:22:50,969 - ERROR - An error occurred while processing .\n",
      "2024-05-05 03:22:50,971 - ERROR - HTTP Error 404: Not Found\n",
      "2024-05-05 03:22:51,093 - ERROR - An error occurred while processing BRK.B.\n",
      "2024-05-05 03:22:51,095 - ERROR - HTTP Error 404: Not Found\n",
      "2024-05-05 03:22:51,214 - ERROR - An error occurred while processing GEV.\n",
      "2024-05-05 03:22:51,216 - ERROR - HTTP Error 400: Bad Request\n",
      "2024-05-05 03:22:51,348 - ERROR - An error occurred while processing SOLV.\n",
      "2024-05-05 03:22:51,349 - ERROR - HTTP Error 400: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "OUTPUT_FILE_TEMPLATE = \"aapl_historical_data_{time_period}_{show}_{frequency}.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL_TEMPLATE = \"https://finance.yahoo.com/quote/{}/history\"  # URL template to be formatted with ticker symbol\n",
    "OUTPUT_FILE = \"aapl_historical_data.csv\"\n",
    "DATE_RANGE_ID = 'date-range-selector'  # Update this with the actual ID or selector for the date range element\n",
    "FREQUENCY_SELECTOR = 'frequency-selector'  # Update this with the actual ID or selector for the frequency dropdown\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "CSV_FILE_PATH = \"./sp500_companies.csv\"\n",
    "DOWNLOAD_DIRECTORY = './yahoo_/yahoo_sp500/'\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "def check_and_download_csv(file_path, max_age_hours=24):\n",
    "    if os.path.exists(file_path):\n",
    "        file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        if datetime.now() - file_mod_time > timedelta(hours=max_age_hours):\n",
    "            logging.info(f\"File {file_path} is older than {max_age_hours} hours, re-downloading.\")\n",
    "            download_file(file_path)\n",
    "        else:\n",
    "            logging.info(f\"File {file_path} is up-to-date, no need to re-download.\")\n",
    "    else:\n",
    "        logging.info(f\"File {file_path} does not exist, downloading now.\")\n",
    "        download_file(file_path)\n",
    "\n",
    "def download_file(file_path):\n",
    "    download_url = \"http://example.com/download.csv\"  # This URL should be dynamically constructed if needed\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    logging.info(f\"Downloaded {file_path}\")\n",
    "\n",
    "def read_tickers(file_path):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        return [row['Ticker'] for row in reader]  # Ensure column name matches your CSV\n",
    "\n",
    "# Use logging in your functions\n",
    "def navigate_to_page(driver, url):\n",
    "    try:\n",
    "        logging.info(f\"Navigating to {url}\")\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while trying to navigate to the page.\")\n",
    "        logging.error(e)\n",
    "\n",
    "def select_time_period(driver, period):\n",
    "    try:\n",
    "        logging.info(f\"Selecting time period: {period}\")\n",
    "        \n",
    "        # Click the date range dropdown to reveal the options\n",
    "        date_range_dropdown = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"svg[data-icon='CoreArrowDown']\"))\n",
    "        )\n",
    "        logging.info(\"Date range dropdown found and clickable.\")\n",
    "        \n",
    "        date_range_dropdown.click()\n",
    "        logging.info(\"Date range dropdown clicked.\")\n",
    "\n",
    "        # Now that the dropdown is open, click the 'Max' option\n",
    "        max_option = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[@data-value='MAX']\"))\n",
    "        )\n",
    "        logging.info(\"Max option found and clickable.\")\n",
    "        \n",
    "        # Here you can add code to verify if the correct option is being selected\n",
    "        if max_option.get_attribute('data-value') == 'MAX':\n",
    "            logging.info(\"Correct 'Max' option is present.\")\n",
    "        else:\n",
    "            logging.warning(\"The 'Max' option does not have the expected 'data-value' attribute.\")\n",
    "\n",
    "        max_option.click()\n",
    "        logging.info(\"Max option clicked.\")\n",
    "\n",
    "        # After clicking, you can also verify the current selected date range\n",
    "        # This assumes that there's an element that reflects the selected date range.\n",
    "        # You may need to update the selector as per the actual website structure.\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find the time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting time period: {period}\")\n",
    "        logging.error(e)\n",
    "\n",
    "def select_frequency(driver, frequency):\n",
    "    try:\n",
    "        logging.info(f\"Selecting frequency: {frequency}\")\n",
    "        # Add additional logging if necessary\n",
    "        # For example, log the current URL or take a screenshot\n",
    "        current_url = driver.current_url\n",
    "        logging.info(f\"Current URL: {current_url}\")\n",
    "\n",
    "        # Your code to select frequency here...\n",
    "\n",
    "        # After selecting, add an explicit wait for the page to load or for the element to be clickable\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'the-id-of-the-frequency-dropdown'))\n",
    "        )\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for frequency option: {frequency}\")\n",
    "        logging.error(e)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find the frequency option: {frequency}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting frequency: {frequency}\")\n",
    "        logging.error(e)\n",
    "        # Optional: Take a screenshot on error\n",
    "        driver.get_screenshot_as_file(\"error_screenshot.png\")\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "def download_csv(download_url, filename, download_directory='./yahoo_/yahoo_sp500/'):\n",
    "    # Ensure the download directory exists\n",
    "    if not os.path.exists(download_directory):\n",
    "        os.makedirs(download_directory)\n",
    "    # Define the full file path\n",
    "    filepath = os.path.join(download_directory, filename)\n",
    "    # Use urllib to download the file\n",
    "    urllib.request.urlretrieve(download_url, filepath)\n",
    "    logging.info(f\"File downloaded: {filepath}\")\n",
    "\n",
    "def read_tickers_from_csv(csv_file, column_name):\n",
    "    tickers = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if column_name in row:\n",
    "                tickers.append(row[column_name])\n",
    "    return tickers\n",
    "\n",
    "# Function to check if the file was downloaded more than 24 hours ago\n",
    "def is_file_old(filepath, hours=24):\n",
    "    if not os.path.exists(filepath):\n",
    "        return True\n",
    "    now = datetime.now()\n",
    "    modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
    "    return (now - modified_time) > timedelta(hours=hours)\n",
    "\n",
    "def main():\n",
    "    # Constants\n",
    "    CSV_FILE = \"./sp500_companies.csv\"\n",
    "    COLUMN_NAME = \"Ticker\"\n",
    "    DOWNLOAD_DIRECTORY = './yahoo_/yahoo_sp500/'\n",
    "\n",
    "    # Read tickers from CSV\n",
    "    tickers = read_tickers_from_csv(CSV_FILE, COLUMN_NAME)\n",
    "\n",
    "    # Iterate over tickers\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Prepare the CSV download URL\n",
    "            csv_download_url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1=-252374400&period2=1699142400&interval=1d&events=history&includeAdjustedClose=true\"\n",
    "            filename = f\"{ticker}.csv\"\n",
    "            filepath = os.path.join(DOWNLOAD_DIRECTORY, filename)\n",
    "\n",
    "            # Check if file needs to be downloaded\n",
    "            if is_file_old(filepath):\n",
    "                # Download CSV file\n",
    "                download_csv(csv_download_url, filename, DOWNLOAD_DIRECTORY)\n",
    "            \n",
    "                # Wait before making the next request\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while processing {ticker}.\")\n",
    "            logging.error(e)\n",
    "            # Optionally, add ticker to a list or file for failed attempts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743b7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686277d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f515cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b505d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 03:22:45,213 - ERROR - An error occurred while processing .\n",
      "2024-05-05 03:22:45,215 - ERROR - HTTP Error 404: Not Found\n",
      "2024-05-05 03:22:45,338 - ERROR - An error occurred while processing BRK.B.\n",
      "2024-05-05 03:22:45,340 - ERROR - HTTP Error 404: Not Found\n",
      "2024-05-05 03:22:45,476 - ERROR - An error occurred while processing GEV.\n",
      "2024-05-05 03:22:45,478 - ERROR - HTTP Error 400: Bad Request\n",
      "2024-05-05 03:22:45,602 - ERROR - An error occurred while processing SOLV.\n",
      "2024-05-05 03:22:45,604 - ERROR - HTTP Error 400: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "OUTPUT_FILE_TEMPLATE = \"aapl_historical_data_{time_period}_{show}_{frequency}.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "BASE_URL_TEMPLATE = \"https://finance.yahoo.com/quote/{}/div\"  # URL template to be formatted with ticker symbol\n",
    "OUTPUT_FILE = \"aapl_historical_data.csv\"\n",
    "DATE_RANGE_ID = 'date-range-selector'  # Update this with the actual ID or selector for the date range element\n",
    "FREQUENCY_SELECTOR = 'frequency-selector'  # Update this with the actual ID or selector for the frequency dropdown\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "CSV_FILE_PATH = \"./sp500_companies.csv\"\n",
    "DOWNLOAD_DIRECTORY = './yahoo_/yahoo_sp500_div/'\n",
    "\n",
    "def setup_driver():\n",
    "    options = FirefoxOptions()\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    return driver\n",
    "\n",
    "def check_and_download_csv(file_path, max_age_hours=24):\n",
    "    if os.path.exists(file_path):\n",
    "        file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "        if datetime.now() - file_mod_time > timedelta(hours=max_age_hours):\n",
    "            logging.info(f\"File {file_path} is older than {max_age_hours} hours, re-downloading.\")\n",
    "            download_file(file_path)\n",
    "        else:\n",
    "            logging.info(f\"File {file_path} is up-to-date, no need to re-download.\")\n",
    "    else:\n",
    "        logging.info(f\"File {file_path} does not exist, downloading now.\")\n",
    "        download_file(file_path)\n",
    "\n",
    "def download_file(file_path):\n",
    "    download_url = \"http://example.com/download.csv\"  # This URL should be dynamically constructed if needed\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    logging.info(f\"Downloaded {file_path}\")\n",
    "\n",
    "def read_tickers(file_path):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        return [row['Ticker'] for row in reader]  # Ensure column name matches your CSV\n",
    "\n",
    "# Use logging in your functions\n",
    "def navigate_to_page(driver, url):\n",
    "    try:\n",
    "        logging.info(f\"Navigating to {url}\")\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred while trying to navigate to the page.\")\n",
    "        logging.error(e)\n",
    "\n",
    "def select_time_period(driver, period):\n",
    "    try:\n",
    "        logging.info(f\"Selecting time period: {period}\")\n",
    "        \n",
    "        # Click the date range dropdown to reveal the options\n",
    "        date_range_dropdown = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"svg[data-icon='CoreArrowDown']\"))\n",
    "        )\n",
    "        logging.info(\"Date range dropdown found and clickable.\")\n",
    "        \n",
    "        date_range_dropdown.click()\n",
    "        logging.info(\"Date range dropdown clicked.\")\n",
    "\n",
    "        # Now that the dropdown is open, click the 'Max' option\n",
    "        max_option = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[@data-value='MAX']\"))\n",
    "        )\n",
    "        logging.info(\"Max option found and clickable.\")\n",
    "        \n",
    "        # Here you can add code to verify if the correct option is being selected\n",
    "        if max_option.get_attribute('data-value') == 'MAX':\n",
    "            logging.info(\"Correct 'Max' option is present.\")\n",
    "        else:\n",
    "            logging.warning(\"The 'Max' option does not have the expected 'data-value' attribute.\")\n",
    "\n",
    "        max_option.click()\n",
    "        logging.info(\"Max option clicked.\")\n",
    "\n",
    "        # After clicking, you can also verify the current selected date range\n",
    "        # This assumes that there's an element that reflects the selected date range.\n",
    "        # You may need to update the selector as per the actual website structure.\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find the time period option: {period}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting time period: {period}\")\n",
    "        logging.error(e)\n",
    "\n",
    "def select_frequency(driver, frequency):\n",
    "    try:\n",
    "        logging.info(f\"Selecting frequency: {frequency}\")\n",
    "        # Add additional logging if necessary\n",
    "        # For example, log the current URL or take a screenshot\n",
    "        current_url = driver.current_url\n",
    "        logging.info(f\"Current URL: {current_url}\")\n",
    "\n",
    "        # Your code to select frequency here...\n",
    "\n",
    "        # After selecting, add an explicit wait for the page to load or for the element to be clickable\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'the-id-of-the-frequency-dropdown'))\n",
    "        )\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timed out waiting for frequency option: {frequency}\")\n",
    "        logging.error(e)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find the frequency option: {frequency}\")\n",
    "        logging.error(e)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while selecting frequency: {frequency}\")\n",
    "        logging.error(e)\n",
    "        # Optional: Take a screenshot on error\n",
    "        driver.get_screenshot_as_file(\"error_screenshot.png\")\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "def download_csv(download_url, filename, download_directory='./yahoo_/yahoo_sp500_div/'):\n",
    "    # Ensure the download directory exists\n",
    "    if not os.path.exists(download_directory):\n",
    "        os.makedirs(download_directory)\n",
    "    # Define the full file path\n",
    "    filepath = os.path.join(download_directory, filename)\n",
    "    # Use urllib to download the file\n",
    "    urllib.request.urlretrieve(download_url, filepath)\n",
    "    logging.info(f\"File downloaded: {filepath}\")\n",
    "\n",
    "def read_tickers_from_csv(csv_file, column_name):\n",
    "    tickers = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if column_name in row:\n",
    "                tickers.append(row[column_name])\n",
    "    return tickers\n",
    "\n",
    "# Function to check if the file was downloaded more than 24 hours ago\n",
    "def is_file_old(filepath, hours=24):\n",
    "    if not os.path.exists(filepath):\n",
    "        return True\n",
    "    now = datetime.now()\n",
    "    modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
    "    return (now - modified_time) > timedelta(hours=hours)\n",
    "\n",
    "def main():\n",
    "    # Constants\n",
    "    CSV_FILE = \"./sp500_companies.csv\"\n",
    "    COLUMN_NAME = \"Ticker\"\n",
    "    DOWNLOAD_DIRECTORY = './yahoo_/yahoo_sp500_div/'\n",
    "\n",
    "    # Read tickers from CSV\n",
    "    tickers = read_tickers_from_csv(CSV_FILE, COLUMN_NAME)\n",
    "\n",
    "    # Iterate over tickers\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Prepare the CSV download URL\n",
    "            csv_download_url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1=-252374400&period2=1699142400&interval=1d&events=div&includeAdjustedClose=true\"\n",
    "            filename = f\"{ticker}.csv\"\n",
    "            filepath = os.path.join(DOWNLOAD_DIRECTORY, filename)\n",
    "\n",
    "            # Check if file needs to be downloaded\n",
    "            if is_file_old(filepath):\n",
    "                # Download CSV file\n",
    "                download_csv(csv_download_url, filename, DOWNLOAD_DIRECTORY)\n",
    "            \n",
    "                # Wait before making the next request\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while processing {ticker}.\")\n",
    "            logging.error(e)\n",
    "            # Optionally, add ticker to a list or file for failed attempts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ganpati@27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0718600",
   "metadata": {},
   "outputs": [],
   "source": [
    "StockX@27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240653b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "passtime1995@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
